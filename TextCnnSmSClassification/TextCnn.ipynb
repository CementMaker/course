{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "df_data = pd.read_table(\"./data/SMSSpamCollection\", header=None, sep='\\t')\n",
    "df_data.columns = ['label', 'sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ham': 4825, 'spam': 747})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "label, sentences = df_data['label'], df_data['sentence']\n",
    "Counter(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyP1/AEOvqCMVgMoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1274f82b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "length = []\n",
    "for sentence in sentences:\n",
    "    length.append(len(sentence))\n",
    "    \n",
    "mat = dict(sorted(Counter(length).items(), key=lambda x: x[0]))\n",
    "x, y = list(mat.keys()),  list(mat.values())\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(label):\n",
    "    import numpy as np\n",
    "    one_hot_label = np.zeros(shape=[len(label), 2])\n",
    "    one_hot_label[np.arange(0, len(label)), label] = 1\n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-91f0194fdf36>:15: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/tang/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/tang/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "number of words : 5395\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Feature(object):\n",
    "    def __init__(self):\n",
    "        self.train_df = pd.read_table(\"./data/SMSSpamCollection\", header=None, sep='\\t')\n",
    "        self.train_df.columns = ['label', 'sentence']\n",
    "        self.sentence = self.train_df['sentence'].values\n",
    "        self.label = [1 if label == 'ham' else 0 for label in self.train_df['label'].values]\n",
    "        self.one_hot_label = one_hot_encoder(self.label)\n",
    "\n",
    "        self.vocab_processor = learn.preprocessing.VocabularyProcessor(200, min_frequency=1)\n",
    "        self.all_context = np.array(list(self.vocab_processor.fit_transform(self.sentence)))\n",
    "\n",
    "        print(\"number of words :\", len(self.vocab_processor.vocabulary_))\n",
    "        self.train_data, self.dev_data, self.train_label, self.dev_label = \\\n",
    "            train_test_split(self.all_context, self.one_hot_label, test_size=0.05)\n",
    "        # print(\"shape of train data:\", self.train_data.shape)\n",
    "        # print(\"shape of dev data:\", self.dev_data.shape)\n",
    "        # print(\"shape of train label:\", self.train_label.shape)\n",
    "        # print(\"shape of dev label:\", self.dev_label.shape)\n",
    "\n",
    "feature = Feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(epoches, batch_size, data, label):\n",
    "    # train_x, train_y = pickle.load(open(\"corpus_train.pkl\", \"rb\"))\n",
    "    samples = list(zip(data, label))\n",
    "    random.shuffle(samples)\n",
    "    for epoch in range(epoches):\n",
    "        for batch in range(0, len(samples), batch_size):\n",
    "            if batch + batch_size < len(samples):\n",
    "                yield samples[batch: (batch + batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def train_step(model, batch, label):\n",
    "    feed_dict = {\n",
    "        model.model.input_sentence: batch,\n",
    "        model.model.label: label\n",
    "    }\n",
    "    _, summary, step, loss, accuracy, = model.sess.run(\n",
    "        fetches=[model.optimizer,\n",
    "                 model.merged_summary_train,\n",
    "                 model.global_step,\n",
    "                 model.model.loss,\n",
    "                 model.model.accuracy],\n",
    "        feed_dict=feed_dict)\n",
    "    model.summary_writer_train.add_summary(summary, step)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    # print(\"{}: step {}, loss {}, accuracy {}\".format(time_str, step, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dev_step(model, batch, label, return_predict=False):\n",
    "    feed_dict = {\n",
    "        model.model.input_sentence: batch,\n",
    "        model.model.label: label\n",
    "    }\n",
    "    summary, step, loss, accuracy, predict = model.sess.run(\n",
    "        fetches=[model.merged_summary_test,\n",
    "                 model.global_step,\n",
    "                 model.model.loss,\n",
    "                 model.model.accuracy,\n",
    "                 model.model.predictions],\n",
    "        feed_dict=feed_dict)\n",
    "    model.summary_writer_test.add_summary(summary, step)\n",
    "    print(\"test: step {}, loss {}, accuracy {}\".format(step, loss, accuracy))\n",
    "    if return_predict == 1:\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class textCnn(object):\n",
    "    def __init__(self, sequence_length, vocab_size, embedding_size, filter_sizes, num_filters, num_classes=2):\n",
    "        # sequence_length： 句子长度\n",
    "        # vocab_size： 词表大小\n",
    "        # embedding_size： 词向量长度\n",
    "        # filter_sizes： 卷积核大小（纵向）\n",
    "        # num_filters：每个filter_size对应的卷积核个数\n",
    "        # num_classes：类别个数，默认为2\n",
    "        \n",
    "        \n",
    "        # 样本label，样本\n",
    "        self.label = tf.placeholder(tf.int32, [None, num_classes], name=\"label\")\n",
    "        self.input_sentence = tf.placeholder(tf.int32, [None, sequence_length], name=\"input\")\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            filter_shape = [vocab_size, embedding_size]\n",
    "            # 使用截断正态分布初始化词向量\n",
    "            w = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"embedding_matrix\")\n",
    "            self.embedded = tf.nn.embedding_lookup(params=w, ids=self.input_sentence)\n",
    "            \n",
    "            # 卷积层输入\n",
    "            self.embedded_expand = tf.expand_dims(self.embedded, -1)\n",
    "            \n",
    "\n",
    "        pooled_outputs = [] # 保存卷积+池化之后特征\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-max-pool-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"Conv_filter_%s\" % filter_size)\n",
    "                \n",
    "                # 使用卷积核\n",
    "                conv = tf.nn.conv2d(input=self.embedded_expand,\n",
    "                                    filter=W,\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                    padding=\"VALID\")\n",
    "                \n",
    "                # 使用最大值池化\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    value=conv,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "       \n",
    "        with tf.name_scope(\"full_connected_layer\"):\n",
    "            # 总的卷积核个数，每个卷积核产生的Feature Map经过池化层会变成一个值，卷积核的个数就是全连接的数字个数\n",
    "            num_filters_total = num_filters * len(filter_sizes) \n",
    "            self.h_pool = tf.concat(pooled_outputs, 3) # 矩阵合并\n",
    "            self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total]) # 句子的特征向量表示\n",
    "            \n",
    "            w = tf.Variable(tf.truncated_normal(shape=[num_filters_total, num_classes], stddev=0.1), name=\"w\")\n",
    "            b = tf.Variable(tf.truncated_normal(shape=[num_classes]), name=\"b\")\n",
    "            self.score = tf.nn.xw_plus_b(self.h_pool_flat, w, b)\n",
    "\n",
    "        # with tf.name_scope(\"softmax\"):\n",
    "        #     self.result = tf.nn.softmax(logits=self.score, axis=1, name='softmax')\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.real = tf.argmax(self.label, axis=1, name=\"real_label\")\n",
    "            self.predictions = tf.argmax(self.score, axis=1, name=\"predictions\")\n",
    "\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.label, logits=self.score)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, self.real)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class textCnnTrain(object):\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.model = textCnn(sequence_length=200,\n",
    "                             embedding_size=50,\n",
    "                             filter_sizes=[1, 2, 3],\n",
    "                             num_filters=10,\n",
    "                             num_classes=2,\n",
    "                             vocab_size=6000)\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.05).minimize(self.model.loss, global_step=self.global_step)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.feature = Feature()\n",
    "        self.train_data, self.dev_data, self.train_label, self.dev_label = \\\n",
    "            self.feature.train_data, self.feature.dev_data, self.feature.train_label, self.feature.dev_label\n",
    "        self.batches = get_batch(10, 100, self.train_data, self.train_label)\n",
    "\n",
    "        tf.summary.scalar('loss', self.model.loss)\n",
    "        tf.summary.scalar('accuracy', self.model.accuracy)\n",
    "        self.merged_summary_train = tf.summary.merge_all()\n",
    "        self.merged_summary_test = tf.summary.merge_all()\n",
    "        self.summary_writer_train = tf.summary.FileWriter(\"./summary/cnn_summary/train\", graph=self.sess.graph)\n",
    "        self.summary_writer_test = tf.summary.FileWriter(\"./summary/cnn_summary/test\", graph=self.sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def main(model):\n",
    "    for data in model.batches:\n",
    "        x_train, y_train = zip(*data)\n",
    "        train_step(model, x_train, y_train)\n",
    "        current_step = tf.train.global_step(model.sess, model.global_step)\n",
    "        if current_step % 2 == 0:\n",
    "            dev_step(model, model.dev_data, model.dev_label)\n",
    "\n",
    "    predict = dev_step(model, model.dev_data, model.dev_label, return_predict=True)\n",
    "    y_true = np.argmax(model.dev_label, axis=1)\n",
    "    print(classification_report(y_true, predict))\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words : 5395\n",
      "test: step 2, loss 0.5890092849731445, accuracy 0.8207885026931763\n",
      "test: step 4, loss 0.5197272896766663, accuracy 0.8207885026931763\n",
      "test: step 6, loss 0.4346756935119629, accuracy 0.8207885026931763\n",
      "test: step 8, loss 0.36475929617881775, accuracy 0.842293918132782\n",
      "test: step 10, loss 0.2504441440105438, accuracy 0.9283154010772705\n",
      "test: step 12, loss 0.18815475702285767, accuracy 0.9426523447036743\n",
      "test: step 14, loss 0.1385929435491562, accuracy 0.9641577005386353\n",
      "test: step 16, loss 0.15506386756896973, accuracy 0.9713261723518372\n",
      "test: step 18, loss 0.11838866770267487, accuracy 0.9749103784561157\n",
      "test: step 20, loss 0.1377718299627304, accuracy 0.9856630563735962\n",
      "test: step 22, loss 0.13820470869541168, accuracy 0.9856630563735962\n",
      "test: step 24, loss 0.10536983609199524, accuracy 0.9784946441650391\n",
      "test: step 26, loss 0.08947295695543289, accuracy 0.9784946441650391\n",
      "test: step 28, loss 0.08792468905448914, accuracy 0.9856630563735962\n",
      "test: step 30, loss 0.08578840643167496, accuracy 0.9820788502693176\n",
      "test: step 32, loss 0.0927119180560112, accuracy 0.9749103784561157\n",
      "test: step 34, loss 0.11879628896713257, accuracy 0.9677419066429138\n",
      "test: step 36, loss 0.08469916135072708, accuracy 0.9677419066429138\n",
      "test: step 38, loss 0.08913248777389526, accuracy 0.9641577005386353\n",
      "test: step 40, loss 0.10421890020370483, accuracy 0.9820788502693176\n",
      "test: step 42, loss 0.09243541955947876, accuracy 0.9856630563735962\n",
      "test: step 44, loss 0.07803786545991898, accuracy 0.9749103784561157\n",
      "test: step 46, loss 0.10692983120679855, accuracy 0.9641577005386353\n",
      "test: step 48, loss 0.14027297496795654, accuracy 0.9677419066429138\n",
      "test: step 50, loss 0.08982124924659729, accuracy 0.9784946441650391\n",
      "test: step 52, loss 0.20447216928005219, accuracy 0.91756272315979\n",
      "test: step 54, loss 0.07162759453058243, accuracy 0.9820788502693176\n",
      "test: step 56, loss 0.1014895886182785, accuracy 0.9820788502693176\n",
      "test: step 58, loss 0.13454818725585938, accuracy 0.9784946441650391\n",
      "test: step 60, loss 0.08688471466302872, accuracy 0.9749103784561157\n",
      "test: step 62, loss 0.10490813851356506, accuracy 0.9677419066429138\n",
      "test: step 64, loss 0.08127672225236893, accuracy 0.9892473220825195\n",
      "test: step 66, loss 0.16544753313064575, accuracy 0.9784946441650391\n",
      "test: step 68, loss 0.18088915944099426, accuracy 0.9784946441650391\n",
      "test: step 70, loss 0.13175606727600098, accuracy 0.9820788502693176\n",
      "test: step 72, loss 0.11552746593952179, accuracy 0.9641577005386353\n",
      "test: step 74, loss 0.1311510056257248, accuracy 0.9677419066429138\n",
      "test: step 76, loss 0.1306743621826172, accuracy 0.9677419066429138\n",
      "test: step 78, loss 0.1441919505596161, accuracy 0.9749103784561157\n",
      "test: step 80, loss 0.17615266144275665, accuracy 0.9820788502693176\n",
      "test: step 82, loss 0.18106709420681, accuracy 0.9820788502693176\n",
      "test: step 84, loss 0.14812232553958893, accuracy 0.9677419066429138\n",
      "test: step 86, loss 0.14410263299942017, accuracy 0.9677419066429138\n",
      "test: step 88, loss 0.1639380156993866, accuracy 0.9820788502693176\n",
      "test: step 90, loss 0.14646370708942413, accuracy 0.9820788502693176\n",
      "test: step 92, loss 0.20589126646518707, accuracy 0.9784946441650391\n",
      "test: step 94, loss 0.20877356827259064, accuracy 0.9713261723518372\n",
      "test: step 96, loss 0.18453055620193481, accuracy 0.9713261723518372\n",
      "test: step 98, loss 0.08998562395572662, accuracy 0.9749103784561157\n",
      "test: step 100, loss 0.16649022698402405, accuracy 0.9534050226211548\n",
      "test: step 102, loss 0.08885824680328369, accuracy 0.9820788502693176\n",
      "test: step 104, loss 0.12059484422206879, accuracy 0.9784946441650391\n",
      "test: step 106, loss 0.17447860538959503, accuracy 0.9713261723518372\n",
      "test: step 108, loss 0.16990767419338226, accuracy 0.9713261723518372\n",
      "test: step 110, loss 0.13032667338848114, accuracy 0.9820788502693176\n",
      "test: step 112, loss 0.12298362702131271, accuracy 0.9820788502693176\n",
      "test: step 114, loss 0.11977997422218323, accuracy 0.9820788502693176\n",
      "test: step 116, loss 0.11639592796564102, accuracy 0.9856630563735962\n",
      "test: step 118, loss 0.129158154129982, accuracy 0.9820788502693176\n",
      "test: step 120, loss 0.1862773895263672, accuracy 0.9713261723518372\n",
      "test: step 122, loss 0.24474802613258362, accuracy 0.9677419066429138\n",
      "test: step 124, loss 0.23471741378307343, accuracy 0.9713261723518372\n",
      "test: step 126, loss 0.18362122774124146, accuracy 0.9784946441650391\n",
      "test: step 128, loss 0.1512395590543747, accuracy 0.9784946441650391\n",
      "test: step 130, loss 0.14000387489795685, accuracy 0.9784946441650391\n",
      "test: step 132, loss 0.15047624707221985, accuracy 0.9784946441650391\n",
      "test: step 134, loss 0.16035979986190796, accuracy 0.9784946441650391\n",
      "test: step 136, loss 0.17048148810863495, accuracy 0.9784946441650391\n",
      "test: step 138, loss 0.1805741935968399, accuracy 0.9784946441650391\n",
      "test: step 140, loss 0.17408603429794312, accuracy 0.9784946441650391\n",
      "test: step 142, loss 0.15799741446971893, accuracy 0.9784946441650391\n",
      "test: step 144, loss 0.1800336390733719, accuracy 0.9784946441650391\n",
      "test: step 146, loss 0.2160351425409317, accuracy 0.9784946441650391\n",
      "test: step 148, loss 0.24937979876995087, accuracy 0.9784946441650391\n",
      "test: step 150, loss 0.28125616908073425, accuracy 0.9749103784561157\n",
      "test: step 152, loss 0.26380953192710876, accuracy 0.9784946441650391\n",
      "test: step 154, loss 0.2506296932697296, accuracy 0.9784946441650391\n",
      "test: step 156, loss 0.24056829512119293, accuracy 0.9784946441650391\n",
      "test: step 158, loss 0.22759419679641724, accuracy 0.9784946441650391\n",
      "test: step 160, loss 0.21867670118808746, accuracy 0.9784946441650391\n",
      "test: step 162, loss 0.2135954201221466, accuracy 0.9749103784561157\n",
      "test: step 164, loss 0.20804363489151, accuracy 0.9749103784561157\n",
      "test: step 166, loss 0.20054121315479279, accuracy 0.9713261723518372\n",
      "test: step 168, loss 0.21537122130393982, accuracy 0.9784946441650391\n",
      "test: step 170, loss 0.23331128060817719, accuracy 0.9784946441650391\n",
      "test: step 172, loss 0.2497386634349823, accuracy 0.9784946441650391\n",
      "test: step 174, loss 0.26357319951057434, accuracy 0.9784946441650391\n",
      "test: step 176, loss 0.2766922116279602, accuracy 0.9784946441650391\n",
      "test: step 178, loss 0.289578378200531, accuracy 0.9749103784561157\n",
      "test: step 180, loss 0.2928735911846161, accuracy 0.9749103784561157\n",
      "test: step 182, loss 0.2887517809867859, accuracy 0.9784946441650391\n",
      "test: step 184, loss 0.2846696972846985, accuracy 0.9784946441650391\n",
      "test: step 186, loss 0.23196306824684143, accuracy 0.9820788502693176\n",
      "test: step 188, loss 0.19460225105285645, accuracy 0.9820788502693176\n",
      "test: step 190, loss 0.17351120710372925, accuracy 0.9784946441650391\n",
      "test: step 192, loss 0.16396072506904602, accuracy 0.9749103784561157\n",
      "test: step 194, loss 0.1574869155883789, accuracy 0.9749103784561157\n",
      "test: step 196, loss 0.15365542471408844, accuracy 0.9749103784561157\n",
      "test: step 198, loss 0.15170758962631226, accuracy 0.9749103784561157\n",
      "test: step 200, loss 0.15097665786743164, accuracy 0.9749103784561157\n",
      "test: step 202, loss 0.1487906128168106, accuracy 0.9784946441650391\n",
      "test: step 204, loss 0.15001504123210907, accuracy 0.9749103784561157\n",
      "test: step 206, loss 0.1534777581691742, accuracy 0.9749103784561157\n",
      "test: step 208, loss 0.16047504544258118, accuracy 0.9784946441650391\n",
      "test: step 210, loss 0.1681005358695984, accuracy 0.9820788502693176\n",
      "test: step 212, loss 0.17585742473602295, accuracy 0.9820788502693176\n",
      "test: step 214, loss 0.18339815735816956, accuracy 0.9820788502693176\n",
      "test: step 216, loss 0.17917370796203613, accuracy 0.9820788502693176\n",
      "test: step 218, loss 0.16798382997512817, accuracy 0.9820788502693176\n",
      "test: step 220, loss 0.16261114180088043, accuracy 0.9784946441650391\n",
      "test: step 222, loss 0.1619097739458084, accuracy 0.9749103784561157\n",
      "test: step 224, loss 0.16426151990890503, accuracy 0.9713261723518372\n",
      "test: step 226, loss 0.23354394733905792, accuracy 0.9820788502693176\n",
      "test: step 228, loss 0.31722378730773926, accuracy 0.9749103784561157\n",
      "test: step 230, loss 0.4148622155189514, accuracy 0.9605734944343567\n",
      "test: step 232, loss 0.39948341250419617, accuracy 0.9605734944343567\n",
      "test: step 234, loss 0.2693302631378174, accuracy 0.9749103784561157\n",
      "test: step 236, loss 0.20948170125484467, accuracy 0.9784946441650391\n",
      "test: step 238, loss 0.18139101564884186, accuracy 0.9749103784561157\n",
      "test: step 240, loss 0.19409623742103577, accuracy 0.9820788502693176\n",
      "test: step 242, loss 0.24903364479541779, accuracy 0.9856630563735962\n",
      "test: step 244, loss 0.3241361081600189, accuracy 0.9749103784561157\n",
      "test: step 246, loss 0.2317955493927002, accuracy 0.9892473220825195\n",
      "test: step 248, loss 0.1708243489265442, accuracy 0.9820788502693176\n",
      "test: step 250, loss 0.19206860661506653, accuracy 0.9713261723518372\n",
      "test: step 252, loss 0.17095434665679932, accuracy 0.9820788502693176\n",
      "test: step 254, loss 0.2331867814064026, accuracy 0.9892473220825195\n",
      "test: step 256, loss 0.37136906385421753, accuracy 0.9820788502693176\n",
      "test: step 258, loss 0.5137612819671631, accuracy 0.9713261723518372\n",
      "test: step 260, loss 0.6204363107681274, accuracy 0.9641577005386353\n",
      "test: step 262, loss 0.45309099555015564, accuracy 0.9820788502693176\n",
      "test: step 264, loss 0.3757304251194, accuracy 0.9784946441650391\n",
      "test: step 266, loss 0.43734925985336304, accuracy 0.9784946441650391\n",
      "test: step 268, loss 0.4683465361595154, accuracy 0.9784946441650391\n",
      "test: step 270, loss 0.4739571809768677, accuracy 0.9784946441650391\n",
      "test: step 272, loss 0.47677141427993774, accuracy 0.9749103784561157\n",
      "test: step 274, loss 0.47609391808509827, accuracy 0.9749103784561157\n",
      "test: step 276, loss 0.47671908140182495, accuracy 0.9713261723518372\n",
      "test: step 278, loss 0.4777194857597351, accuracy 0.9713261723518372\n",
      "test: step 280, loss 0.46134045720100403, accuracy 0.9713261723518372\n",
      "test: step 282, loss 0.43831390142440796, accuracy 0.9677419066429138\n",
      "test: step 284, loss 0.42139744758605957, accuracy 0.9677419066429138\n",
      "test: step 286, loss 0.41434890031814575, accuracy 0.9713261723518372\n",
      "test: step 288, loss 0.41368499398231506, accuracy 0.9677419066429138\n",
      "test: step 290, loss 0.4209239184856415, accuracy 0.9713261723518372\n",
      "test: step 292, loss 0.43173906207084656, accuracy 0.9713261723518372\n",
      "test: step 294, loss 0.47121694684028625, accuracy 0.9605734944343567\n",
      "test: step 296, loss 0.51671302318573, accuracy 0.9569892287254333\n",
      "test: step 298, loss 0.48208361864089966, accuracy 0.9605734944343567\n",
      "test: step 300, loss 0.5110878348350525, accuracy 0.9749103784561157\n",
      "test: step 302, loss 0.4656074643135071, accuracy 0.9820788502693176\n",
      "test: step 304, loss 0.47424590587615967, accuracy 0.9749103784561157\n",
      "test: step 306, loss 0.4820379912853241, accuracy 0.9713261723518372\n",
      "test: step 308, loss 0.522546112537384, accuracy 0.9677419066429138\n",
      "test: step 310, loss 0.508834719657898, accuracy 0.9641577005386353\n",
      "test: step 312, loss 0.4496577978134155, accuracy 0.9713261723518372\n",
      "test: step 314, loss 0.41112542152404785, accuracy 0.9713261723518372\n",
      "test: step 316, loss 0.3911694288253784, accuracy 0.9677419066429138\n",
      "test: step 318, loss 0.3727470338344574, accuracy 0.9749103784561157\n",
      "test: step 320, loss 0.4561538100242615, accuracy 0.9749103784561157\n",
      "test: step 322, loss 0.5502340197563171, accuracy 0.9749103784561157\n",
      "test: step 324, loss 0.6399402618408203, accuracy 0.9713261723518372\n",
      "test: step 326, loss 0.7108569741249084, accuracy 0.9713261723518372\n",
      "test: step 328, loss 0.5732567310333252, accuracy 0.9713261723518372\n",
      "test: step 330, loss 0.6101224422454834, accuracy 0.9677419066429138\n",
      "test: step 332, loss 0.721200704574585, accuracy 0.9749103784561157\n",
      "test: step 334, loss 0.8358505368232727, accuracy 0.9677419066429138\n",
      "test: step 336, loss 0.9938510060310364, accuracy 0.9641577005386353\n",
      "test: step 338, loss 1.2242685556411743, accuracy 0.9605734944343567\n",
      "test: step 340, loss 1.51937997341156, accuracy 0.9534050226211548\n",
      "test: step 342, loss 0.7021748423576355, accuracy 0.9784946441650391\n",
      "test: step 344, loss 1.0309505462646484, accuracy 0.9247311949729919\n",
      "test: step 346, loss 0.710322380065918, accuracy 0.9784946441650391\n",
      "test: step 348, loss 1.4441497325897217, accuracy 0.9498208165168762\n",
      "test: step 350, loss 1.3647716045379639, accuracy 0.9534050226211548\n",
      "test: step 352, loss 1.0923523902893066, accuracy 0.9749103784561157\n",
      "test: step 354, loss 0.9571479558944702, accuracy 0.9605734944343567\n",
      "test: step 356, loss 1.0643436908721924, accuracy 0.9641577005386353\n",
      "test: step 358, loss 1.0027456283569336, accuracy 0.9713261723518372\n",
      "test: step 360, loss 0.9583289623260498, accuracy 0.9677419066429138\n",
      "test: step 362, loss 0.8753525018692017, accuracy 0.9713261723518372\n",
      "test: step 364, loss 0.891717255115509, accuracy 0.9677419066429138\n",
      "test: step 366, loss 1.1582739353179932, accuracy 0.9713261723518372\n",
      "test: step 368, loss 2.0492942333221436, accuracy 0.9569892287254333\n",
      "test: step 370, loss 2.3159902095794678, accuracy 0.9462365508079529\n",
      "test: step 372, loss 1.3881440162658691, accuracy 0.9426523447036743\n",
      "test: step 374, loss 1.0896806716918945, accuracy 0.9498208165168762\n",
      "test: step 376, loss 1.5834861993789673, accuracy 0.9605734944343567\n",
      "test: step 378, loss 2.649944305419922, accuracy 0.9569892287254333\n",
      "test: step 380, loss 3.427980899810791, accuracy 0.9498208165168762\n",
      "test: step 382, loss 1.8929778337478638, accuracy 0.9605734944343567\n",
      "test: step 384, loss 1.8812800645828247, accuracy 0.9354838728904724\n",
      "test: step 386, loss 2.893719434738159, accuracy 0.9605734944343567\n",
      "test: step 388, loss 4.172964096069336, accuracy 0.9605734944343567\n",
      "test: step 390, loss 4.8421807289123535, accuracy 0.9426523447036743\n",
      "test: step 392, loss 4.262328147888184, accuracy 0.9605734944343567\n",
      "test: step 394, loss 2.886692523956299, accuracy 0.9569892287254333\n",
      "test: step 396, loss 2.9771652221679688, accuracy 0.9426523447036743\n",
      "test: step 398, loss 3.989783763885498, accuracy 0.9247311949729919\n",
      "test: step 400, loss 3.1862263679504395, accuracy 0.9426523447036743\n",
      "test: step 402, loss 3.539029359817505, accuracy 0.9534050226211548\n",
      "test: step 404, loss 5.189330101013184, accuracy 0.9498208165168762\n",
      "test: step 406, loss 3.596341371536255, accuracy 0.9534050226211548\n",
      "test: step 408, loss 3.000101089477539, accuracy 0.9605734944343567\n",
      "test: step 410, loss 4.102646350860596, accuracy 0.9354838728904724\n",
      "test: step 412, loss 3.4682905673980713, accuracy 0.9462365508079529\n",
      "test: step 414, loss 3.7869856357574463, accuracy 0.9498208165168762\n",
      "test: step 416, loss 4.319709300994873, accuracy 0.9605734944343567\n",
      "test: step 418, loss 4.297161102294922, accuracy 0.9569892287254333\n",
      "test: step 420, loss 4.01657247543335, accuracy 0.9569892287254333\n",
      "test: step 422, loss 4.4377288818359375, accuracy 0.9641577005386353\n",
      "test: step 424, loss 4.880953788757324, accuracy 0.9534050226211548\n",
      "test: step 426, loss 2.8420820236206055, accuracy 0.9677419066429138\n",
      "test: step 428, loss 2.863830089569092, accuracy 0.9605734944343567\n",
      "test: step 430, loss 2.8660154342651367, accuracy 0.9605734944343567\n",
      "test: step 432, loss 3.186612367630005, accuracy 0.9749103784561157\n",
      "test: step 434, loss 3.737793445587158, accuracy 0.9641577005386353\n",
      "test: step 436, loss 3.5735490322113037, accuracy 0.9713261723518372\n",
      "test: step 438, loss 3.401890754699707, accuracy 0.9749103784561157\n",
      "test: step 440, loss 3.5843286514282227, accuracy 0.9713261723518372\n",
      "test: step 442, loss 3.530973196029663, accuracy 0.9677419066429138\n",
      "test: step 444, loss 3.507988452911377, accuracy 0.9677419066429138\n",
      "test: step 446, loss 3.264387607574463, accuracy 0.9677419066429138\n",
      "test: step 448, loss 3.0997769832611084, accuracy 0.9641577005386353\n",
      "test: step 450, loss 2.697978973388672, accuracy 0.9713261723518372\n",
      "test: step 452, loss 2.798024892807007, accuracy 0.9784946441650391\n",
      "test: step 454, loss 2.481037139892578, accuracy 0.9784946441650391\n",
      "test: step 456, loss 2.156532049179077, accuracy 0.9784946441650391\n",
      "test: step 458, loss 2.1657488346099854, accuracy 0.9749103784561157\n",
      "test: step 460, loss 2.0739424228668213, accuracy 0.9713261723518372\n",
      "test: step 462, loss 2.007866859436035, accuracy 0.9605734944343567\n",
      "test: step 464, loss 2.3659610748291016, accuracy 0.9677419066429138\n",
      "test: step 466, loss 1.9412347078323364, accuracy 0.9677419066429138\n",
      "test: step 468, loss 2.769002914428711, accuracy 0.9354838728904724\n",
      "test: step 470, loss 2.7815165519714355, accuracy 0.9713261723518372\n",
      "test: step 472, loss 6.804771423339844, accuracy 0.9139785170555115\n",
      "test: step 474, loss 3.057758331298828, accuracy 0.9784946441650391\n",
      "test: step 476, loss 3.107182502746582, accuracy 0.9713261723518372\n",
      "test: step 478, loss 3.3798394203186035, accuracy 0.9677419066429138\n",
      "test: step 480, loss 3.608322858810425, accuracy 0.9677419066429138\n",
      "test: step 482, loss 3.8004767894744873, accuracy 0.9677419066429138\n",
      "test: step 484, loss 4.017617702484131, accuracy 0.9677419066429138\n",
      "test: step 486, loss 4.212899208068848, accuracy 0.9713261723518372\n",
      "test: step 488, loss 4.41172981262207, accuracy 0.9749103784561157\n",
      "test: step 490, loss 4.481034278869629, accuracy 0.9641577005386353\n",
      "test: step 492, loss 4.6224684715271, accuracy 0.9569892287254333\n",
      "test: step 494, loss 4.8234639167785645, accuracy 0.9534050226211548\n",
      "test: step 496, loss 4.912839889526367, accuracy 0.9569892287254333\n",
      "test: step 498, loss 4.9068756103515625, accuracy 0.9605734944343567\n",
      "test: step 500, loss 4.952051639556885, accuracy 0.9749103784561157\n",
      "test: step 502, loss 5.191336154937744, accuracy 0.9749103784561157\n",
      "test: step 504, loss 5.389916896820068, accuracy 0.9749103784561157\n",
      "test: step 506, loss 5.426538467407227, accuracy 0.9749103784561157\n",
      "test: step 508, loss 5.2531538009643555, accuracy 0.9641577005386353\n",
      "test: step 510, loss 6.2532243728637695, accuracy 0.9605734944343567\n",
      "test: step 512, loss 7.310631275177002, accuracy 0.9426523447036743\n",
      "test: step 514, loss 5.476212501525879, accuracy 0.9641577005386353\n",
      "test: step 516, loss 5.629504203796387, accuracy 0.9749103784561157\n",
      "test: step 518, loss 5.982044696807861, accuracy 0.9713261723518372\n",
      "test: step 520, loss 5.516124248504639, accuracy 0.9784946441650391\n",
      "test: step 520, loss 5.516124248504639, accuracy 0.9784946441650391\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.90      0.94        50\n",
      "          1       0.98      1.00      0.99       229\n",
      "\n",
      "avg / total       0.98      0.98      0.98       279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Net = textCnnTrain()\n",
    "main(Net)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
