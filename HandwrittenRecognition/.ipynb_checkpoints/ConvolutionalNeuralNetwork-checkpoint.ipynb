{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/fashion/t10k-labels-idx1-ubyte.gz\n",
      "(110000, 784) (110000,)\n",
      "(107800, 28, 28) (107800,)\n",
      "(2200, 28, 28) (2200,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "data = input_data.read_data_sets('./data/fashion')\n",
    "sample = data.train.next_batch(110000)\n",
    "dataset, dataLabel = sample[0], sample[1]\n",
    "\n",
    "train_data, test_data, train_label, test_label = train_test_split(dataset, dataLabel, test_size=0.02)\n",
    "train_data, test_data = train_data.reshape(-1, 28, 28), test_data.reshape(-1, 28, 28)\n",
    "\n",
    "print(dataset.shape, dataLabel.shape)\n",
    "print(train_data.shape, train_label.shape)\n",
    "print(test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 1, 2, 8]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Label\tDescription\n",
    "0\tT-shirt/top  1\tTrouser  2\tPullover  3\tDress 4\tCoat  5\tSandal 6\tShirt 7\tSneaker  8\tBag 9\tAnkle boot\n",
    "'''\n",
    "\n",
    "[test_label[10], test_label[20], test_label[30], test_label[40]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAD8CAYAAAABraMFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRVF8iS5+iqL4El38FEXxJbr4KYriS/4/yT4+4NzKshUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13981ceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(test_data[10], cmap=plt.get_cmap('PuBuGn_r'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(test_data[20], cmap=plt.get_cmap('PuBuGn_r'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(test_data[30], cmap=plt.get_cmap('PuBuGn_r'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(test_data[40], cmap=plt.get_cmap('PuBuGn_r'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ConvolutionalNeuralNetwork(object):\n",
    "    def __init__(self, filter_size=5, num_filters=6, num_classes=10):\n",
    "        self.data = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name=\"data\")\n",
    "        self.label = tf.placeholder(tf.int32, shape=[None, 10], name=\"label\")\n",
    "        \n",
    "        with tf.name_scope(\"one\"):\n",
    "            filter_shape = [filter_size, filter_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            self.conv1 = tf.nn.conv2d(input=self.data, filter=W, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "            self.pool1 = tf.nn.avg_pool(value=self.conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')            \n",
    "            \n",
    "        with tf.name_scope(\"two\"):\n",
    "            filter_shape = [filter_size, filter_size, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            self.conv2 = tf.nn.conv2d(input=self.pool1, filter=W, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "            self.pool2 = tf.nn.avg_pool(value=self.conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "            \n",
    "        with tf.name_scope(\"full_connected_layer\"):\n",
    "            self.feature = tf.reshape(self.pool2, [-1, 160])\n",
    "            W = tf.Variable(tf.truncated_normal(shape=[160, num_classes], stddev=0.1), name=\"full_connected_layer_W\")\n",
    "            b = tf.Variable(tf.truncated_normal(shape=[num_classes], stddev=0.1), name=\"full_connected_layer_b\")\n",
    "            self.logits = tf.nn.xw_plus_b(self.feature, W, b)\n",
    "        \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.predict = tf.arg_max(self.logits, dimension=1)\n",
    "            self.equal_tmp = tf.equal(self.predict, tf.arg_max(self.label, dimension=1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.equal_tmp, dtype=tf.float16))\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.label, logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(self.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch(epoches, batch_size):\n",
    "    data = list(zip(train_data, train_label))\n",
    "    for epoch in range(epoches):\n",
    "        random.shuffle(data)\n",
    "        for batch in range(0, len(data), batch_size):\n",
    "            if batch + batch_size >= len(data):\n",
    "                yield data[batch: len(data)]\n",
    "            else:\n",
    "                yield data[batch: (batch + batch_size)]\n",
    "\n",
    "class ConvolutionalNeuralNetworkTrain(object):\n",
    "    def __init__(self):\n",
    "        # 定义CNN网络，对话窗口以及optimizer\n",
    "        self.sess = tf.Session()\n",
    "        self.CNN = ConvolutionalNeuralNetwork(filter_size=5, num_filters=10)\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.CNN.loss, global_step=self.global_step)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.batches = get_batch(5, 300)\n",
    "\n",
    "        # tensorboard\n",
    "        tf.summary.scalar(\"loss\", self.CNN.loss)\n",
    "        tf.summary.scalar(\"accuracy\", self.CNN.accuracy)\n",
    "        self.merged_summary_op_train = tf.summary.merge_all()\n",
    "        self.merged_summary_op_test = tf.summary.merge_all()\n",
    "        self.summary_writer_train = tf.summary.FileWriter(\"./summary/train\", graph=self.sess.graph)\n",
    "        self.summary_writer_test = tf.summary.FileWriter(\"./summary/test\", graph=self.sess.graph)\n",
    "\n",
    "\n",
    "    def train_step(self, batch, label):\n",
    "        feed_dict = {\n",
    "            self.CNN.data: batch,\n",
    "            self.CNN.label: label\n",
    "        }\n",
    "        _, summary, step, loss, accuracy = self.sess.run(\n",
    "            fetches=[self.optimizer, self.merged_summary_op_train, self.global_step,\n",
    "                     self.CNN.loss, self.CNN.accuracy],\n",
    "            feed_dict=feed_dict)\n",
    "        self.summary_writer_train.add_summary(summary, step)\n",
    "\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {}, accuracy {}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "    def dev_step(self, batch, label):\n",
    "        feed_dict = {\n",
    "            self.CNN.data: batch,\n",
    "            self.CNN.label: label\n",
    "        }\n",
    "        summary, step, loss, accuracy = self.sess.run(\n",
    "            fetches=[self.merged_summary_op_test, self.global_step, self.CNN.loss, self.CNN.accuracy],\n",
    "            feed_dict=feed_dict)\n",
    "        self.summary_writer_test.add_summary(summary, step)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, accuracy {}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "    def main(self):\n",
    "        def one_hot_encoder(ipt):\n",
    "            zeros_ipt = np.zeros(shape=[len(ipt), 10], dtype=int)\n",
    "            zeros_ipt[range(len(ipt)), np.array(ipt)] = 1\n",
    "            return zeros_ipt\n",
    "            \n",
    "        for data in self.batches:\n",
    "            x_train, y_train = zip(*data)\n",
    "            x_train = np.expand_dims(np.array(x_train, dtype=float), -1)\n",
    "            \n",
    "            y_train = one_hot_encoder(y_train)\n",
    "            self.train_step(x_train, y_train)\n",
    "            \n",
    "            current_step = tf.train.global_step(self.sess, self.global_step)\n",
    "            if current_step % 20 == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                self.dev_step(np.expand_dims(np.array(test_data, dtype=float), -1), one_hot_encoder(test_label))\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-10T00:47:57.688800: step 1, loss 2.2975542545318604, accuracy 0.106689453125\n",
      "2019-04-10T00:47:57.740864: step 2, loss 2.3523011207580566, accuracy 0.08001708984375\n",
      "2019-04-10T00:47:57.786005: step 3, loss 2.173553466796875, accuracy 0.1400146484375\n",
      "2019-04-10T00:47:57.833485: step 4, loss 2.148407220840454, accuracy 0.1800537109375\n",
      "2019-04-10T00:47:57.879111: step 5, loss 1.9589015245437622, accuracy 0.38671875\n",
      "2019-04-10T00:47:57.927849: step 6, loss 1.8507270812988281, accuracy 0.5\n",
      "2019-04-10T00:47:57.976304: step 7, loss 1.6395161151885986, accuracy 0.456787109375\n",
      "2019-04-10T00:47:58.020429: step 8, loss 1.5037671327590942, accuracy 0.419921875\n",
      "2019-04-10T00:47:58.069523: step 9, loss 1.3204296827316284, accuracy 0.5166015625\n",
      "2019-04-10T00:47:58.115015: step 10, loss 1.2256733179092407, accuracy 0.493408203125\n",
      "2019-04-10T00:47:58.163233: step 11, loss 1.1415232419967651, accuracy 0.533203125\n",
      "2019-04-10T00:47:58.214590: step 12, loss 1.0963469743728638, accuracy 0.603515625\n",
      "2019-04-10T00:47:58.259856: step 13, loss 0.8909654021263123, accuracy 0.66015625\n",
      "2019-04-10T00:47:58.304363: step 14, loss 1.0008326768875122, accuracy 0.59326171875\n",
      "2019-04-10T00:47:58.349232: step 15, loss 1.1804442405700684, accuracy 0.56005859375\n",
      "2019-04-10T00:47:58.396433: step 16, loss 1.1426331996917725, accuracy 0.61669921875\n",
      "2019-04-10T00:47:58.450002: step 17, loss 0.8820949196815491, accuracy 0.66015625\n",
      "2019-04-10T00:47:58.495281: step 18, loss 0.9734305739402771, accuracy 0.6533203125\n",
      "2019-04-10T00:47:58.539950: step 19, loss 0.8397325873374939, accuracy 0.716796875\n",
      "2019-04-10T00:47:58.585273: step 20, loss 0.9221262335777283, accuracy 0.669921875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:47:58.831257: step 20, loss 0.823001, accuracy 0.70068359375\n",
      "\n",
      "\n",
      "2019-04-10T00:47:58.881846: step 21, loss 0.7818323373794556, accuracy 0.71337890625\n",
      "2019-04-10T00:47:58.928251: step 22, loss 0.7704372406005859, accuracy 0.7265625\n",
      "2019-04-10T00:47:58.973739: step 23, loss 0.809227466583252, accuracy 0.73681640625\n",
      "2019-04-10T00:47:59.019758: step 24, loss 0.8073253035545349, accuracy 0.7431640625\n",
      "2019-04-10T00:47:59.068925: step 25, loss 0.8235624432563782, accuracy 0.7333984375\n",
      "2019-04-10T00:47:59.118801: step 26, loss 0.8093610405921936, accuracy 0.68994140625\n",
      "2019-04-10T00:47:59.163243: step 27, loss 0.7884395718574524, accuracy 0.70654296875\n",
      "2019-04-10T00:47:59.208224: step 28, loss 0.7646459341049194, accuracy 0.6767578125\n",
      "2019-04-10T00:47:59.253662: step 29, loss 0.8648883104324341, accuracy 0.68310546875\n",
      "2019-04-10T00:47:59.303187: step 30, loss 0.9161430597305298, accuracy 0.6767578125\n",
      "2019-04-10T00:47:59.353295: step 31, loss 0.7831161022186279, accuracy 0.716796875\n",
      "2019-04-10T00:47:59.401466: step 32, loss 0.72822505235672, accuracy 0.740234375\n",
      "2019-04-10T00:47:59.447183: step 33, loss 0.7239101529121399, accuracy 0.7568359375\n",
      "2019-04-10T00:47:59.491760: step 34, loss 0.6966336369514465, accuracy 0.75\n",
      "2019-04-10T00:47:59.542258: step 35, loss 0.7149050831794739, accuracy 0.7333984375\n",
      "2019-04-10T00:47:59.592840: step 36, loss 0.7223504185676575, accuracy 0.7265625\n",
      "2019-04-10T00:47:59.639259: step 37, loss 0.6863269805908203, accuracy 0.71337890625\n",
      "2019-04-10T00:47:59.684171: step 38, loss 0.7000449895858765, accuracy 0.75\n",
      "2019-04-10T00:47:59.729967: step 39, loss 0.7151585817337036, accuracy 0.716796875\n",
      "2019-04-10T00:47:59.779479: step 40, loss 0.6571510434150696, accuracy 0.74658203125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:47:59.975385: step 40, loss 0.676071, accuracy 0.74658203125\n",
      "\n",
      "\n",
      "2019-04-10T00:48:00.030648: step 41, loss 0.7089859247207642, accuracy 0.7265625\n",
      "2019-04-10T00:48:00.085468: step 42, loss 0.5650883913040161, accuracy 0.80322265625\n",
      "2019-04-10T00:48:00.132228: step 43, loss 0.8460906744003296, accuracy 0.669921875\n",
      "2019-04-10T00:48:00.177477: step 44, loss 0.7917875647544861, accuracy 0.6767578125\n",
      "2019-04-10T00:48:00.228210: step 45, loss 0.7275708913803101, accuracy 0.72998046875\n",
      "2019-04-10T00:48:00.290536: step 46, loss 0.6513262391090393, accuracy 0.7265625\n",
      "2019-04-10T00:48:00.350736: step 47, loss 0.7232956886291504, accuracy 0.72998046875\n",
      "2019-04-10T00:48:00.413647: step 48, loss 0.6211153268814087, accuracy 0.75\n",
      "2019-04-10T00:48:00.470917: step 49, loss 0.6283425092697144, accuracy 0.76318359375\n",
      "2019-04-10T00:48:00.530699: step 50, loss 0.6593844294548035, accuracy 0.7734375\n",
      "2019-04-10T00:48:00.595497: step 51, loss 0.6622525453567505, accuracy 0.7568359375\n",
      "2019-04-10T00:48:00.656262: step 52, loss 0.702741801738739, accuracy 0.71337890625\n",
      "2019-04-10T00:48:00.713363: step 53, loss 0.7023449540138245, accuracy 0.77978515625\n",
      "2019-04-10T00:48:00.768759: step 54, loss 0.6705021262168884, accuracy 0.77978515625\n",
      "2019-04-10T00:48:00.831780: step 55, loss 0.7014196515083313, accuracy 0.759765625\n",
      "2019-04-10T00:48:00.889897: step 56, loss 0.6735915541648865, accuracy 0.740234375\n",
      "2019-04-10T00:48:00.946487: step 57, loss 0.6327753067016602, accuracy 0.75\n",
      "2019-04-10T00:48:01.001615: step 58, loss 0.7050357460975647, accuracy 0.7265625\n",
      "2019-04-10T00:48:01.068325: step 59, loss 0.6429051756858826, accuracy 0.75\n",
      "2019-04-10T00:48:01.118128: step 60, loss 0.6538503766059875, accuracy 0.73681640625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:01.341824: step 60, loss 0.655462, accuracy 0.77294921875\n",
      "\n",
      "\n",
      "2019-04-10T00:48:01.391182: step 61, loss 0.7053174376487732, accuracy 0.7333984375\n",
      "2019-04-10T00:48:01.436347: step 62, loss 0.601516842842102, accuracy 0.79345703125\n",
      "2019-04-10T00:48:01.483652: step 63, loss 0.5997486710548401, accuracy 0.77001953125\n",
      "2019-04-10T00:48:01.530409: step 64, loss 0.7640767097473145, accuracy 0.7265625\n",
      "2019-04-10T00:48:01.588147: step 65, loss 0.7369036674499512, accuracy 0.75\n",
      "2019-04-10T00:48:01.650796: step 66, loss 0.5999229550361633, accuracy 0.78662109375\n",
      "2019-04-10T00:48:01.706350: step 67, loss 0.5996294617652893, accuracy 0.78662109375\n",
      "2019-04-10T00:48:01.753507: step 68, loss 0.5730239152908325, accuracy 0.806640625\n",
      "2019-04-10T00:48:01.801773: step 69, loss 0.6880186200141907, accuracy 0.75341796875\n",
      "2019-04-10T00:48:01.860087: step 70, loss 0.6093937754631042, accuracy 0.78662109375\n",
      "2019-04-10T00:48:01.916266: step 71, loss 0.7071596384048462, accuracy 0.73681640625\n",
      "2019-04-10T00:48:01.967568: step 72, loss 0.7165961861610413, accuracy 0.783203125\n",
      "2019-04-10T00:48:02.018700: step 73, loss 0.729658305644989, accuracy 0.72998046875\n",
      "2019-04-10T00:48:02.079725: step 74, loss 0.6584144234657288, accuracy 0.75\n",
      "2019-04-10T00:48:02.127387: step 75, loss 0.682053804397583, accuracy 0.75\n",
      "2019-04-10T00:48:02.173178: step 76, loss 0.658173143863678, accuracy 0.7333984375\n",
      "2019-04-10T00:48:02.219276: step 77, loss 0.5669516324996948, accuracy 0.80322265625\n",
      "2019-04-10T00:48:02.272275: step 78, loss 0.6101772785186768, accuracy 0.796875\n",
      "2019-04-10T00:48:02.324289: step 79, loss 0.6358299851417542, accuracy 0.79345703125\n",
      "2019-04-10T00:48:02.368785: step 80, loss 0.6417267322540283, accuracy 0.78662109375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:02.585722: step 80, loss 0.633507, accuracy 0.77783203125\n",
      "\n",
      "\n",
      "2019-04-10T00:48:02.634162: step 81, loss 0.6485689878463745, accuracy 0.78662109375\n",
      "2019-04-10T00:48:02.680088: step 82, loss 0.6266584992408752, accuracy 0.7900390625\n",
      "2019-04-10T00:48:02.726030: step 83, loss 0.6816194653511047, accuracy 0.73681640625\n",
      "2019-04-10T00:48:02.770455: step 84, loss 0.6832378506660461, accuracy 0.759765625\n",
      "2019-04-10T00:48:02.819863: step 85, loss 0.6981470584869385, accuracy 0.73681640625\n",
      "2019-04-10T00:48:02.873184: step 86, loss 0.6301693916320801, accuracy 0.7265625\n",
      "2019-04-10T00:48:02.918986: step 87, loss 0.6390480399131775, accuracy 0.74658203125\n",
      "2019-04-10T00:48:02.964165: step 88, loss 0.616828978061676, accuracy 0.79345703125\n",
      "2019-04-10T00:48:03.014977: step 89, loss 0.7702881097793579, accuracy 0.72021484375\n",
      "2019-04-10T00:48:03.074537: step 90, loss 0.6296656131744385, accuracy 0.77978515625\n",
      "2019-04-10T00:48:03.139455: step 91, loss 0.6868099570274353, accuracy 0.7734375\n",
      "2019-04-10T00:48:03.197038: step 92, loss 0.5590704083442688, accuracy 0.77001953125\n",
      "2019-04-10T00:48:03.244380: step 93, loss 0.5770087242126465, accuracy 0.796875\n",
      "2019-04-10T00:48:03.290796: step 94, loss 0.5910910367965698, accuracy 0.7998046875\n",
      "2019-04-10T00:48:03.353209: step 95, loss 0.6166627407073975, accuracy 0.77978515625\n",
      "2019-04-10T00:48:03.410826: step 96, loss 0.5894786715507507, accuracy 0.78662109375\n",
      "2019-04-10T00:48:03.466468: step 97, loss 0.5762637257575989, accuracy 0.7998046875\n",
      "2019-04-10T00:48:03.517051: step 98, loss 0.5726127624511719, accuracy 0.7900390625\n",
      "2019-04-10T00:48:03.567709: step 99, loss 0.6231517195701599, accuracy 0.77001953125\n",
      "2019-04-10T00:48:03.613084: step 100, loss 0.5749390721321106, accuracy 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:03.818086: step 100, loss 0.592164, accuracy 0.79296875\n",
      "\n",
      "\n",
      "2019-04-10T00:48:03.867709: step 101, loss 0.5436033010482788, accuracy 0.796875\n",
      "2019-04-10T00:48:03.917319: step 102, loss 0.6135210394859314, accuracy 0.78662109375\n",
      "2019-04-10T00:48:03.971603: step 103, loss 0.564805805683136, accuracy 0.77978515625\n",
      "2019-04-10T00:48:04.027546: step 104, loss 0.5482974052429199, accuracy 0.82666015625\n",
      "2019-04-10T00:48:04.083878: step 105, loss 0.5772936344146729, accuracy 0.783203125\n",
      "2019-04-10T00:48:04.139487: step 106, loss 0.6359778046607971, accuracy 0.7734375\n",
      "2019-04-10T00:48:04.191314: step 107, loss 0.6580865383148193, accuracy 0.783203125\n",
      "2019-04-10T00:48:04.238179: step 108, loss 0.6452898383140564, accuracy 0.75\n",
      "2019-04-10T00:48:04.290587: step 109, loss 0.6190633773803711, accuracy 0.7734375\n",
      "2019-04-10T00:48:04.336846: step 110, loss 0.5465606451034546, accuracy 0.806640625\n",
      "2019-04-10T00:48:04.388855: step 111, loss 0.5730958580970764, accuracy 0.81689453125\n",
      "2019-04-10T00:48:04.445747: step 112, loss 0.5657298564910889, accuracy 0.7998046875\n",
      "2019-04-10T00:48:04.501932: step 113, loss 0.6326465606689453, accuracy 0.77001953125\n",
      "2019-04-10T00:48:04.557848: step 114, loss 0.48775166273117065, accuracy 0.830078125\n",
      "2019-04-10T00:48:04.613046: step 115, loss 0.651236355304718, accuracy 0.7568359375\n",
      "2019-04-10T00:48:04.665748: step 116, loss 0.6184678077697754, accuracy 0.7734375\n",
      "2019-04-10T00:48:04.719474: step 117, loss 0.5498729944229126, accuracy 0.7998046875\n",
      "2019-04-10T00:48:04.775090: step 118, loss 0.6213030219078064, accuracy 0.796875\n",
      "2019-04-10T00:48:04.831270: step 119, loss 0.5647278428077698, accuracy 0.79345703125\n",
      "2019-04-10T00:48:04.885816: step 120, loss 0.6272544264793396, accuracy 0.77001953125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:05.092147: step 120, loss 0.592048, accuracy 0.79541015625\n",
      "\n",
      "\n",
      "2019-04-10T00:48:05.147457: step 121, loss 0.5172370672225952, accuracy 0.8232421875\n",
      "2019-04-10T00:48:05.194067: step 122, loss 0.541862428188324, accuracy 0.77978515625\n",
      "2019-04-10T00:48:05.240068: step 123, loss 0.6762239933013916, accuracy 0.7734375\n",
      "2019-04-10T00:48:05.288895: step 124, loss 0.5498811602592468, accuracy 0.81982421875\n",
      "2019-04-10T00:48:05.348493: step 125, loss 0.5674433708190918, accuracy 0.7900390625\n",
      "2019-04-10T00:48:05.397588: step 126, loss 0.6210359930992126, accuracy 0.7666015625\n",
      "2019-04-10T00:48:05.442780: step 127, loss 0.559221625328064, accuracy 0.80322265625\n",
      "2019-04-10T00:48:05.487959: step 128, loss 0.5840499401092529, accuracy 0.7900390625\n",
      "2019-04-10T00:48:05.533990: step 129, loss 0.6792251467704773, accuracy 0.75\n",
      "2019-04-10T00:48:05.583213: step 130, loss 0.5444229245185852, accuracy 0.80322265625\n",
      "2019-04-10T00:48:05.643623: step 131, loss 0.510891854763031, accuracy 0.79345703125\n",
      "2019-04-10T00:48:05.698690: step 132, loss 0.5555206537246704, accuracy 0.796875\n",
      "2019-04-10T00:48:05.752479: step 133, loss 0.5808173418045044, accuracy 0.7900390625\n",
      "2019-04-10T00:48:05.810118: step 134, loss 0.5542429089546204, accuracy 0.8134765625\n",
      "2019-04-10T00:48:05.867048: step 135, loss 0.694016695022583, accuracy 0.75\n",
      "2019-04-10T00:48:05.930477: step 136, loss 0.5753498673439026, accuracy 0.81689453125\n",
      "2019-04-10T00:48:05.980319: step 137, loss 0.6832534670829773, accuracy 0.7734375\n",
      "2019-04-10T00:48:06.035105: step 138, loss 0.5836425423622131, accuracy 0.806640625\n",
      "2019-04-10T00:48:06.092282: step 139, loss 0.5668725371360779, accuracy 0.783203125\n",
      "2019-04-10T00:48:06.144797: step 140, loss 0.6191039085388184, accuracy 0.7666015625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:06.352711: step 140, loss 0.562608, accuracy 0.80859375\n",
      "\n",
      "\n",
      "2019-04-10T00:48:06.405513: step 141, loss 0.5919972658157349, accuracy 0.77685546875\n",
      "2019-04-10T00:48:06.454917: step 142, loss 0.4834541380405426, accuracy 0.83984375\n",
      "2019-04-10T00:48:06.506421: step 143, loss 0.5379638671875, accuracy 0.8134765625\n",
      "2019-04-10T00:48:06.555362: step 144, loss 0.5859125852584839, accuracy 0.79345703125\n",
      "2019-04-10T00:48:06.611234: step 145, loss 0.633271336555481, accuracy 0.7734375\n",
      "2019-04-10T00:48:06.661769: step 146, loss 0.5633867383003235, accuracy 0.81005859375\n",
      "2019-04-10T00:48:06.709772: step 147, loss 0.5358567833900452, accuracy 0.81982421875\n",
      "2019-04-10T00:48:06.756029: step 148, loss 0.5374729633331299, accuracy 0.7998046875\n",
      "2019-04-10T00:48:06.802066: step 149, loss 0.6519821286201477, accuracy 0.78662109375\n",
      "2019-04-10T00:48:06.854735: step 150, loss 0.543186366558075, accuracy 0.796875\n",
      "2019-04-10T00:48:06.905822: step 151, loss 0.5616859197616577, accuracy 0.8134765625\n",
      "2019-04-10T00:48:06.950003: step 152, loss 0.5984206795692444, accuracy 0.79345703125\n",
      "2019-04-10T00:48:06.996283: step 153, loss 0.5762297511100769, accuracy 0.78662109375\n",
      "2019-04-10T00:48:07.042475: step 154, loss 0.5951921343803406, accuracy 0.7900390625\n",
      "2019-04-10T00:48:07.090384: step 155, loss 0.5904823541641235, accuracy 0.7666015625\n",
      "2019-04-10T00:48:07.139566: step 156, loss 0.7042354345321655, accuracy 0.7900390625\n",
      "2019-04-10T00:48:07.185080: step 157, loss 0.6166715025901794, accuracy 0.8134765625\n",
      "2019-04-10T00:48:07.235096: step 158, loss 0.6851917505264282, accuracy 0.77685546875\n",
      "2019-04-10T00:48:07.284716: step 159, loss 0.48824894428253174, accuracy 0.81689453125\n",
      "2019-04-10T00:48:07.335589: step 160, loss 0.542829155921936, accuracy 0.830078125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:07.549553: step 160, loss 0.548573, accuracy 0.81640625\n",
      "\n",
      "\n",
      "2019-04-10T00:48:07.610262: step 161, loss 0.5356006026268005, accuracy 0.81982421875\n",
      "2019-04-10T00:48:07.667079: step 162, loss 0.5936626195907593, accuracy 0.783203125\n",
      "2019-04-10T00:48:07.719485: step 163, loss 0.5004960298538208, accuracy 0.81982421875\n",
      "2019-04-10T00:48:07.783325: step 164, loss 0.59211665391922, accuracy 0.8134765625\n",
      "2019-04-10T00:48:07.848823: step 165, loss 0.5716396570205688, accuracy 0.8134765625\n",
      "2019-04-10T00:48:07.910926: step 166, loss 0.4660366177558899, accuracy 0.81689453125\n",
      "2019-04-10T00:48:07.968904: step 167, loss 0.5548021197319031, accuracy 0.8134765625\n",
      "2019-04-10T00:48:08.035061: step 168, loss 0.578125, accuracy 0.7998046875\n",
      "2019-04-10T00:48:08.094775: step 169, loss 0.5519306659698486, accuracy 0.82666015625\n",
      "2019-04-10T00:48:08.145955: step 170, loss 0.47369012236595154, accuracy 0.8232421875\n",
      "2019-04-10T00:48:08.198751: step 171, loss 0.6324613094329834, accuracy 0.76318359375\n",
      "2019-04-10T00:48:08.248783: step 172, loss 0.6114626526832581, accuracy 0.77978515625\n",
      "2019-04-10T00:48:08.307503: step 173, loss 0.6266959309577942, accuracy 0.79345703125\n",
      "2019-04-10T00:48:08.367952: step 174, loss 0.5183807611465454, accuracy 0.7998046875\n",
      "2019-04-10T00:48:08.425146: step 175, loss 0.5962430834770203, accuracy 0.82666015625\n",
      "2019-04-10T00:48:08.487213: step 176, loss 0.6830782294273376, accuracy 0.759765625\n",
      "2019-04-10T00:48:08.549743: step 177, loss 0.5935108661651611, accuracy 0.80322265625\n",
      "2019-04-10T00:48:08.611526: step 178, loss 0.5318374037742615, accuracy 0.796875\n",
      "2019-04-10T00:48:08.665632: step 179, loss 0.5697021484375, accuracy 0.80322265625\n",
      "2019-04-10T00:48:08.715061: step 180, loss 0.5659266710281372, accuracy 0.8134765625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:08.949490: step 180, loss 0.541577, accuracy 0.81591796875\n",
      "\n",
      "\n",
      "2019-04-10T00:48:09.019121: step 181, loss 0.532443642616272, accuracy 0.81689453125\n",
      "2019-04-10T00:48:09.083944: step 182, loss 0.5223105549812317, accuracy 0.81689453125\n",
      "2019-04-10T00:48:09.141241: step 183, loss 0.6203093528747559, accuracy 0.80322265625\n",
      "2019-04-10T00:48:09.200998: step 184, loss 0.5084648728370667, accuracy 0.81005859375\n",
      "2019-04-10T00:48:09.256524: step 185, loss 0.44910532236099243, accuracy 0.83642578125\n",
      "2019-04-10T00:48:09.310028: step 186, loss 0.5588096976280212, accuracy 0.82666015625\n",
      "2019-04-10T00:48:09.360452: step 187, loss 0.6480798125267029, accuracy 0.77001953125\n",
      "2019-04-10T00:48:09.412880: step 188, loss 0.4714963734149933, accuracy 0.83349609375\n",
      "2019-04-10T00:48:09.468297: step 189, loss 0.4891613721847534, accuracy 0.806640625\n",
      "2019-04-10T00:48:09.520679: step 190, loss 0.68993079662323, accuracy 0.78662109375\n",
      "2019-04-10T00:48:09.565931: step 191, loss 0.5820584297180176, accuracy 0.7900390625\n",
      "2019-04-10T00:48:09.611540: step 192, loss 0.6042822003364563, accuracy 0.806640625\n",
      "2019-04-10T00:48:09.662476: step 193, loss 0.6099860668182373, accuracy 0.78662109375\n",
      "2019-04-10T00:48:09.711883: step 194, loss 0.4686892330646515, accuracy 0.8232421875\n",
      "2019-04-10T00:48:09.759269: step 195, loss 0.6363750696182251, accuracy 0.783203125\n",
      "2019-04-10T00:48:09.806979: step 196, loss 0.6939027905464172, accuracy 0.77001953125\n",
      "2019-04-10T00:48:09.853305: step 197, loss 0.5772096514701843, accuracy 0.80322265625\n",
      "2019-04-10T00:48:09.902422: step 198, loss 0.5058085322380066, accuracy 0.84326171875\n",
      "2019-04-10T00:48:09.952338: step 199, loss 0.581167459487915, accuracy 0.796875\n",
      "2019-04-10T00:48:10.001521: step 200, loss 0.6174691319465637, accuracy 0.77978515625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:10.203352: step 200, loss 0.546036, accuracy 0.81884765625\n",
      "\n",
      "\n",
      "2019-04-10T00:48:10.253306: step 201, loss 0.6089920997619629, accuracy 0.78662109375\n",
      "2019-04-10T00:48:10.298752: step 202, loss 0.47653746604919434, accuracy 0.81005859375\n",
      "2019-04-10T00:48:10.345686: step 203, loss 0.538459062576294, accuracy 0.82666015625\n",
      "2019-04-10T00:48:10.391134: step 204, loss 0.5720507502555847, accuracy 0.8232421875\n",
      "2019-04-10T00:48:10.440547: step 205, loss 0.5740944147109985, accuracy 0.8134765625\n",
      "2019-04-10T00:48:10.493570: step 206, loss 0.5699182748794556, accuracy 0.80322265625\n",
      "2019-04-10T00:48:10.539900: step 207, loss 0.6326779127120972, accuracy 0.78662109375\n",
      "2019-04-10T00:48:10.587988: step 208, loss 0.5208866000175476, accuracy 0.81689453125\n",
      "2019-04-10T00:48:10.633237: step 209, loss 0.6770141124725342, accuracy 0.77001953125\n",
      "2019-04-10T00:48:10.685869: step 210, loss 0.5083113312721252, accuracy 0.82666015625\n",
      "2019-04-10T00:48:10.735726: step 211, loss 0.43991729617118835, accuracy 0.83642578125\n",
      "2019-04-10T00:48:10.781504: step 212, loss 0.5468593835830688, accuracy 0.796875\n",
      "2019-04-10T00:48:10.828785: step 213, loss 0.6732027530670166, accuracy 0.7998046875\n",
      "2019-04-10T00:48:10.875009: step 214, loss 0.5867700576782227, accuracy 0.77978515625\n",
      "2019-04-10T00:48:10.926557: step 215, loss 0.4990761876106262, accuracy 0.8232421875\n",
      "2019-04-10T00:48:10.977423: step 216, loss 0.6145439147949219, accuracy 0.81689453125\n",
      "2019-04-10T00:48:11.026038: step 217, loss 0.6581325531005859, accuracy 0.7666015625\n",
      "2019-04-10T00:48:11.072218: step 218, loss 0.5945432782173157, accuracy 0.77978515625\n",
      "2019-04-10T00:48:11.117309: step 219, loss 0.5728318095207214, accuracy 0.81005859375\n",
      "2019-04-10T00:48:11.167992: step 220, loss 0.6178233027458191, accuracy 0.7734375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:11.369506: step 220, loss 0.557909, accuracy 0.814453125\n",
      "\n",
      "\n",
      "2019-04-10T00:48:11.421892: step 221, loss 0.5675458908081055, accuracy 0.7734375\n",
      "2019-04-10T00:48:11.471099: step 222, loss 0.6065382957458496, accuracy 0.7900390625\n",
      "2019-04-10T00:48:11.517534: step 223, loss 0.4979802072048187, accuracy 0.806640625\n",
      "2019-04-10T00:48:11.562272: step 224, loss 0.5718120336532593, accuracy 0.796875\n",
      "2019-04-10T00:48:11.607674: step 225, loss 0.6522592306137085, accuracy 0.7666015625\n",
      "2019-04-10T00:48:11.653986: step 226, loss 0.5044299364089966, accuracy 0.83642578125\n",
      "2019-04-10T00:48:11.707648: step 227, loss 0.6530340909957886, accuracy 0.74658203125\n",
      "2019-04-10T00:48:11.754265: step 228, loss 0.5146313905715942, accuracy 0.81689453125\n",
      "2019-04-10T00:48:11.798382: step 229, loss 0.690187931060791, accuracy 0.7568359375\n",
      "2019-04-10T00:48:11.843338: step 230, loss 0.5969809293746948, accuracy 0.7998046875\n",
      "2019-04-10T00:48:11.891952: step 231, loss 0.4962998926639557, accuracy 0.8134765625\n",
      "2019-04-10T00:48:11.943512: step 232, loss 0.5574783086776733, accuracy 0.830078125\n",
      "2019-04-10T00:48:11.989191: step 233, loss 0.5077559947967529, accuracy 0.7998046875\n",
      "2019-04-10T00:48:12.035841: step 234, loss 0.5038878321647644, accuracy 0.7998046875\n",
      "2019-04-10T00:48:12.081650: step 235, loss 0.5023182034492493, accuracy 0.81005859375\n",
      "2019-04-10T00:48:12.135629: step 236, loss 0.6206448078155518, accuracy 0.7734375\n",
      "2019-04-10T00:48:12.186288: step 237, loss 0.5864158868789673, accuracy 0.81689453125\n",
      "2019-04-10T00:48:12.234130: step 238, loss 0.650744616985321, accuracy 0.7900390625\n",
      "2019-04-10T00:48:12.281485: step 239, loss 0.663217306137085, accuracy 0.759765625\n",
      "2019-04-10T00:48:12.331822: step 240, loss 0.5653942227363586, accuracy 0.7998046875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:12.543607: step 240, loss 0.540228, accuracy 0.8115234375\n",
      "\n",
      "\n",
      "2019-04-10T00:48:12.596045: step 241, loss 0.4741221070289612, accuracy 0.8466796875\n",
      "2019-04-10T00:48:12.641434: step 242, loss 0.42473265528678894, accuracy 0.853515625\n",
      "2019-04-10T00:48:12.687260: step 243, loss 0.7059078216552734, accuracy 0.75\n",
      "2019-04-10T00:48:12.731557: step 244, loss 0.521115243434906, accuracy 0.8134765625\n",
      "2019-04-10T00:48:12.781254: step 245, loss 0.5834518671035767, accuracy 0.806640625\n",
      "2019-04-10T00:48:12.832869: step 246, loss 0.526342511177063, accuracy 0.81982421875\n",
      "2019-04-10T00:48:12.879885: step 247, loss 0.5220264196395874, accuracy 0.83984375\n",
      "2019-04-10T00:48:12.924993: step 248, loss 0.5325131416320801, accuracy 0.7900390625\n",
      "2019-04-10T00:48:12.969993: step 249, loss 0.5761728286743164, accuracy 0.80322265625\n",
      "2019-04-10T00:48:13.017505: step 250, loss 0.49145326018333435, accuracy 0.830078125\n",
      "2019-04-10T00:48:13.069679: step 251, loss 0.49722304940223694, accuracy 0.84326171875\n",
      "2019-04-10T00:48:13.116563: step 252, loss 0.6217775940895081, accuracy 0.75341796875\n",
      "2019-04-10T00:48:13.161009: step 253, loss 0.5976585149765015, accuracy 0.79345703125\n",
      "2019-04-10T00:48:13.206348: step 254, loss 0.5543181300163269, accuracy 0.80322265625\n",
      "2019-04-10T00:48:13.254190: step 255, loss 0.479740709066391, accuracy 0.83984375\n",
      "2019-04-10T00:48:13.306053: step 256, loss 0.6006686687469482, accuracy 0.80322265625\n",
      "2019-04-10T00:48:13.352737: step 257, loss 0.5182337164878845, accuracy 0.81689453125\n",
      "2019-04-10T00:48:13.397494: step 258, loss 0.598099946975708, accuracy 0.7900390625\n",
      "2019-04-10T00:48:13.442905: step 259, loss 0.6462461352348328, accuracy 0.783203125\n",
      "2019-04-10T00:48:13.490502: step 260, loss 0.5157400965690613, accuracy 0.77978515625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:13.685781: step 260, loss 0.551313, accuracy 0.80224609375\n",
      "\n",
      "\n",
      "2019-04-10T00:48:13.740074: step 261, loss 0.49152833223342896, accuracy 0.81005859375\n",
      "2019-04-10T00:48:13.790152: step 262, loss 0.5051055550575256, accuracy 0.82666015625\n",
      "2019-04-10T00:48:13.841974: step 263, loss 0.5260797739028931, accuracy 0.81689453125\n",
      "2019-04-10T00:48:13.888130: step 264, loss 0.4412527084350586, accuracy 0.83642578125\n",
      "2019-04-10T00:48:13.933615: step 265, loss 0.6036018133163452, accuracy 0.77978515625\n",
      "2019-04-10T00:48:13.981467: step 266, loss 0.5024769902229309, accuracy 0.81005859375\n",
      "2019-04-10T00:48:14.033916: step 267, loss 0.5401082038879395, accuracy 0.81982421875\n",
      "2019-04-10T00:48:14.079179: step 268, loss 0.6942430734634399, accuracy 0.7998046875\n",
      "2019-04-10T00:48:14.123497: step 269, loss 0.5037113428115845, accuracy 0.830078125\n",
      "2019-04-10T00:48:14.167935: step 270, loss 0.5567466020584106, accuracy 0.796875\n",
      "2019-04-10T00:48:14.215796: step 271, loss 0.5158462524414062, accuracy 0.830078125\n",
      "2019-04-10T00:48:14.267455: step 272, loss 0.5103872418403625, accuracy 0.8134765625\n",
      "2019-04-10T00:48:14.314010: step 273, loss 0.5988171100616455, accuracy 0.796875\n",
      "2019-04-10T00:48:14.359017: step 274, loss 0.4939057528972626, accuracy 0.853515625\n",
      "2019-04-10T00:48:14.404274: step 275, loss 0.5969517827033997, accuracy 0.78662109375\n",
      "2019-04-10T00:48:14.454192: step 276, loss 0.5936867594718933, accuracy 0.7998046875\n",
      "2019-04-10T00:48:14.504351: step 277, loss 0.633274495601654, accuracy 0.783203125\n",
      "2019-04-10T00:48:14.549584: step 278, loss 0.4564693570137024, accuracy 0.83642578125\n",
      "2019-04-10T00:48:14.593682: step 279, loss 0.6499884128570557, accuracy 0.806640625\n",
      "2019-04-10T00:48:14.638198: step 280, loss 0.6319589018821716, accuracy 0.783203125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:14.848483: step 280, loss 0.550047, accuracy 0.80859375\n",
      "\n",
      "\n",
      "2019-04-10T00:48:14.896780: step 281, loss 0.6130520701408386, accuracy 0.79345703125\n",
      "2019-04-10T00:48:14.941840: step 282, loss 0.5165271759033203, accuracy 0.7900390625\n",
      "2019-04-10T00:48:14.987329: step 283, loss 0.5179441571235657, accuracy 0.81689453125\n",
      "2019-04-10T00:48:15.031551: step 284, loss 0.6718144416809082, accuracy 0.7734375\n",
      "2019-04-10T00:48:15.079048: step 285, loss 0.6189215779304504, accuracy 0.7734375\n",
      "2019-04-10T00:48:15.129145: step 286, loss 0.5404756665229797, accuracy 0.80322265625\n",
      "2019-04-10T00:48:15.178291: step 287, loss 0.5300201177597046, accuracy 0.8134765625\n",
      "2019-04-10T00:48:15.223531: step 288, loss 0.5926446318626404, accuracy 0.783203125\n",
      "2019-04-10T00:48:15.268810: step 289, loss 0.5543426275253296, accuracy 0.7900390625\n",
      "2019-04-10T00:48:15.317734: step 290, loss 0.4938431680202484, accuracy 0.81689453125\n",
      "2019-04-10T00:48:15.368860: step 291, loss 0.6031001210212708, accuracy 0.77685546875\n",
      "2019-04-10T00:48:15.415844: step 292, loss 0.5094714164733887, accuracy 0.82666015625\n",
      "2019-04-10T00:48:15.459915: step 293, loss 0.5385192632675171, accuracy 0.7998046875\n",
      "2019-04-10T00:48:15.505147: step 294, loss 0.5192320346832275, accuracy 0.82666015625\n",
      "2019-04-10T00:48:15.552283: step 295, loss 0.459046870470047, accuracy 0.8564453125\n",
      "2019-04-10T00:48:15.602310: step 296, loss 0.47253066301345825, accuracy 0.83984375\n",
      "2019-04-10T00:48:15.650137: step 297, loss 0.4651694595813751, accuracy 0.8232421875\n",
      "2019-04-10T00:48:15.697763: step 298, loss 0.42071524262428284, accuracy 0.85009765625\n",
      "2019-04-10T00:48:15.742799: step 299, loss 0.6255831718444824, accuracy 0.78662109375\n",
      "2019-04-10T00:48:15.790606: step 300, loss 0.6336479783058167, accuracy 0.77685546875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:15.995679: step 300, loss 0.566425, accuracy 0.80126953125\n",
      "\n",
      "\n",
      "2019-04-10T00:48:16.047764: step 301, loss 0.43547987937927246, accuracy 0.81982421875\n",
      "2019-04-10T00:48:16.101010: step 302, loss 0.49982497096061707, accuracy 0.82666015625\n",
      "2019-04-10T00:48:16.146095: step 303, loss 0.4990478456020355, accuracy 0.806640625\n",
      "2019-04-10T00:48:16.192068: step 304, loss 0.5230937600135803, accuracy 0.81005859375\n",
      "2019-04-10T00:48:16.240701: step 305, loss 0.503309428691864, accuracy 0.8466796875\n",
      "2019-04-10T00:48:16.293261: step 306, loss 0.521742582321167, accuracy 0.81982421875\n",
      "2019-04-10T00:48:16.340585: step 307, loss 0.546262264251709, accuracy 0.83984375\n",
      "2019-04-10T00:48:16.386859: step 308, loss 0.7643029689788818, accuracy 0.7265625\n",
      "2019-04-10T00:48:16.431750: step 309, loss 0.4778684973716736, accuracy 0.83642578125\n",
      "2019-04-10T00:48:16.479273: step 310, loss 0.5092625021934509, accuracy 0.81982421875\n",
      "2019-04-10T00:48:16.532553: step 311, loss 0.5967344045639038, accuracy 0.7900390625\n",
      "2019-04-10T00:48:16.578775: step 312, loss 0.5292062163352966, accuracy 0.83349609375\n",
      "2019-04-10T00:48:16.624629: step 313, loss 0.6190804243087769, accuracy 0.806640625\n",
      "2019-04-10T00:48:16.670134: step 314, loss 0.6625305414199829, accuracy 0.7666015625\n",
      "2019-04-10T00:48:16.719880: step 315, loss 0.5936354398727417, accuracy 0.7900390625\n",
      "2019-04-10T00:48:16.769952: step 316, loss 0.571761429309845, accuracy 0.78662109375\n",
      "2019-04-10T00:48:16.817505: step 317, loss 0.5438175797462463, accuracy 0.82666015625\n",
      "2019-04-10T00:48:16.862646: step 318, loss 0.5791617035865784, accuracy 0.78662109375\n",
      "2019-04-10T00:48:16.909178: step 319, loss 0.5026760697364807, accuracy 0.81689453125\n",
      "2019-04-10T00:48:16.959577: step 320, loss 0.5251241326332092, accuracy 0.80322265625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:17.161667: step 320, loss 0.530923, accuracy 0.81201171875\n",
      "\n",
      "\n",
      "2019-04-10T00:48:17.215193: step 321, loss 0.48952701687812805, accuracy 0.83349609375\n",
      "2019-04-10T00:48:17.266048: step 322, loss 0.5100980401039124, accuracy 0.83349609375\n",
      "2019-04-10T00:48:17.313050: step 323, loss 0.47513070702552795, accuracy 0.81689453125\n",
      "2019-04-10T00:48:17.357889: step 324, loss 0.5539910197257996, accuracy 0.7900390625\n",
      "2019-04-10T00:48:17.406627: step 325, loss 0.4479275941848755, accuracy 0.83642578125\n",
      "2019-04-10T00:48:17.459601: step 326, loss 0.531746506690979, accuracy 0.81005859375\n",
      "2019-04-10T00:48:17.506225: step 327, loss 0.44071537256240845, accuracy 0.8466796875\n",
      "2019-04-10T00:48:17.552168: step 328, loss 0.5732192397117615, accuracy 0.81005859375\n",
      "2019-04-10T00:48:17.599356: step 329, loss 0.5742242932319641, accuracy 0.806640625\n",
      "2019-04-10T00:48:17.648613: step 330, loss 0.4869888424873352, accuracy 0.85009765625\n",
      "2019-04-10T00:48:17.702052: step 331, loss 0.44552555680274963, accuracy 0.83642578125\n",
      "2019-04-10T00:48:17.751830: step 332, loss 0.46875855326652527, accuracy 0.83642578125\n",
      "2019-04-10T00:48:17.802135: step 333, loss 0.6207062005996704, accuracy 0.80322265625\n",
      "2019-04-10T00:48:17.851844: step 334, loss 0.4868742823600769, accuracy 0.8466796875\n",
      "2019-04-10T00:48:17.904530: step 335, loss 0.5181179046630859, accuracy 0.82666015625\n",
      "2019-04-10T00:48:17.956600: step 336, loss 0.5055593252182007, accuracy 0.8134765625\n",
      "2019-04-10T00:48:18.003706: step 337, loss 0.5768797993659973, accuracy 0.80322265625\n",
      "2019-04-10T00:48:18.048538: step 338, loss 0.5194249749183655, accuracy 0.84326171875\n",
      "2019-04-10T00:48:18.097661: step 339, loss 0.5239658355712891, accuracy 0.81689453125\n",
      "2019-04-10T00:48:18.151456: step 340, loss 0.4722694456577301, accuracy 0.83984375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:18.355611: step 340, loss 0.533235, accuracy 0.818359375\n",
      "\n",
      "\n",
      "2019-04-10T00:48:18.410252: step 341, loss 0.4938793480396271, accuracy 0.83642578125\n",
      "2019-04-10T00:48:18.456264: step 342, loss 0.5465278625488281, accuracy 0.7900390625\n",
      "2019-04-10T00:48:18.501724: step 343, loss 0.4724034070968628, accuracy 0.830078125\n",
      "2019-04-10T00:48:18.547083: step 344, loss 0.5392455458641052, accuracy 0.83349609375\n",
      "2019-04-10T00:48:18.598089: step 345, loss 0.5530164241790771, accuracy 0.7998046875\n",
      "2019-04-10T00:48:18.648900: step 346, loss 0.5030871033668518, accuracy 0.83642578125\n",
      "2019-04-10T00:48:18.697041: step 347, loss 0.5976006984710693, accuracy 0.796875\n",
      "2019-04-10T00:48:18.741677: step 348, loss 0.4581189453601837, accuracy 0.83984375\n",
      "2019-04-10T00:48:18.786371: step 349, loss 0.5181444883346558, accuracy 0.81005859375\n",
      "2019-04-10T00:48:18.835717: step 350, loss 0.4932461678981781, accuracy 0.80322265625\n",
      "2019-04-10T00:48:18.886430: step 351, loss 0.518399178981781, accuracy 0.8232421875\n",
      "2019-04-10T00:48:18.935793: step 352, loss 0.5321973562240601, accuracy 0.82666015625\n",
      "2019-04-10T00:48:18.980751: step 353, loss 0.5573626756668091, accuracy 0.806640625\n",
      "2019-04-10T00:48:19.028385: step 354, loss 0.5494985580444336, accuracy 0.81005859375\n",
      "2019-04-10T00:48:19.077534: step 355, loss 0.552409291267395, accuracy 0.8232421875\n",
      "2019-04-10T00:48:19.128053: step 356, loss 0.5100749731063843, accuracy 0.830078125\n",
      "2019-04-10T00:48:19.177762: step 357, loss 0.47766321897506714, accuracy 0.81005859375\n",
      "2019-04-10T00:48:19.223327: step 358, loss 0.6635892987251282, accuracy 0.77685546875\n",
      "2019-04-10T00:48:19.269637: step 359, loss 0.45420029759407043, accuracy 0.853515625\n",
      "2019-04-10T00:48:19.301001: step 360, loss 0.6094837188720703, accuracy 0.81982421875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:48:19.537454: step 360, loss 0.528269, accuracy 0.8212890625\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model_train = ConvolutionalNeuralNetworkTrain()\n",
    "aaa = model_train.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
