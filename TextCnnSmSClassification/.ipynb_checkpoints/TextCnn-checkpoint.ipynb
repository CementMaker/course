{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "df_data = pd.read_table(\"./data/SMSSpamCollection\", header=None, sep='\\t')\n",
    "df_data.columns = ['label', 'sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'ham': 4825, 'spam': 747})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "label, sentences = df_data['label'], df_data['sentence']\n",
    "Counter(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyP1/AEOvqCMVgMoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1274f82b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "length = []\n",
    "for sentence in sentences:\n",
    "    length.append(len(sentence))\n",
    "    \n",
    "mat = dict(sorted(Counter(length).items(), key=lambda x: x[0]))\n",
    "x, y = list(mat.keys()),  list(mat.values())\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(label):\n",
    "    import numpy as np\n",
    "    one_hot_label = np.zeros(shape=[len(label), 2])\n",
    "    one_hot_label[np.arange(0, len(label)), label] = 1\n",
    "    return one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-91f0194fdf36>:15: VocabularyProcessor.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/tang/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:154: CategoricalVocabulary.__init__ (from tensorflow.contrib.learn.python.learn.preprocessing.categorical_vocabulary) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "WARNING:tensorflow:From /Users/tang/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/preprocessing/text.py:170: tokenizer (from tensorflow.contrib.learn.python.learn.preprocessing.text) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tensorflow/transform or tf.data.\n",
      "number of words : 5395\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.contrib import learn\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Feature(object):\n",
    "    def __init__(self):\n",
    "        self.train_df = pd.read_table(\"./data/SMSSpamCollection\", header=None, sep='\\t')\n",
    "        self.train_df.columns = ['label', 'sentence']\n",
    "        self.sentence = self.train_df['sentence'].values\n",
    "        self.label = [1 if label == 'ham' else 0 for label in self.train_df['label'].values]\n",
    "        self.one_hot_label = one_hot_encoder(self.label)\n",
    "\n",
    "        self.vocab_processor = learn.preprocessing.VocabularyProcessor(200, min_frequency=1)\n",
    "        self.all_context = np.array(list(self.vocab_processor.fit_transform(self.sentence)))\n",
    "\n",
    "        print(\"number of words :\", len(self.vocab_processor.vocabulary_))\n",
    "        self.train_data, self.dev_data, self.train_label, self.dev_label = \\\n",
    "            train_test_split(self.all_context, self.one_hot_label, test_size=0.05)\n",
    "        # print(\"shape of train data:\", self.train_data.shape)\n",
    "        # print(\"shape of dev data:\", self.dev_data.shape)\n",
    "        # print(\"shape of train label:\", self.train_label.shape)\n",
    "        # print(\"shape of dev label:\", self.dev_label.shape)\n",
    "\n",
    "feature = Feature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_batch(epoches, batch_size, data, label):\n",
    "    # train_x, train_y = pickle.load(open(\"corpus_train.pkl\", \"rb\"))\n",
    "    samples = list(zip(data, label))\n",
    "    random.shuffle(samples)\n",
    "    for epoch in range(epoches):\n",
    "        for batch in range(0, len(samples), batch_size):\n",
    "            if batch + batch_size < len(samples):\n",
    "                yield samples[batch: (batch + batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def train_step(model, batch, label):\n",
    "    feed_dict = {\n",
    "        model.model.input_sentence: batch,\n",
    "        model.model.label: label\n",
    "    }\n",
    "    _, summary, step, loss, accuracy, = model.sess.run(\n",
    "        fetches=[model.optimizer,\n",
    "                 model.merged_summary_train,\n",
    "                 model.global_step,\n",
    "                 model.model.loss,\n",
    "                 model.model.accuracy],\n",
    "        feed_dict=feed_dict)\n",
    "    model.summary_writer_train.add_summary(summary, step)\n",
    "    time_str = datetime.datetime.now().isoformat()\n",
    "    # print(\"{}: step {}, loss {}, accuracy {}\".format(time_str, step, loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dev_step(model, batch, label, return_predict=False):\n",
    "    feed_dict = {\n",
    "        model.model.input_sentence: batch,\n",
    "        model.model.label: label\n",
    "    }\n",
    "    summary, step, loss, accuracy, predict = model.sess.run(\n",
    "        fetches=[model.merged_summary_test,\n",
    "                 model.global_step,\n",
    "                 model.model.loss,\n",
    "                 model.model.accuracy,\n",
    "                 model.model.predictions],\n",
    "        feed_dict=feed_dict)\n",
    "    model.summary_writer_test.add_summary(summary, step)\n",
    "    print(\"test: step {}, loss {}, accuracy {}\".format(step, loss, accuracy))\n",
    "    if return_predict == 1:\n",
    "        return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class textCnn(object):\n",
    "    def __init__(self, sequence_length, vocab_size, embedding_size, filter_sizes, num_filters, num_classes=2):\n",
    "        # sequence_length： 句子长度\n",
    "        # vocab_size： 词表大小\n",
    "        # embedding_size： 词向量长度\n",
    "        # filter_sizes： 卷积核大小（纵向）\n",
    "        # num_filters：每个filter_size对应的卷积核个数\n",
    "        # num_classes：类别个数，默认为2\n",
    "        \n",
    "        \n",
    "        # 样本label，样本\n",
    "        self.label = tf.placeholder(tf.int32, [None, num_classes], name=\"label\")\n",
    "        self.input_sentence = tf.placeholder(tf.int32, [None, sequence_length], name=\"input\")\n",
    "\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "            filter_shape = [vocab_size, embedding_size]\n",
    "            # 使用截断正态分布初始化词向量\n",
    "            w = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"embedding_matrix\")\n",
    "            self.embedded = tf.nn.embedding_lookup(params=w, ids=self.input_sentence)\n",
    "            \n",
    "            # 卷积层输入\n",
    "            self.embedded_expand = tf.expand_dims(self.embedded, -1)\n",
    "            \n",
    "\n",
    "        pooled_outputs = [] # 保存卷积+池化之后特征\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-max-pool-%s\" % filter_size):\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"Conv_filter_%s\" % filter_size)\n",
    "                \n",
    "                # 使用卷积核\n",
    "                conv = tf.nn.conv2d(input=self.embedded_expand,\n",
    "                                    filter=W,\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                    padding=\"VALID\")\n",
    "                \n",
    "                # 使用最大值池化\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    value=conv,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "       \n",
    "        with tf.name_scope(\"full_connected_layer\"):\n",
    "            # 总的卷积核个数，每个卷积核产生的Feature Map经过池化层会变成一个值，卷积核的个数就是全连接的数字个数\n",
    "            num_filters_total = num_filters * len(filter_sizes) \n",
    "            self.h_pool = tf.concat(pooled_outputs, 3) # 矩阵合并\n",
    "            self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total]) # 句子的特征向量表示\n",
    "            \n",
    "            w = tf.Variable(tf.truncated_normal(shape=[num_filters_total, num_classes], stddev=0.1), name=\"w\")\n",
    "            b = tf.Variable(tf.truncated_normal(shape=[num_classes]), name=\"b\")\n",
    "            self.score = tf.nn.xw_plus_b(self.h_pool_flat, w, b)\n",
    "\n",
    "        # with tf.name_scope(\"softmax\"):\n",
    "        #     self.result = tf.nn.softmax(logits=self.score, axis=1, name='softmax')\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.real = tf.argmax(self.label, axis=1, name=\"real_label\")\n",
    "            self.predictions = tf.argmax(self.score, axis=1, name=\"predictions\")\n",
    "\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.label, logits=self.score)\n",
    "            self.loss = tf.reduce_mean(losses)\n",
    "\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, self.real)\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class textCnnTrain(object):\n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        self.model = textCnn(sequence_length=200,\n",
    "                             embedding_size=50,\n",
    "                             filter_sizes=[1, 2, 3],\n",
    "                             num_filters=10,\n",
    "                             num_classes=2,\n",
    "                             vocab_size=6000)\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.05).minimize(self.model.loss, global_step=self.global_step)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.feature = Feature()\n",
    "        self.train_data, self.dev_data, self.train_label, self.dev_label = \\\n",
    "            self.feature.train_data, self.feature.dev_data, self.feature.train_label, self.feature.dev_label\n",
    "        self.batches = get_batch(10, 100, self.train_data, self.train_label)\n",
    "\n",
    "        tf.summary.scalar('loss', self.model.loss)\n",
    "        tf.summary.scalar('accuracy', self.model.accuracy)\n",
    "        self.merged_summary_train = tf.summary.merge_all()\n",
    "        self.merged_summary_test = tf.summary.merge_all()\n",
    "        self.summary_writer_train = tf.summary.FileWriter(\"./summary/cnn_summary/train\", graph=self.sess.graph)\n",
    "        self.summary_writer_test = tf.summary.FileWriter(\"./summary/cnn_summary/test\", graph=self.sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def main(model):\n",
    "    for data in model.batches:\n",
    "        x_train, y_train = zip(*data)\n",
    "        train_step(model, x_train, y_train)\n",
    "        current_step = tf.train.global_step(model.sess, model.global_step)\n",
    "        if current_step % 2 == 0:\n",
    "            dev_step(model, model.dev_data, model.dev_label)\n",
    "\n",
    "    predict = dev_step(model, model.dev_data, model.dev_label, return_predict=True)\n",
    "    y_true = np.argmax(model.dev_label, axis=1)\n",
    "    print(classification_report(y_true, predict))\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of words : 5395\n",
      "test: step 2, loss 0.4165300130844116, accuracy 0.8709677457809448\n",
      "test: step 4, loss 0.4378064274787903, accuracy 0.8709677457809448\n",
      "test: step 6, loss 0.4265703558921814, accuracy 0.8458781242370605\n",
      "test: step 8, loss 0.4150092303752899, accuracy 0.8387096524238586\n",
      "test: step 10, loss 0.3847312927246094, accuracy 0.8458781242370605\n",
      "test: step 12, loss 0.3580191433429718, accuracy 0.8530465960502625\n",
      "test: step 14, loss 0.37403327226638794, accuracy 0.8387096524238586\n",
      "test: step 16, loss 0.3566538989543915, accuracy 0.8458781242370605\n",
      "test: step 18, loss 0.2395993024110794, accuracy 0.9103942513465881\n",
      "test: step 20, loss 0.25793036818504333, accuracy 0.9068100452423096\n",
      "test: step 22, loss 0.23932108283042908, accuracy 0.9139785170555115\n",
      "test: step 24, loss 0.22121243178844452, accuracy 0.9283154010772705\n",
      "test: step 26, loss 0.23091767728328705, accuracy 0.9139785170555115\n",
      "test: step 28, loss 0.2015511840581894, accuracy 0.9426523447036743\n",
      "test: step 30, loss 0.24775031208992004, accuracy 0.939068078994751\n",
      "test: step 32, loss 0.17429737746715546, accuracy 0.9569892287254333\n",
      "test: step 34, loss 0.15815573930740356, accuracy 0.9534050226211548\n",
      "test: step 36, loss 0.3116355240345001, accuracy 0.8781362175941467\n",
      "test: step 38, loss 0.14509545266628265, accuracy 0.9641577005386353\n",
      "test: step 40, loss 0.3123267590999603, accuracy 0.9247311949729919\n",
      "test: step 42, loss 0.43361103534698486, accuracy 0.9211469292640686\n",
      "test: step 44, loss 0.2685495913028717, accuracy 0.9498208165168762\n",
      "test: step 46, loss 0.19673234224319458, accuracy 0.9641577005386353\n",
      "test: step 48, loss 0.23291198909282684, accuracy 0.9605734944343567\n",
      "test: step 50, loss 0.2677936255931854, accuracy 0.9534050226211548\n",
      "test: step 52, loss 0.260838121175766, accuracy 0.9569892287254333\n",
      "test: step 54, loss 0.21776744723320007, accuracy 0.9677419066429138\n",
      "test: step 56, loss 0.19621708989143372, accuracy 0.9677419066429138\n",
      "test: step 58, loss 0.36682194471359253, accuracy 0.9211469292640686\n",
      "test: step 60, loss 0.1853729784488678, accuracy 0.9569892287254333\n",
      "test: step 62, loss 0.355428010225296, accuracy 0.9462365508079529\n",
      "test: step 64, loss 0.4469989538192749, accuracy 0.9426523447036743\n",
      "test: step 66, loss 0.21892738342285156, accuracy 0.9605734944343567\n",
      "test: step 68, loss 0.4135189950466156, accuracy 0.8888888955116272\n",
      "test: step 70, loss 0.7374082803726196, accuracy 0.8028674125671387\n",
      "test: step 72, loss 0.20855355262756348, accuracy 0.9749103784561157\n",
      "test: step 74, loss 0.4305833876132965, accuracy 0.9641577005386353\n",
      "test: step 76, loss 0.3571702837944031, accuracy 0.9677419066429138\n",
      "test: step 78, loss 0.2549397647380829, accuracy 0.9713261723518372\n",
      "test: step 80, loss 0.20689503848552704, accuracy 0.9749103784561157\n",
      "test: step 82, loss 0.23342034220695496, accuracy 0.9713261723518372\n",
      "test: step 84, loss 0.23680536448955536, accuracy 0.9713261723518372\n",
      "test: step 86, loss 0.20824867486953735, accuracy 0.9749103784561157\n",
      "test: step 88, loss 0.2689663767814636, accuracy 0.9677419066429138\n",
      "test: step 90, loss 0.29475465416908264, accuracy 0.9641577005386353\n",
      "test: step 92, loss 0.2081080675125122, accuracy 0.9677419066429138\n",
      "test: step 94, loss 0.192854106426239, accuracy 0.9641577005386353\n",
      "test: step 96, loss 0.2066727876663208, accuracy 0.9569892287254333\n",
      "test: step 98, loss 0.2154826670885086, accuracy 0.9462365508079529\n",
      "test: step 100, loss 0.21030235290527344, accuracy 0.9426523447036743\n",
      "test: step 102, loss 0.16797097027301788, accuracy 0.9605734944343567\n",
      "test: step 104, loss 0.2213919311761856, accuracy 0.9534050226211548\n",
      "test: step 106, loss 0.4331062436103821, accuracy 0.9247311949729919\n",
      "test: step 108, loss 0.38369303941726685, accuracy 0.939068078994751\n",
      "test: step 110, loss 0.16075311601161957, accuracy 0.9569892287254333\n",
      "test: step 112, loss 0.2828418016433716, accuracy 0.9211469292640686\n",
      "test: step 114, loss 0.1660073697566986, accuracy 0.9713261723518372\n",
      "test: step 116, loss 0.17860662937164307, accuracy 0.9749103784561157\n",
      "test: step 118, loss 0.5785306096076965, accuracy 0.9354838728904724\n",
      "test: step 120, loss 0.5492433309555054, accuracy 0.9426523447036743\n",
      "test: step 122, loss 0.23729905486106873, accuracy 0.9713261723518372\n",
      "test: step 124, loss 0.2848794460296631, accuracy 0.9605734944343567\n",
      "test: step 126, loss 0.2985933721065521, accuracy 0.9641577005386353\n",
      "test: step 128, loss 0.2862185537815094, accuracy 0.9605734944343567\n",
      "test: step 130, loss 0.39504295587539673, accuracy 0.9749103784561157\n",
      "test: step 132, loss 0.7956225275993347, accuracy 0.9426523447036743\n",
      "test: step 134, loss 1.046341896057129, accuracy 0.9462365508079529\n",
      "test: step 136, loss 0.4828347861766815, accuracy 0.9749103784561157\n",
      "test: step 138, loss 0.4738042652606964, accuracy 0.9569892287254333\n",
      "test: step 140, loss 0.6879775524139404, accuracy 0.9283154010772705\n",
      "test: step 142, loss 0.731741726398468, accuracy 0.9211469292640686\n",
      "test: step 144, loss 0.6222376227378845, accuracy 0.9462365508079529\n",
      "test: step 146, loss 0.5957976579666138, accuracy 0.9498208165168762\n",
      "test: step 148, loss 0.5994520783424377, accuracy 0.9534050226211548\n",
      "test: step 150, loss 0.6413426995277405, accuracy 0.9462365508079529\n",
      "test: step 152, loss 0.7080735564231873, accuracy 0.939068078994751\n",
      "test: step 154, loss 0.4645771384239197, accuracy 0.9498208165168762\n",
      "test: step 156, loss 0.647659182548523, accuracy 0.9426523447036743\n",
      "test: step 158, loss 0.6561069488525391, accuracy 0.9462365508079529\n",
      "test: step 160, loss 0.4472466707229614, accuracy 0.9534050226211548\n",
      "test: step 162, loss 0.46862760186195374, accuracy 0.9534050226211548\n",
      "test: step 164, loss 0.5752257108688354, accuracy 0.9426523447036743\n",
      "test: step 166, loss 0.4909104108810425, accuracy 0.9605734944343567\n",
      "test: step 168, loss 0.4877184331417084, accuracy 0.9641577005386353\n",
      "test: step 170, loss 0.588824450969696, accuracy 0.9534050226211548\n",
      "test: step 172, loss 0.5972048044204712, accuracy 0.9534050226211548\n",
      "test: step 174, loss 0.4587920904159546, accuracy 0.9534050226211548\n",
      "test: step 176, loss 0.4488579034805298, accuracy 0.9605734944343567\n",
      "test: step 178, loss 0.8598211407661438, accuracy 0.9462365508079529\n",
      "test: step 180, loss 0.34683701395988464, accuracy 0.9749103784561157\n",
      "test: step 182, loss 1.4622722864151, accuracy 0.856630802154541\n",
      "test: step 184, loss 0.5532600283622742, accuracy 0.9354838728904724\n",
      "test: step 186, loss 0.9078225493431091, accuracy 0.9534050226211548\n",
      "test: step 188, loss 2.4766688346862793, accuracy 0.8996415734291077\n",
      "test: step 190, loss 0.9513986110687256, accuracy 0.9534050226211548\n",
      "test: step 192, loss 0.4293258786201477, accuracy 0.9605734944343567\n",
      "test: step 194, loss 0.8955824971199036, accuracy 0.9068100452423096\n",
      "test: step 196, loss 2.6708571910858154, accuracy 0.7634408473968506\n",
      "test: step 198, loss 1.2138731479644775, accuracy 0.9032257795333862\n",
      "test: step 200, loss 0.8026474118232727, accuracy 0.9641577005386353\n",
      "test: step 202, loss 1.0478484630584717, accuracy 0.9641577005386353\n",
      "test: step 204, loss 1.3133984804153442, accuracy 0.9498208165168762\n",
      "test: step 206, loss 1.5381364822387695, accuracy 0.9426523447036743\n",
      "test: step 208, loss 2.086477518081665, accuracy 0.91756272315979\n",
      "test: step 210, loss 1.4352513551712036, accuracy 0.9283154010772705\n",
      "test: step 212, loss 1.0889233350753784, accuracy 0.9426523447036743\n",
      "test: step 214, loss 1.1490546464920044, accuracy 0.9247311949729919\n",
      "test: step 216, loss 0.9490249156951904, accuracy 0.9498208165168762\n",
      "test: step 218, loss 0.9079346656799316, accuracy 0.9534050226211548\n",
      "test: step 220, loss 0.9914205074310303, accuracy 0.9534050226211548\n",
      "test: step 222, loss 1.0158637762069702, accuracy 0.9569892287254333\n",
      "test: step 224, loss 1.1992454528808594, accuracy 0.9605734944343567\n",
      "test: step 226, loss 1.3152486085891724, accuracy 0.9641577005386353\n",
      "test: step 228, loss 1.47684907913208, accuracy 0.9569892287254333\n",
      "test: step 230, loss 1.5565241575241089, accuracy 0.9498208165168762\n",
      "test: step 232, loss 1.506861925125122, accuracy 0.9677419066429138\n",
      "test: step 234, loss 1.7150068283081055, accuracy 0.9605734944343567\n",
      "test: step 236, loss 1.6259554624557495, accuracy 0.9641577005386353\n",
      "test: step 238, loss 1.5988353490829468, accuracy 0.9677419066429138\n",
      "test: step 240, loss 1.6752989292144775, accuracy 0.9605734944343567\n",
      "test: step 242, loss 1.6167550086975098, accuracy 0.9641577005386353\n",
      "test: step 244, loss 1.5838474035263062, accuracy 0.9605734944343567\n",
      "test: step 246, loss 1.5607802867889404, accuracy 0.9605734944343567\n",
      "test: step 248, loss 1.500471830368042, accuracy 0.9641577005386353\n",
      "test: step 250, loss 1.4268180131912231, accuracy 0.9569892287254333\n",
      "test: step 252, loss 1.6952440738677979, accuracy 0.9605734944343567\n",
      "test: step 254, loss 2.267855167388916, accuracy 0.9534050226211548\n",
      "test: step 256, loss 1.5974359512329102, accuracy 0.9569892287254333\n",
      "test: step 258, loss 1.3590363264083862, accuracy 0.9534050226211548\n",
      "test: step 260, loss 1.6330128908157349, accuracy 0.9318996667861938\n",
      "test: step 262, loss 1.5714281797409058, accuracy 0.9354838728904724\n",
      "test: step 264, loss 1.3873138427734375, accuracy 0.939068078994751\n",
      "test: step 264, loss 1.3873138427734375, accuracy 0.939068078994751\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.71      0.89      0.79        36\n",
      "          1       0.98      0.95      0.96       243\n",
      "\n",
      "avg / total       0.95      0.94      0.94       279\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Net = textCnnTrain()\n",
    "main(Net)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
