{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/fashion/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/fashion/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/fashion/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/fashion/t10k-labels-idx1-ubyte.gz\n",
      "(110000, 784) (110000,)\n",
      "(107800, 28, 28) (107800,)\n",
      "(2200, 28, 28) (2200,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "data = input_data.read_data_sets('./data/fashion')\n",
    "sample = data.train.next_batch(110000)\n",
    "dataset, dataLabel = sample[0], sample[1]\n",
    "\n",
    "train_data, test_data, train_label, test_label = train_test_split(dataset, dataLabel, test_size=0.02)\n",
    "train_data, test_data = train_data.reshape(-1, 28, 28), test_data.reshape(-1, 28, 28)\n",
    "\n",
    "print(dataset.shape, dataLabel.shape)\n",
    "print(train_data.shape, train_label.shape)\n",
    "print(test_data.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAD8CAYAAAABraMFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRVF8iS5+iqL4El38FEXxJbr4KYriS/4/yT4+4NzKshUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13981ceb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(test_data[10], cmap=plt.get_cmap('PuBuGn_r'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(test_data[20], cmap=plt.get_cmap('PuBuGn_r'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(test_data[30], cmap=plt.get_cmap('PuBuGn_r'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(test_data[40], cmap=plt.get_cmap('PuBuGn_r'))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class ConvolutionalNeuralNetwork(object):\n",
    "    def __init__(self, filter_size=5, num_filters=6, num_classes=10):\n",
    "        self.data = tf.placeholder(tf.float32, shape=[None, 28, 28, 1], name=\"data\")\n",
    "        self.label = tf.placeholder(tf.int32, shape=[None, 10], name=\"label\")\n",
    "        \n",
    "        with tf.name_scope(\"one\"):\n",
    "            filter_shape = [filter_size, filter_size, 1, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            self.conv1 = tf.nn.conv2d(input=self.data, filter=W, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "            self.pool1 = tf.nn.avg_pool(value=self.conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')            \n",
    "            \n",
    "        with tf.name_scope(\"two\"):\n",
    "            filter_shape = [filter_size, filter_size, num_filters, num_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            self.conv2 = tf.nn.conv2d(input=self.pool1, filter=W, strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "            self.pool2 = tf.nn.avg_pool(value=self.conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='VALID')\n",
    "            \n",
    "        with tf.name_scope(\"full_connected_layer\"):\n",
    "            self.feature = tf.reshape(self.pool2, [-1, 160])\n",
    "            W = tf.Variable(tf.truncated_normal(shape=[160, num_classes], stddev=0.1), name=\"full_connected_layer_W\")\n",
    "            b = tf.Variable(tf.truncated_normal(shape=[num_classes], stddev=0.1), name=\"full_connected_layer_b\")\n",
    "            self.logits = tf.nn.xw_plus_b(self.feature, W, b)\n",
    "        \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            self.predict = tf.arg_max(self.logits, dimension=1)\n",
    "            self.equal_tmp = tf.equal(self.predict, tf.arg_max(self.label, dimension=1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.equal_tmp, dtype=tf.float16))\n",
    "            \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            self.losses = tf.nn.softmax_cross_entropy_with_logits_v2(labels=self.label, logits=self.logits)\n",
    "            self.loss = tf.reduce_mean(self.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch(epoches, batch_size):\n",
    "    data = list(zip(train_data, train_label))\n",
    "    for epoch in range(epoches):\n",
    "        random.shuffle(data)\n",
    "        for batch in range(0, len(data), batch_size):\n",
    "            if batch + batch_size >= len(data):\n",
    "                yield data[batch: len(data)]\n",
    "            else:\n",
    "                yield data[batch: (batch + batch_size)]\n",
    "\n",
    "class ConvolutionalNeuralNetworkTrain(object):\n",
    "    def __init__(self):\n",
    "        # 定义CNN网络，对话窗口以及optimizer\n",
    "        self.sess = tf.Session()\n",
    "        self.CNN = ConvolutionalNeuralNetwork(filter_size=5, num_filters=10)\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.CNN.loss, global_step=self.global_step)\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.batches = get_batch(5, 300)\n",
    "\n",
    "        # tensorboard\n",
    "        tf.summary.scalar(\"loss\", self.CNN.loss)\n",
    "        tf.summary.scalar(\"accuracy\", self.CNN.accuracy)\n",
    "        self.merged_summary_op_train = tf.summary.merge_all()\n",
    "        self.merged_summary_op_test = tf.summary.merge_all()\n",
    "        self.summary_writer_train = tf.summary.FileWriter(\"./summary/train\", graph=self.sess.graph)\n",
    "        self.summary_writer_test = tf.summary.FileWriter(\"./summary/test\", graph=self.sess.graph)\n",
    "\n",
    "\n",
    "    def train_step(self, batch, label):\n",
    "        feed_dict = {\n",
    "            self.CNN.data: batch,\n",
    "            self.CNN.label: label\n",
    "        }\n",
    "        _, summary, step, loss, accuracy = self.sess.run(\n",
    "            fetches=[self.optimizer, self.merged_summary_op_train, self.global_step,\n",
    "                     self.CNN.loss, self.CNN.accuracy],\n",
    "            feed_dict=feed_dict)\n",
    "        self.summary_writer_train.add_summary(summary, step)\n",
    "\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {}, accuracy {}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "    def dev_step(self, batch, label):\n",
    "        feed_dict = {\n",
    "            self.CNN.data: batch,\n",
    "            self.CNN.label: label\n",
    "        }\n",
    "        summary, step, loss, accuracy = self.sess.run(\n",
    "            fetches=[self.merged_summary_op_test, self.global_step, self.CNN.loss, self.CNN.accuracy],\n",
    "            feed_dict=feed_dict)\n",
    "        self.summary_writer_test.add_summary(summary, step)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: step {}, loss {:g}, accuracy {}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "    def main(self):\n",
    "        def one_hot_encoder(ipt):\n",
    "            zeros_ipt = np.zeros(shape=[len(ipt), 10], dtype=int)\n",
    "            zeros_ipt[range(len(ipt)), np.array(ipt)] = 1\n",
    "            return zeros_ipt\n",
    "            \n",
    "        for data in self.batches:\n",
    "            x_train, y_train = zip(*data)\n",
    "            x_train = np.expand_dims(np.array(x_train, dtype=float), -1)\n",
    "            \n",
    "            y_train = one_hot_encoder(y_train)\n",
    "            self.train_step(x_train, y_train)\n",
    "            \n",
    "            current_step = tf.train.global_step(self.sess, self.global_step)\n",
    "            if current_step % 20 == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                self.dev_step(np.expand_dims(np.array(test_data, dtype=float), -1), one_hot_encoder(test_label))\n",
    "                print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-04-10T00:49:57.762512: step 1, loss 2.346548557281494, accuracy 0.1400146484375\n",
      "2019-04-10T00:49:57.810258: step 2, loss 2.235687017440796, accuracy 0.0999755859375\n",
      "2019-04-10T00:49:57.856632: step 3, loss 2.105565071105957, accuracy 0.28662109375\n",
      "2019-04-10T00:49:57.902892: step 4, loss 1.895173192024231, accuracy 0.42333984375\n",
      "2019-04-10T00:49:57.949899: step 5, loss 1.661730408668518, accuracy 0.416748046875\n",
      "2019-04-10T00:49:58.000373: step 6, loss 1.4527477025985718, accuracy 0.56689453125\n",
      "2019-04-10T00:49:58.051097: step 7, loss 1.3320865631103516, accuracy 0.449951171875\n",
      "2019-04-10T00:49:58.097726: step 8, loss 1.3091408014297485, accuracy 0.54345703125\n",
      "2019-04-10T00:49:58.143625: step 9, loss 1.3012017011642456, accuracy 0.4599609375\n",
      "2019-04-10T00:49:58.189817: step 10, loss 1.110469937324524, accuracy 0.646484375\n",
      "2019-04-10T00:49:58.244024: step 11, loss 1.237715244293213, accuracy 0.56005859375\n",
      "2019-04-10T00:49:58.294754: step 12, loss 1.0434966087341309, accuracy 0.5732421875\n",
      "2019-04-10T00:49:58.340348: step 13, loss 1.22262704372406, accuracy 0.56689453125\n",
      "2019-04-10T00:49:58.388502: step 14, loss 1.0885257720947266, accuracy 0.58984375\n",
      "2019-04-10T00:49:58.434547: step 15, loss 0.9500367045402527, accuracy 0.60986328125\n",
      "2019-04-10T00:49:58.492174: step 16, loss 1.0826958417892456, accuracy 0.61328125\n",
      "2019-04-10T00:49:58.543017: step 17, loss 0.9470278024673462, accuracy 0.6064453125\n",
      "2019-04-10T00:49:58.591434: step 18, loss 0.9101176857948303, accuracy 0.61328125\n",
      "2019-04-10T00:49:58.638069: step 19, loss 0.8457885980606079, accuracy 0.6533203125\n",
      "2019-04-10T00:49:58.684940: step 20, loss 0.9379773736000061, accuracy 0.66357421875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:49:58.906591: step 20, loss 0.853424, accuracy 0.6875\n",
      "\n",
      "\n",
      "2019-04-10T00:49:58.954378: step 21, loss 0.9027118682861328, accuracy 0.6435546875\n",
      "2019-04-10T00:49:59.003321: step 22, loss 0.8317071199417114, accuracy 0.68994140625\n",
      "2019-04-10T00:49:59.051078: step 23, loss 0.9964704513549805, accuracy 0.6767578125\n",
      "2019-04-10T00:49:59.102919: step 24, loss 0.9163205027580261, accuracy 0.66015625\n",
      "2019-04-10T00:49:59.167593: step 25, loss 0.9056065082550049, accuracy 0.68994140625\n",
      "2019-04-10T00:49:59.235403: step 26, loss 0.8196953535079956, accuracy 0.70654296875\n",
      "2019-04-10T00:49:59.294720: step 27, loss 0.7601178884506226, accuracy 0.72998046875\n",
      "2019-04-10T00:49:59.354561: step 28, loss 0.7893727421760559, accuracy 0.69677734375\n",
      "2019-04-10T00:49:59.415792: step 29, loss 0.7233154773712158, accuracy 0.7265625\n",
      "2019-04-10T00:49:59.469970: step 30, loss 0.8013865947723389, accuracy 0.7431640625\n",
      "2019-04-10T00:49:59.523630: step 31, loss 0.7330112457275391, accuracy 0.73681640625\n",
      "2019-04-10T00:49:59.570528: step 32, loss 0.8422843217849731, accuracy 0.68310546875\n",
      "2019-04-10T00:49:59.616784: step 33, loss 0.882236659526825, accuracy 0.67333984375\n",
      "2019-04-10T00:49:59.678554: step 34, loss 0.9136602878570557, accuracy 0.64013671875\n",
      "2019-04-10T00:49:59.727278: step 35, loss 0.7453001141548157, accuracy 0.73681640625\n",
      "2019-04-10T00:49:59.773740: step 36, loss 0.7081859707832336, accuracy 0.7734375\n",
      "2019-04-10T00:49:59.819709: step 37, loss 0.8544391989707947, accuracy 0.7001953125\n",
      "2019-04-10T00:49:59.866057: step 38, loss 0.778226912021637, accuracy 0.7333984375\n",
      "2019-04-10T00:49:59.920427: step 39, loss 0.8256986141204834, accuracy 0.716796875\n",
      "2019-04-10T00:49:59.973596: step 40, loss 0.865186333656311, accuracy 0.68994140625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:00.181812: step 40, loss 0.717566, accuracy 0.74169921875\n",
      "\n",
      "\n",
      "2019-04-10T00:50:00.230977: step 41, loss 0.8455846905708313, accuracy 0.7001953125\n",
      "2019-04-10T00:50:00.277246: step 42, loss 0.6281142830848694, accuracy 0.78662109375\n",
      "2019-04-10T00:50:00.324746: step 43, loss 0.785769522190094, accuracy 0.70654296875\n",
      "2019-04-10T00:50:00.378877: step 44, loss 0.784174382686615, accuracy 0.66357421875\n",
      "2019-04-10T00:50:00.450375: step 45, loss 0.747592031955719, accuracy 0.72998046875\n",
      "2019-04-10T00:50:00.511605: step 46, loss 0.7619436383247375, accuracy 0.72998046875\n",
      "2019-04-10T00:50:00.570693: step 47, loss 0.693069577217102, accuracy 0.7734375\n",
      "2019-04-10T00:50:00.618034: step 48, loss 0.788557767868042, accuracy 0.69677734375\n",
      "2019-04-10T00:50:00.668102: step 49, loss 0.829521656036377, accuracy 0.6767578125\n",
      "2019-04-10T00:50:00.725605: step 50, loss 0.7183677554130554, accuracy 0.73681640625\n",
      "2019-04-10T00:50:00.773664: step 51, loss 0.7195427417755127, accuracy 0.72314453125\n",
      "2019-04-10T00:50:00.819763: step 52, loss 0.7254828810691833, accuracy 0.7333984375\n",
      "2019-04-10T00:50:00.867783: step 53, loss 0.713973879814148, accuracy 0.72998046875\n",
      "2019-04-10T00:50:00.923108: step 54, loss 0.6460510492324829, accuracy 0.73681640625\n",
      "2019-04-10T00:50:00.974505: step 55, loss 0.6454562544822693, accuracy 0.75\n",
      "2019-04-10T00:50:01.020778: step 56, loss 0.790533185005188, accuracy 0.7001953125\n",
      "2019-04-10T00:50:01.067369: step 57, loss 0.767595648765564, accuracy 0.693359375\n",
      "2019-04-10T00:50:01.114117: step 58, loss 0.6084922552108765, accuracy 0.783203125\n",
      "2019-04-10T00:50:01.167467: step 59, loss 0.6288632750511169, accuracy 0.78662109375\n",
      "2019-04-10T00:50:01.222528: step 60, loss 0.6070769429206848, accuracy 0.77685546875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:01.447749: step 60, loss 0.651705, accuracy 0.755859375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:01.500787: step 61, loss 0.7210040092468262, accuracy 0.75\n",
      "2019-04-10T00:50:01.547705: step 62, loss 0.6625862717628479, accuracy 0.77001953125\n",
      "2019-04-10T00:50:01.595977: step 63, loss 0.6972766518592834, accuracy 0.7333984375\n",
      "2019-04-10T00:50:01.642586: step 64, loss 0.582167387008667, accuracy 0.75\n",
      "2019-04-10T00:50:01.694784: step 65, loss 0.644456148147583, accuracy 0.77978515625\n",
      "2019-04-10T00:50:01.745597: step 66, loss 0.6252044439315796, accuracy 0.75341796875\n",
      "2019-04-10T00:50:01.792853: step 67, loss 0.7623064517974854, accuracy 0.71337890625\n",
      "2019-04-10T00:50:01.839781: step 68, loss 0.6942891478538513, accuracy 0.7099609375\n",
      "2019-04-10T00:50:01.885690: step 69, loss 0.5683292746543884, accuracy 0.80322265625\n",
      "2019-04-10T00:50:01.936706: step 70, loss 0.5900139212608337, accuracy 0.77001953125\n",
      "2019-04-10T00:50:01.987355: step 71, loss 0.6774734854698181, accuracy 0.75341796875\n",
      "2019-04-10T00:50:02.035470: step 72, loss 0.5418157577514648, accuracy 0.79345703125\n",
      "2019-04-10T00:50:02.081498: step 73, loss 0.6381391882896423, accuracy 0.7666015625\n",
      "2019-04-10T00:50:02.126528: step 74, loss 0.6028218269348145, accuracy 0.79345703125\n",
      "2019-04-10T00:50:02.177077: step 75, loss 0.590861439704895, accuracy 0.81982421875\n",
      "2019-04-10T00:50:02.238854: step 76, loss 0.6468493938446045, accuracy 0.77001953125\n",
      "2019-04-10T00:50:02.294624: step 77, loss 0.6775702238082886, accuracy 0.77978515625\n",
      "2019-04-10T00:50:02.362082: step 78, loss 0.6658070087432861, accuracy 0.759765625\n",
      "2019-04-10T00:50:02.424283: step 79, loss 0.6277145147323608, accuracy 0.77978515625\n",
      "2019-04-10T00:50:02.485931: step 80, loss 0.6125775575637817, accuracy 0.7734375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:02.700068: step 80, loss 0.636611, accuracy 0.7666015625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:02.758647: step 81, loss 0.5893362164497375, accuracy 0.7734375\n",
      "2019-04-10T00:50:02.816744: step 82, loss 0.6843894720077515, accuracy 0.75\n",
      "2019-04-10T00:50:02.867205: step 83, loss 0.6286171078681946, accuracy 0.75\n",
      "2019-04-10T00:50:02.917777: step 84, loss 0.5919377207756042, accuracy 0.796875\n",
      "2019-04-10T00:50:02.980360: step 85, loss 0.6960641741752625, accuracy 0.72021484375\n",
      "2019-04-10T00:50:03.040691: step 86, loss 0.6186287999153137, accuracy 0.783203125\n",
      "2019-04-10T00:50:03.092561: step 87, loss 0.7398320436477661, accuracy 0.69677734375\n",
      "2019-04-10T00:50:03.143016: step 88, loss 0.6265820860862732, accuracy 0.75\n",
      "2019-04-10T00:50:03.196830: step 89, loss 0.6834993362426758, accuracy 0.78662109375\n",
      "2019-04-10T00:50:03.255729: step 90, loss 0.6626380681991577, accuracy 0.80322265625\n",
      "2019-04-10T00:50:03.309762: step 91, loss 0.5745726227760315, accuracy 0.77978515625\n",
      "2019-04-10T00:50:03.363194: step 92, loss 0.7440622448921204, accuracy 0.72998046875\n",
      "2019-04-10T00:50:03.417901: step 93, loss 0.5505872368812561, accuracy 0.80322265625\n",
      "2019-04-10T00:50:03.466772: step 94, loss 0.6206517815589905, accuracy 0.75341796875\n",
      "2019-04-10T00:50:03.514435: step 95, loss 0.6037794351577759, accuracy 0.80322265625\n",
      "2019-04-10T00:50:03.561468: step 96, loss 0.5878294110298157, accuracy 0.783203125\n",
      "2019-04-10T00:50:03.612515: step 97, loss 0.5820006728172302, accuracy 0.796875\n",
      "2019-04-10T00:50:03.659209: step 98, loss 0.8124460577964783, accuracy 0.73681640625\n",
      "2019-04-10T00:50:03.708052: step 99, loss 0.7061685919761658, accuracy 0.76318359375\n",
      "2019-04-10T00:50:03.753781: step 100, loss 0.6666024923324585, accuracy 0.759765625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:03.963015: step 100, loss 0.59866, accuracy 0.79150390625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:04.018796: step 101, loss 0.5987464785575867, accuracy 0.796875\n",
      "2019-04-10T00:50:04.071424: step 102, loss 0.5752685070037842, accuracy 0.79345703125\n",
      "2019-04-10T00:50:04.124833: step 103, loss 0.5743191242218018, accuracy 0.8134765625\n",
      "2019-04-10T00:50:04.177400: step 104, loss 0.5675219297409058, accuracy 0.796875\n",
      "2019-04-10T00:50:04.232613: step 105, loss 0.6045377254486084, accuracy 0.796875\n",
      "2019-04-10T00:50:04.289572: step 106, loss 0.6179168820381165, accuracy 0.783203125\n",
      "2019-04-10T00:50:04.354543: step 107, loss 0.6730033159255981, accuracy 0.78662109375\n",
      "2019-04-10T00:50:04.423720: step 108, loss 0.5474300384521484, accuracy 0.806640625\n",
      "2019-04-10T00:50:04.477199: step 109, loss 0.6790100336074829, accuracy 0.7568359375\n",
      "2019-04-10T00:50:04.532170: step 110, loss 0.5247761607170105, accuracy 0.79345703125\n",
      "2019-04-10T00:50:04.581614: step 111, loss 0.6203452944755554, accuracy 0.7734375\n",
      "2019-04-10T00:50:04.630129: step 112, loss 0.6279672980308533, accuracy 0.78662109375\n",
      "2019-04-10T00:50:04.682670: step 113, loss 0.5156116485595703, accuracy 0.806640625\n",
      "2019-04-10T00:50:04.731712: step 114, loss 0.5850759148597717, accuracy 0.806640625\n",
      "2019-04-10T00:50:04.777845: step 115, loss 0.6146511435508728, accuracy 0.77685546875\n",
      "2019-04-10T00:50:04.823179: step 116, loss 0.6584834456443787, accuracy 0.7568359375\n",
      "2019-04-10T00:50:04.873507: step 117, loss 0.593161404132843, accuracy 0.783203125\n",
      "2019-04-10T00:50:04.920045: step 118, loss 0.5649294853210449, accuracy 0.83984375\n",
      "2019-04-10T00:50:04.967885: step 119, loss 0.5449188947677612, accuracy 0.81005859375\n",
      "2019-04-10T00:50:05.016972: step 120, loss 0.5918721556663513, accuracy 0.77978515625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:05.227954: step 120, loss 0.574907, accuracy 0.79833984375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:05.277818: step 121, loss 0.5591660141944885, accuracy 0.77978515625\n",
      "2019-04-10T00:50:05.337194: step 122, loss 0.58040452003479, accuracy 0.80322265625\n",
      "2019-04-10T00:50:05.393867: step 123, loss 0.6045098900794983, accuracy 0.77685546875\n",
      "2019-04-10T00:50:05.449161: step 124, loss 0.6259974837303162, accuracy 0.7734375\n",
      "2019-04-10T00:50:05.509734: step 125, loss 0.5562787055969238, accuracy 0.81005859375\n",
      "2019-04-10T00:50:05.563858: step 126, loss 0.5639621615409851, accuracy 0.77978515625\n",
      "2019-04-10T00:50:05.610425: step 127, loss 0.5774429440498352, accuracy 0.78662109375\n",
      "2019-04-10T00:50:05.661830: step 128, loss 0.6269647479057312, accuracy 0.7568359375\n",
      "2019-04-10T00:50:05.714678: step 129, loss 0.5468468070030212, accuracy 0.80322265625\n",
      "2019-04-10T00:50:05.761669: step 130, loss 0.5053420066833496, accuracy 0.81689453125\n",
      "2019-04-10T00:50:05.811702: step 131, loss 0.5846866965293884, accuracy 0.77001953125\n",
      "2019-04-10T00:50:05.870399: step 132, loss 0.6453715562820435, accuracy 0.77685546875\n",
      "2019-04-10T00:50:05.935443: step 133, loss 0.5386267304420471, accuracy 0.806640625\n",
      "2019-04-10T00:50:05.993040: step 134, loss 0.6887336373329163, accuracy 0.75341796875\n",
      "2019-04-10T00:50:06.055573: step 135, loss 0.5907490253448486, accuracy 0.7568359375\n",
      "2019-04-10T00:50:06.109432: step 136, loss 0.4722873866558075, accuracy 0.830078125\n",
      "2019-04-10T00:50:06.164281: step 137, loss 0.5956783890724182, accuracy 0.79345703125\n",
      "2019-04-10T00:50:06.224466: step 138, loss 0.6702036261558533, accuracy 0.759765625\n",
      "2019-04-10T00:50:06.277135: step 139, loss 0.6321926712989807, accuracy 0.80322265625\n",
      "2019-04-10T00:50:06.325568: step 140, loss 0.5903767943382263, accuracy 0.81005859375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:06.526384: step 140, loss 0.575037, accuracy 0.79833984375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:06.582188: step 141, loss 0.637503981590271, accuracy 0.74658203125\n",
      "2019-04-10T00:50:06.630170: step 142, loss 0.5699502825737, accuracy 0.77685546875\n",
      "2019-04-10T00:50:06.675616: step 143, loss 0.5198276042938232, accuracy 0.81982421875\n",
      "2019-04-10T00:50:06.723673: step 144, loss 0.6570081114768982, accuracy 0.76318359375\n",
      "2019-04-10T00:50:06.769014: step 145, loss 0.6121914982795715, accuracy 0.77685546875\n",
      "2019-04-10T00:50:06.820314: step 146, loss 0.5436683297157288, accuracy 0.806640625\n",
      "2019-04-10T00:50:06.866853: step 147, loss 0.6185560822486877, accuracy 0.77685546875\n",
      "2019-04-10T00:50:06.912215: step 148, loss 0.5964105129241943, accuracy 0.7998046875\n",
      "2019-04-10T00:50:06.958099: step 149, loss 0.5535477995872498, accuracy 0.81982421875\n",
      "2019-04-10T00:50:07.005907: step 150, loss 0.6103001236915588, accuracy 0.77978515625\n",
      "2019-04-10T00:50:07.055822: step 151, loss 0.5732378959655762, accuracy 0.7900390625\n",
      "2019-04-10T00:50:07.103661: step 152, loss 0.5386451482772827, accuracy 0.796875\n",
      "2019-04-10T00:50:07.152030: step 153, loss 0.48792794346809387, accuracy 0.84326171875\n",
      "2019-04-10T00:50:07.197450: step 154, loss 0.5696882605552673, accuracy 0.796875\n",
      "2019-04-10T00:50:07.244090: step 155, loss 0.6307132840156555, accuracy 0.77978515625\n",
      "2019-04-10T00:50:07.295196: step 156, loss 0.6688929796218872, accuracy 0.7900390625\n",
      "2019-04-10T00:50:07.342307: step 157, loss 0.5408249497413635, accuracy 0.796875\n",
      "2019-04-10T00:50:07.389974: step 158, loss 0.5781620740890503, accuracy 0.81982421875\n",
      "2019-04-10T00:50:07.436062: step 159, loss 0.5761227607727051, accuracy 0.830078125\n",
      "2019-04-10T00:50:07.490704: step 160, loss 0.6704902648925781, accuracy 0.73681640625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:07.735938: step 160, loss 0.558788, accuracy 0.80224609375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:07.800759: step 161, loss 0.5403871536254883, accuracy 0.81005859375\n",
      "2019-04-10T00:50:07.860422: step 162, loss 0.5445807576179504, accuracy 0.81982421875\n",
      "2019-04-10T00:50:07.908128: step 163, loss 0.6100954413414001, accuracy 0.783203125\n",
      "2019-04-10T00:50:07.956082: step 164, loss 0.5293784141540527, accuracy 0.78662109375\n",
      "2019-04-10T00:50:08.009986: step 165, loss 0.636424720287323, accuracy 0.7568359375\n",
      "2019-04-10T00:50:08.056926: step 166, loss 0.5664451122283936, accuracy 0.8134765625\n",
      "2019-04-10T00:50:08.104556: step 167, loss 0.6234005689620972, accuracy 0.783203125\n",
      "2019-04-10T00:50:08.151358: step 168, loss 0.4989714026451111, accuracy 0.82666015625\n",
      "2019-04-10T00:50:08.202669: step 169, loss 0.5655081272125244, accuracy 0.7998046875\n",
      "2019-04-10T00:50:08.252219: step 170, loss 0.6081281304359436, accuracy 0.7666015625\n",
      "2019-04-10T00:50:08.300019: step 171, loss 0.5218990445137024, accuracy 0.81005859375\n",
      "2019-04-10T00:50:08.346629: step 172, loss 0.5819700360298157, accuracy 0.7900390625\n",
      "2019-04-10T00:50:08.393778: step 173, loss 0.5942519903182983, accuracy 0.7734375\n",
      "2019-04-10T00:50:08.444363: step 174, loss 0.5176501870155334, accuracy 0.8134765625\n",
      "2019-04-10T00:50:08.493496: step 175, loss 0.5729166865348816, accuracy 0.8134765625\n",
      "2019-04-10T00:50:08.540311: step 176, loss 0.48100972175598145, accuracy 0.83349609375\n",
      "2019-04-10T00:50:08.587287: step 177, loss 0.4576478600502014, accuracy 0.80322265625\n",
      "2019-04-10T00:50:08.632993: step 178, loss 0.5137906074523926, accuracy 0.81689453125\n",
      "2019-04-10T00:50:08.684372: step 179, loss 0.4842282235622406, accuracy 0.8466796875\n",
      "2019-04-10T00:50:08.754488: step 180, loss 0.5582156181335449, accuracy 0.796875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:09.009350: step 180, loss 0.537343, accuracy 0.806640625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:09.069309: step 181, loss 0.5591787695884705, accuracy 0.7900390625\n",
      "2019-04-10T00:50:09.126807: step 182, loss 0.49406111240386963, accuracy 0.82666015625\n",
      "2019-04-10T00:50:09.183475: step 183, loss 0.5882688164710999, accuracy 0.7734375\n",
      "2019-04-10T00:50:09.242867: step 184, loss 0.6086418032646179, accuracy 0.77978515625\n",
      "2019-04-10T00:50:09.306771: step 185, loss 0.4749029576778412, accuracy 0.83349609375\n",
      "2019-04-10T00:50:09.369483: step 186, loss 0.4965025782585144, accuracy 0.7900390625\n",
      "2019-04-10T00:50:09.438028: step 187, loss 0.5306352972984314, accuracy 0.79345703125\n",
      "2019-04-10T00:50:09.510986: step 188, loss 0.6375965476036072, accuracy 0.783203125\n",
      "2019-04-10T00:50:09.575771: step 189, loss 0.5478461980819702, accuracy 0.8134765625\n",
      "2019-04-10T00:50:09.632128: step 190, loss 0.6564427018165588, accuracy 0.79345703125\n",
      "2019-04-10T00:50:09.693160: step 191, loss 0.6215476393699646, accuracy 0.759765625\n",
      "2019-04-10T00:50:09.753603: step 192, loss 0.6104137897491455, accuracy 0.7734375\n",
      "2019-04-10T00:50:09.814919: step 193, loss 0.5038169622421265, accuracy 0.796875\n",
      "2019-04-10T00:50:09.875886: step 194, loss 0.5691770315170288, accuracy 0.796875\n",
      "2019-04-10T00:50:09.929196: step 195, loss 0.6017917990684509, accuracy 0.81005859375\n",
      "2019-04-10T00:50:09.987732: step 196, loss 0.4870701730251312, accuracy 0.81689453125\n",
      "2019-04-10T00:50:10.048982: step 197, loss 0.678158700466156, accuracy 0.78662109375\n",
      "2019-04-10T00:50:10.100434: step 198, loss 0.7422577738761902, accuracy 0.75341796875\n",
      "2019-04-10T00:50:10.151328: step 199, loss 0.588611900806427, accuracy 0.79345703125\n",
      "2019-04-10T00:50:10.215180: step 200, loss 0.6069231033325195, accuracy 0.8134765625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:10.416371: step 200, loss 0.543339, accuracy 0.81103515625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:10.469071: step 201, loss 0.5269216895103455, accuracy 0.8134765625\n",
      "2019-04-10T00:50:10.523254: step 202, loss 0.5117359161376953, accuracy 0.84326171875\n",
      "2019-04-10T00:50:10.571522: step 203, loss 0.5306684970855713, accuracy 0.7734375\n",
      "2019-04-10T00:50:10.618179: step 204, loss 0.5537502765655518, accuracy 0.796875\n",
      "2019-04-10T00:50:10.670494: step 205, loss 0.5912236571311951, accuracy 0.7734375\n",
      "2019-04-10T00:50:10.723670: step 206, loss 0.682339608669281, accuracy 0.7900390625\n",
      "2019-04-10T00:50:10.771386: step 207, loss 0.6708390116691589, accuracy 0.7568359375\n",
      "2019-04-10T00:50:10.817572: step 208, loss 0.5664044618606567, accuracy 0.796875\n",
      "2019-04-10T00:50:10.863552: step 209, loss 0.6493118405342102, accuracy 0.76318359375\n",
      "2019-04-10T00:50:10.912151: step 210, loss 0.4836117625236511, accuracy 0.81982421875\n",
      "2019-04-10T00:50:10.968317: step 211, loss 0.5209974050521851, accuracy 0.84326171875\n",
      "2019-04-10T00:50:11.027787: step 212, loss 0.5184784531593323, accuracy 0.806640625\n",
      "2019-04-10T00:50:11.084616: step 213, loss 0.5837377905845642, accuracy 0.830078125\n",
      "2019-04-10T00:50:11.135759: step 214, loss 0.4926682412624359, accuracy 0.80322265625\n",
      "2019-04-10T00:50:11.189497: step 215, loss 0.5493360161781311, accuracy 0.81982421875\n",
      "2019-04-10T00:50:11.245319: step 216, loss 0.681288480758667, accuracy 0.77001953125\n",
      "2019-04-10T00:50:11.293519: step 217, loss 0.5297606587409973, accuracy 0.82666015625\n",
      "2019-04-10T00:50:11.340212: step 218, loss 0.5117816925048828, accuracy 0.830078125\n",
      "2019-04-10T00:50:11.392310: step 219, loss 0.436436265707016, accuracy 0.84326171875\n",
      "2019-04-10T00:50:11.449126: step 220, loss 0.5136662125587463, accuracy 0.83642578125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:11.663064: step 220, loss 0.556334, accuracy 0.82177734375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:11.718928: step 221, loss 0.63916015625, accuracy 0.77001953125\n",
      "2019-04-10T00:50:11.776120: step 222, loss 0.5026142597198486, accuracy 0.82666015625\n",
      "2019-04-10T00:50:11.824625: step 223, loss 0.5284043550491333, accuracy 0.7998046875\n",
      "2019-04-10T00:50:11.871236: step 224, loss 0.46118804812431335, accuracy 0.81005859375\n",
      "2019-04-10T00:50:11.921802: step 225, loss 0.6274353861808777, accuracy 0.77001953125\n",
      "2019-04-10T00:50:11.979304: step 226, loss 0.49832943081855774, accuracy 0.830078125\n",
      "2019-04-10T00:50:12.036454: step 227, loss 0.5348918437957764, accuracy 0.796875\n",
      "2019-04-10T00:50:12.089725: step 228, loss 0.460136353969574, accuracy 0.83642578125\n",
      "2019-04-10T00:50:12.143242: step 229, loss 0.5504348278045654, accuracy 0.83349609375\n",
      "2019-04-10T00:50:12.200390: step 230, loss 0.55803382396698, accuracy 0.79345703125\n",
      "2019-04-10T00:50:12.249698: step 231, loss 0.5766048431396484, accuracy 0.783203125\n",
      "2019-04-10T00:50:12.297304: step 232, loss 0.5352704524993896, accuracy 0.8134765625\n",
      "2019-04-10T00:50:12.373334: step 233, loss 0.4525085985660553, accuracy 0.84326171875\n",
      "2019-04-10T00:50:12.438797: step 234, loss 0.5215761065483093, accuracy 0.806640625\n",
      "2019-04-10T00:50:12.504173: step 235, loss 0.5139539241790771, accuracy 0.7900390625\n",
      "2019-04-10T00:50:12.561733: step 236, loss 0.5949805974960327, accuracy 0.81005859375\n",
      "2019-04-10T00:50:12.623607: step 237, loss 0.543978214263916, accuracy 0.7998046875\n",
      "2019-04-10T00:50:12.679720: step 238, loss 0.5254918336868286, accuracy 0.796875\n",
      "2019-04-10T00:50:12.735160: step 239, loss 0.5853655934333801, accuracy 0.77001953125\n",
      "2019-04-10T00:50:12.795503: step 240, loss 0.536984384059906, accuracy 0.83349609375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:13.007173: step 240, loss 0.548816, accuracy 0.81298828125\n",
      "\n",
      "\n",
      "2019-04-10T00:50:13.064225: step 241, loss 0.49606379866600037, accuracy 0.8232421875\n",
      "2019-04-10T00:50:13.128517: step 242, loss 0.6388747096061707, accuracy 0.77685546875\n",
      "2019-04-10T00:50:13.187114: step 243, loss 0.5505940914154053, accuracy 0.81982421875\n",
      "2019-04-10T00:50:13.250079: step 244, loss 0.5321435332298279, accuracy 0.806640625\n",
      "2019-04-10T00:50:13.316355: step 245, loss 0.5525957345962524, accuracy 0.7998046875\n",
      "2019-04-10T00:50:13.377029: step 246, loss 0.4889273941516876, accuracy 0.82666015625\n",
      "2019-04-10T00:50:13.431102: step 247, loss 0.5807313919067383, accuracy 0.7998046875\n",
      "2019-04-10T00:50:13.487904: step 248, loss 0.5701922178268433, accuracy 0.80322265625\n",
      "2019-04-10T00:50:13.545751: step 249, loss 0.5647749900817871, accuracy 0.796875\n",
      "2019-04-10T00:50:13.613640: step 250, loss 0.4513374865055084, accuracy 0.82666015625\n",
      "2019-04-10T00:50:13.674288: step 251, loss 0.5533992648124695, accuracy 0.80322265625\n",
      "2019-04-10T00:50:13.725431: step 252, loss 0.6032106280326843, accuracy 0.78662109375\n",
      "2019-04-10T00:50:13.776820: step 253, loss 0.5612200498580933, accuracy 0.806640625\n",
      "2019-04-10T00:50:13.824720: step 254, loss 0.4844288229942322, accuracy 0.83349609375\n",
      "2019-04-10T00:50:13.873429: step 255, loss 0.5938646197319031, accuracy 0.783203125\n",
      "2019-04-10T00:50:13.921371: step 256, loss 0.5943295955657959, accuracy 0.79345703125\n",
      "2019-04-10T00:50:13.968079: step 257, loss 0.5341506600379944, accuracy 0.80322265625\n",
      "2019-04-10T00:50:14.018471: step 258, loss 0.5352851748466492, accuracy 0.83642578125\n",
      "2019-04-10T00:50:14.064446: step 259, loss 0.6515096426010132, accuracy 0.75\n",
      "2019-04-10T00:50:14.113259: step 260, loss 0.5398464202880859, accuracy 0.7734375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:14.314141: step 260, loss 0.526606, accuracy 0.81494140625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:14.364856: step 261, loss 0.5493061542510986, accuracy 0.7900390625\n",
      "2019-04-10T00:50:14.411985: step 262, loss 0.6045234203338623, accuracy 0.81005859375\n",
      "2019-04-10T00:50:14.459344: step 263, loss 0.5967056155204773, accuracy 0.81982421875\n",
      "2019-04-10T00:50:14.507735: step 264, loss 0.6873329877853394, accuracy 0.78662109375\n",
      "2019-04-10T00:50:14.554334: step 265, loss 0.5577447414398193, accuracy 0.7998046875\n",
      "2019-04-10T00:50:14.604040: step 266, loss 0.5425668954849243, accuracy 0.80322265625\n",
      "2019-04-10T00:50:14.649939: step 267, loss 0.5787837505340576, accuracy 0.7900390625\n",
      "2019-04-10T00:50:14.697523: step 268, loss 0.5212045311927795, accuracy 0.8134765625\n",
      "2019-04-10T00:50:14.744793: step 269, loss 0.5907244086265564, accuracy 0.7568359375\n",
      "2019-04-10T00:50:14.802051: step 270, loss 0.5119039416313171, accuracy 0.806640625\n",
      "2019-04-10T00:50:14.848366: step 271, loss 0.5081436038017273, accuracy 0.8134765625\n",
      "2019-04-10T00:50:14.894813: step 272, loss 0.5721362233161926, accuracy 0.783203125\n",
      "2019-04-10T00:50:14.944946: step 273, loss 0.5673669576644897, accuracy 0.8134765625\n",
      "2019-04-10T00:50:14.993391: step 274, loss 0.4687681198120117, accuracy 0.82666015625\n",
      "2019-04-10T00:50:15.042842: step 275, loss 0.4371843934059143, accuracy 0.84326171875\n",
      "2019-04-10T00:50:15.089371: step 276, loss 0.6509791612625122, accuracy 0.78662109375\n",
      "2019-04-10T00:50:15.138558: step 277, loss 0.5638494491577148, accuracy 0.81005859375\n",
      "2019-04-10T00:50:15.185770: step 278, loss 0.5009448528289795, accuracy 0.7998046875\n",
      "2019-04-10T00:50:15.232784: step 279, loss 0.5363113880157471, accuracy 0.7998046875\n",
      "2019-04-10T00:50:15.282491: step 280, loss 0.5217994451522827, accuracy 0.81689453125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:15.481489: step 280, loss 0.534162, accuracy 0.8173828125\n",
      "\n",
      "\n",
      "2019-04-10T00:50:15.531897: step 281, loss 0.49788838624954224, accuracy 0.8232421875\n",
      "2019-04-10T00:50:15.580693: step 282, loss 0.5534241199493408, accuracy 0.81982421875\n",
      "2019-04-10T00:50:15.628308: step 283, loss 0.5290009379386902, accuracy 0.83349609375\n",
      "2019-04-10T00:50:15.675769: step 284, loss 0.468490332365036, accuracy 0.8232421875\n",
      "2019-04-10T00:50:15.724176: step 285, loss 0.534990668296814, accuracy 0.80322265625\n",
      "2019-04-10T00:50:15.770272: step 286, loss 0.42395028471946716, accuracy 0.85009765625\n",
      "2019-04-10T00:50:15.816816: step 287, loss 0.46962451934814453, accuracy 0.8134765625\n",
      "2019-04-10T00:50:15.862611: step 288, loss 0.5358526706695557, accuracy 0.7998046875\n",
      "2019-04-10T00:50:15.908128: step 289, loss 0.5145190954208374, accuracy 0.83349609375\n",
      "2019-04-10T00:50:15.956521: step 290, loss 0.5518943667411804, accuracy 0.7998046875\n",
      "2019-04-10T00:50:16.003392: step 291, loss 0.5989605784416199, accuracy 0.7734375\n",
      "2019-04-10T00:50:16.049279: step 292, loss 0.5705692172050476, accuracy 0.81005859375\n",
      "2019-04-10T00:50:16.097339: step 293, loss 0.5374376177787781, accuracy 0.83984375\n",
      "2019-04-10T00:50:16.142678: step 294, loss 0.5219078063964844, accuracy 0.830078125\n",
      "2019-04-10T00:50:16.190143: step 295, loss 0.6067302227020264, accuracy 0.78662109375\n",
      "2019-04-10T00:50:16.236187: step 296, loss 0.48503348231315613, accuracy 0.8466796875\n",
      "2019-04-10T00:50:16.282213: step 297, loss 0.45749175548553467, accuracy 0.81689453125\n",
      "2019-04-10T00:50:16.329097: step 298, loss 0.5886498689651489, accuracy 0.8134765625\n",
      "2019-04-10T00:50:16.374786: step 299, loss 0.4968525171279907, accuracy 0.83349609375\n",
      "2019-04-10T00:50:16.421800: step 300, loss 0.5383663177490234, accuracy 0.81005859375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:16.618362: step 300, loss 0.535578, accuracy 0.81640625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:16.668520: step 301, loss 0.554567813873291, accuracy 0.82666015625\n",
      "2019-04-10T00:50:16.713926: step 302, loss 0.5467790961265564, accuracy 0.81689453125\n",
      "2019-04-10T00:50:16.761090: step 303, loss 0.5480798482894897, accuracy 0.806640625\n",
      "2019-04-10T00:50:16.808086: step 304, loss 0.4846123158931732, accuracy 0.81982421875\n",
      "2019-04-10T00:50:16.853825: step 305, loss 0.5391290783882141, accuracy 0.81005859375\n",
      "2019-04-10T00:50:16.899854: step 306, loss 0.5001658201217651, accuracy 0.80322265625\n",
      "2019-04-10T00:50:16.945466: step 307, loss 0.5717398524284363, accuracy 0.7900390625\n",
      "2019-04-10T00:50:16.993751: step 308, loss 0.57381671667099, accuracy 0.80322265625\n",
      "2019-04-10T00:50:17.040665: step 309, loss 0.5155320763587952, accuracy 0.806640625\n",
      "2019-04-10T00:50:17.089615: step 310, loss 0.44103336334228516, accuracy 0.85986328125\n",
      "2019-04-10T00:50:17.135858: step 311, loss 0.48632022738456726, accuracy 0.830078125\n",
      "2019-04-10T00:50:17.181560: step 312, loss 0.50496906042099, accuracy 0.81689453125\n",
      "2019-04-10T00:50:17.230382: step 313, loss 0.5324784517288208, accuracy 0.79345703125\n",
      "2019-04-10T00:50:17.276081: step 314, loss 0.5124451518058777, accuracy 0.81689453125\n",
      "2019-04-10T00:50:17.324546: step 315, loss 0.4509103298187256, accuracy 0.81982421875\n",
      "2019-04-10T00:50:17.371355: step 316, loss 0.5000994205474854, accuracy 0.830078125\n",
      "2019-04-10T00:50:17.416393: step 317, loss 0.572422206401825, accuracy 0.7734375\n",
      "2019-04-10T00:50:17.462214: step 318, loss 0.46464285254478455, accuracy 0.83642578125\n",
      "2019-04-10T00:50:17.507697: step 319, loss 0.602723240852356, accuracy 0.8134765625\n",
      "2019-04-10T00:50:17.555451: step 320, loss 0.48320555686950684, accuracy 0.830078125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:17.753268: step 320, loss 0.528598, accuracy 0.82080078125\n",
      "\n",
      "\n",
      "2019-04-10T00:50:17.803057: step 321, loss 0.5246387124061584, accuracy 0.830078125\n",
      "2019-04-10T00:50:17.848590: step 322, loss 0.5249901413917542, accuracy 0.80322265625\n",
      "2019-04-10T00:50:17.895831: step 323, loss 0.5668725371360779, accuracy 0.796875\n",
      "2019-04-10T00:50:17.940849: step 324, loss 0.6134801506996155, accuracy 0.7568359375\n",
      "2019-04-10T00:50:17.988924: step 325, loss 0.5067500472068787, accuracy 0.81689453125\n",
      "2019-04-10T00:50:18.038119: step 326, loss 0.6272245645523071, accuracy 0.80322265625\n",
      "2019-04-10T00:50:18.085602: step 327, loss 0.5176330208778381, accuracy 0.81005859375\n",
      "2019-04-10T00:50:18.131550: step 328, loss 0.4838981032371521, accuracy 0.80322265625\n",
      "2019-04-10T00:50:18.177132: step 329, loss 0.5227493643760681, accuracy 0.8134765625\n",
      "2019-04-10T00:50:18.223573: step 330, loss 0.46689656376838684, accuracy 0.82666015625\n",
      "2019-04-10T00:50:18.271098: step 331, loss 0.44168227910995483, accuracy 0.81982421875\n",
      "2019-04-10T00:50:18.317779: step 332, loss 0.5092036724090576, accuracy 0.8232421875\n",
      "2019-04-10T00:50:18.363367: step 333, loss 0.5665231347084045, accuracy 0.78662109375\n",
      "2019-04-10T00:50:18.412013: step 334, loss 0.543802797794342, accuracy 0.81005859375\n",
      "2019-04-10T00:50:18.459147: step 335, loss 0.5848568677902222, accuracy 0.77685546875\n",
      "2019-04-10T00:50:18.507073: step 336, loss 0.4837169349193573, accuracy 0.83984375\n",
      "2019-04-10T00:50:18.553840: step 337, loss 0.550266444683075, accuracy 0.81689453125\n",
      "2019-04-10T00:50:18.601822: step 338, loss 0.571549654006958, accuracy 0.796875\n",
      "2019-04-10T00:50:18.648259: step 339, loss 0.47399771213531494, accuracy 0.830078125\n",
      "2019-04-10T00:50:18.694430: step 340, loss 0.6347943544387817, accuracy 0.79345703125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:18.893212: step 340, loss 0.525114, accuracy 0.81591796875\n",
      "\n",
      "\n",
      "2019-04-10T00:50:18.943099: step 341, loss 0.6277289390563965, accuracy 0.77685546875\n",
      "2019-04-10T00:50:18.989745: step 342, loss 0.5484161376953125, accuracy 0.783203125\n",
      "2019-04-10T00:50:19.040308: step 343, loss 0.6013554930686951, accuracy 0.78662109375\n",
      "2019-04-10T00:50:19.087941: step 344, loss 0.6119011044502258, accuracy 0.7998046875\n",
      "2019-04-10T00:50:19.136744: step 345, loss 0.5782403349876404, accuracy 0.80322265625\n",
      "2019-04-10T00:50:19.183436: step 346, loss 0.5489631295204163, accuracy 0.80322265625\n",
      "2019-04-10T00:50:19.232719: step 347, loss 0.5483424663543701, accuracy 0.83349609375\n",
      "2019-04-10T00:50:19.280094: step 348, loss 0.5673307180404663, accuracy 0.8232421875\n",
      "2019-04-10T00:50:19.325785: step 349, loss 0.5223793387413025, accuracy 0.806640625\n",
      "2019-04-10T00:50:19.372612: step 350, loss 0.521533191204071, accuracy 0.81982421875\n",
      "2019-04-10T00:50:19.418844: step 351, loss 0.5476734042167664, accuracy 0.77978515625\n",
      "2019-04-10T00:50:19.465858: step 352, loss 0.600690484046936, accuracy 0.806640625\n",
      "2019-04-10T00:50:19.512385: step 353, loss 0.46762964129447937, accuracy 0.83349609375\n",
      "2019-04-10T00:50:19.559544: step 354, loss 0.622029721736908, accuracy 0.81689453125\n",
      "2019-04-10T00:50:19.605968: step 355, loss 0.5908911824226379, accuracy 0.83642578125\n",
      "2019-04-10T00:50:19.651869: step 356, loss 0.47235891222953796, accuracy 0.84326171875\n",
      "2019-04-10T00:50:19.699196: step 357, loss 0.41262099146842957, accuracy 0.853515625\n",
      "2019-04-10T00:50:19.745429: step 358, loss 0.49272751808166504, accuracy 0.83642578125\n",
      "2019-04-10T00:50:19.793293: step 359, loss 0.5293650031089783, accuracy 0.81689453125\n",
      "2019-04-10T00:50:19.813366: step 360, loss 0.5528799295425415, accuracy 0.77978515625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:20.011546: step 360, loss 0.526857, accuracy 0.8193359375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:20.172637: step 361, loss 0.5108092427253723, accuracy 0.81005859375\n",
      "2019-04-10T00:50:20.236722: step 362, loss 0.4716506898403168, accuracy 0.8134765625\n",
      "2019-04-10T00:50:20.296530: step 363, loss 0.4652740955352783, accuracy 0.83642578125\n",
      "2019-04-10T00:50:20.354460: step 364, loss 0.5582185387611389, accuracy 0.80322265625\n",
      "2019-04-10T00:50:20.405853: step 365, loss 0.4944759011268616, accuracy 0.83984375\n",
      "2019-04-10T00:50:20.455650: step 366, loss 0.5026659369468689, accuracy 0.83642578125\n",
      "2019-04-10T00:50:20.509167: step 367, loss 0.4945078492164612, accuracy 0.7998046875\n",
      "2019-04-10T00:50:20.560310: step 368, loss 0.5449872612953186, accuracy 0.81005859375\n",
      "2019-04-10T00:50:20.616331: step 369, loss 0.45864182710647583, accuracy 0.830078125\n",
      "2019-04-10T00:50:20.668095: step 370, loss 0.44434016942977905, accuracy 0.8466796875\n",
      "2019-04-10T00:50:20.720060: step 371, loss 0.4945039451122284, accuracy 0.8232421875\n",
      "2019-04-10T00:50:20.770730: step 372, loss 0.5595515966415405, accuracy 0.79345703125\n",
      "2019-04-10T00:50:20.824048: step 373, loss 0.49344122409820557, accuracy 0.83642578125\n",
      "2019-04-10T00:50:20.873317: step 374, loss 0.5474680066108704, accuracy 0.81005859375\n",
      "2019-04-10T00:50:20.925959: step 375, loss 0.5724964141845703, accuracy 0.77001953125\n",
      "2019-04-10T00:50:20.978407: step 376, loss 0.5855879187583923, accuracy 0.796875\n",
      "2019-04-10T00:50:21.028379: step 377, loss 0.5099880695343018, accuracy 0.81982421875\n",
      "2019-04-10T00:50:21.081229: step 378, loss 0.45392149686813354, accuracy 0.83984375\n",
      "2019-04-10T00:50:21.131529: step 379, loss 0.58176189661026, accuracy 0.81689453125\n",
      "2019-04-10T00:50:21.183127: step 380, loss 0.5162858963012695, accuracy 0.830078125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:21.407991: step 380, loss 0.526644, accuracy 0.8193359375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:21.462130: step 381, loss 0.4305417835712433, accuracy 0.86328125\n",
      "2019-04-10T00:50:21.512511: step 382, loss 0.5047528147697449, accuracy 0.830078125\n",
      "2019-04-10T00:50:21.565571: step 383, loss 0.6087141036987305, accuracy 0.81005859375\n",
      "2019-04-10T00:50:21.616955: step 384, loss 0.5066484808921814, accuracy 0.8466796875\n",
      "2019-04-10T00:50:21.667551: step 385, loss 0.4019480347633362, accuracy 0.86328125\n",
      "2019-04-10T00:50:21.717609: step 386, loss 0.5470538139343262, accuracy 0.81982421875\n",
      "2019-04-10T00:50:21.769662: step 387, loss 0.43292853236198425, accuracy 0.84326171875\n",
      "2019-04-10T00:50:21.822266: step 388, loss 0.5446719527244568, accuracy 0.8134765625\n",
      "2019-04-10T00:50:21.872040: step 389, loss 0.5325525999069214, accuracy 0.8134765625\n",
      "2019-04-10T00:50:21.922967: step 390, loss 0.551920473575592, accuracy 0.7666015625\n",
      "2019-04-10T00:50:21.972631: step 391, loss 0.5035029649734497, accuracy 0.830078125\n",
      "2019-04-10T00:50:22.025498: step 392, loss 0.6201193332672119, accuracy 0.77978515625\n",
      "2019-04-10T00:50:22.077125: step 393, loss 0.453240305185318, accuracy 0.8466796875\n",
      "2019-04-10T00:50:22.127433: step 394, loss 0.515835702419281, accuracy 0.8232421875\n",
      "2019-04-10T00:50:22.178608: step 395, loss 0.5468441843986511, accuracy 0.8134765625\n",
      "2019-04-10T00:50:22.229787: step 396, loss 0.5484454035758972, accuracy 0.79345703125\n",
      "2019-04-10T00:50:22.282016: step 397, loss 0.532101571559906, accuracy 0.8466796875\n",
      "2019-04-10T00:50:22.334609: step 398, loss 0.5400553941726685, accuracy 0.8232421875\n",
      "2019-04-10T00:50:22.384826: step 399, loss 0.5005934238433838, accuracy 0.7998046875\n",
      "2019-04-10T00:50:22.435565: step 400, loss 0.4877329468727112, accuracy 0.81689453125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:22.652335: step 400, loss 0.523903, accuracy 0.822265625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:22.706452: step 401, loss 0.6137459874153137, accuracy 0.7900390625\n",
      "2019-04-10T00:50:22.757723: step 402, loss 0.6137516498565674, accuracy 0.78662109375\n",
      "2019-04-10T00:50:22.810368: step 403, loss 0.44185546040534973, accuracy 0.84326171875\n",
      "2019-04-10T00:50:22.861758: step 404, loss 0.7210113406181335, accuracy 0.7568359375\n",
      "2019-04-10T00:50:22.912483: step 405, loss 0.520775556564331, accuracy 0.830078125\n",
      "2019-04-10T00:50:22.962914: step 406, loss 0.49379095435142517, accuracy 0.83642578125\n",
      "2019-04-10T00:50:23.015400: step 407, loss 0.5430801510810852, accuracy 0.80322265625\n",
      "2019-04-10T00:50:23.067785: step 408, loss 0.5835070013999939, accuracy 0.77978515625\n",
      "2019-04-10T00:50:23.121416: step 409, loss 0.5301176905632019, accuracy 0.81689453125\n",
      "2019-04-10T00:50:23.173526: step 410, loss 0.518462061882019, accuracy 0.806640625\n",
      "2019-04-10T00:50:23.227003: step 411, loss 0.6208315491676331, accuracy 0.77685546875\n",
      "2019-04-10T00:50:23.279291: step 412, loss 0.6098710894584656, accuracy 0.80322265625\n",
      "2019-04-10T00:50:23.331142: step 413, loss 0.49419113993644714, accuracy 0.80322265625\n",
      "2019-04-10T00:50:23.383784: step 414, loss 0.5051584243774414, accuracy 0.81982421875\n",
      "2019-04-10T00:50:23.435758: step 415, loss 0.5512332916259766, accuracy 0.853515625\n",
      "2019-04-10T00:50:23.487327: step 416, loss 0.5042310357093811, accuracy 0.81689453125\n",
      "2019-04-10T00:50:23.537888: step 417, loss 0.5069555640220642, accuracy 0.80322265625\n",
      "2019-04-10T00:50:23.590196: step 418, loss 0.5323317646980286, accuracy 0.8134765625\n",
      "2019-04-10T00:50:23.640818: step 419, loss 0.5842865109443665, accuracy 0.79345703125\n",
      "2019-04-10T00:50:23.695436: step 420, loss 0.5527223944664001, accuracy 0.806640625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:23.907507: step 420, loss 0.507025, accuracy 0.82568359375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:23.961762: step 421, loss 0.5533924102783203, accuracy 0.806640625\n",
      "2019-04-10T00:50:24.015362: step 422, loss 0.505404531955719, accuracy 0.8466796875\n",
      "2019-04-10T00:50:24.071939: step 423, loss 0.5504273176193237, accuracy 0.78662109375\n",
      "2019-04-10T00:50:24.125713: step 424, loss 0.40388038754463196, accuracy 0.85986328125\n",
      "2019-04-10T00:50:24.177428: step 425, loss 0.45772063732147217, accuracy 0.84326171875\n",
      "2019-04-10T00:50:24.229678: step 426, loss 0.5150356292724609, accuracy 0.82666015625\n",
      "2019-04-10T00:50:24.286322: step 427, loss 0.5430980920791626, accuracy 0.806640625\n",
      "2019-04-10T00:50:24.342893: step 428, loss 0.5261236429214478, accuracy 0.82666015625\n",
      "2019-04-10T00:50:24.394274: step 429, loss 0.50189208984375, accuracy 0.83984375\n",
      "2019-04-10T00:50:24.448379: step 430, loss 0.5193107724189758, accuracy 0.8232421875\n",
      "2019-04-10T00:50:24.501931: step 431, loss 0.423312246799469, accuracy 0.853515625\n",
      "2019-04-10T00:50:24.554583: step 432, loss 0.49710577726364136, accuracy 0.86669921875\n",
      "2019-04-10T00:50:24.610685: step 433, loss 0.6023450493812561, accuracy 0.77685546875\n",
      "2019-04-10T00:50:24.664022: step 434, loss 0.5519272089004517, accuracy 0.83349609375\n",
      "2019-04-10T00:50:24.718991: step 435, loss 0.5078467130661011, accuracy 0.8232421875\n",
      "2019-04-10T00:50:24.773050: step 436, loss 0.49739760160446167, accuracy 0.83349609375\n",
      "2019-04-10T00:50:24.825940: step 437, loss 0.5413776636123657, accuracy 0.7998046875\n",
      "2019-04-10T00:50:24.878827: step 438, loss 0.5262331962585449, accuracy 0.830078125\n",
      "2019-04-10T00:50:24.932711: step 439, loss 0.49144119024276733, accuracy 0.8232421875\n",
      "2019-04-10T00:50:24.982585: step 440, loss 0.5247151851654053, accuracy 0.83642578125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:25.200978: step 440, loss 0.506729, accuracy 0.826171875\n",
      "\n",
      "\n",
      "2019-04-10T00:50:25.259133: step 441, loss 0.39480382204055786, accuracy 0.8466796875\n",
      "2019-04-10T00:50:25.311766: step 442, loss 0.615336000919342, accuracy 0.796875\n",
      "2019-04-10T00:50:25.363919: step 443, loss 0.5194416642189026, accuracy 0.82666015625\n",
      "2019-04-10T00:50:25.417955: step 444, loss 0.6523144245147705, accuracy 0.7666015625\n",
      "2019-04-10T00:50:25.473322: step 445, loss 0.5493171811103821, accuracy 0.80322265625\n",
      "2019-04-10T00:50:25.526876: step 446, loss 0.46440044045448303, accuracy 0.84326171875\n",
      "2019-04-10T00:50:25.583390: step 447, loss 0.5129601955413818, accuracy 0.81689453125\n",
      "2019-04-10T00:50:25.638850: step 448, loss 0.5664697885513306, accuracy 0.7998046875\n",
      "2019-04-10T00:50:25.696117: step 449, loss 0.5340621471405029, accuracy 0.806640625\n",
      "2019-04-10T00:50:25.749044: step 450, loss 0.4864235520362854, accuracy 0.8232421875\n",
      "2019-04-10T00:50:25.803043: step 451, loss 0.45949599146842957, accuracy 0.83349609375\n",
      "2019-04-10T00:50:25.857098: step 452, loss 0.47980162501335144, accuracy 0.83984375\n",
      "2019-04-10T00:50:25.910568: step 453, loss 0.467735230922699, accuracy 0.8564453125\n",
      "2019-04-10T00:50:25.964873: step 454, loss 0.5049921870231628, accuracy 0.83349609375\n",
      "2019-04-10T00:50:26.018622: step 455, loss 0.5004987120628357, accuracy 0.82666015625\n",
      "2019-04-10T00:50:26.073475: step 456, loss 0.43089064955711365, accuracy 0.83642578125\n",
      "2019-04-10T00:50:26.126960: step 457, loss 0.5237275958061218, accuracy 0.81982421875\n",
      "2019-04-10T00:50:26.181540: step 458, loss 0.3888159990310669, accuracy 0.8701171875\n",
      "2019-04-10T00:50:26.237080: step 459, loss 0.4686044752597809, accuracy 0.82666015625\n",
      "2019-04-10T00:50:26.289365: step 460, loss 0.49284932017326355, accuracy 0.82666015625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:26.516879: step 460, loss 0.51246, accuracy 0.82275390625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:26.569448: step 461, loss 0.5850628018379211, accuracy 0.7998046875\n",
      "2019-04-10T00:50:26.620781: step 462, loss 0.5076041221618652, accuracy 0.8134765625\n",
      "2019-04-10T00:50:26.673223: step 463, loss 0.5124677419662476, accuracy 0.81005859375\n",
      "2019-04-10T00:50:26.723947: step 464, loss 0.45346054434776306, accuracy 0.85009765625\n",
      "2019-04-10T00:50:26.776934: step 465, loss 0.610155463218689, accuracy 0.7998046875\n",
      "2019-04-10T00:50:26.827900: step 466, loss 0.5878066420555115, accuracy 0.77978515625\n",
      "2019-04-10T00:50:26.878923: step 467, loss 0.5680083632469177, accuracy 0.7666015625\n",
      "2019-04-10T00:50:26.926748: step 468, loss 0.4912026524543762, accuracy 0.81982421875\n",
      "2019-04-10T00:50:26.977981: step 469, loss 0.5812923908233643, accuracy 0.80322265625\n",
      "2019-04-10T00:50:27.028265: step 470, loss 0.5347350835800171, accuracy 0.8134765625\n",
      "2019-04-10T00:50:27.078724: step 471, loss 0.47346603870391846, accuracy 0.83642578125\n",
      "2019-04-10T00:50:27.128729: step 472, loss 0.46392303705215454, accuracy 0.86328125\n",
      "2019-04-10T00:50:27.175545: step 473, loss 0.5027222037315369, accuracy 0.8134765625\n",
      "2019-04-10T00:50:27.226103: step 474, loss 0.403270959854126, accuracy 0.88330078125\n",
      "2019-04-10T00:50:27.275266: step 475, loss 0.5071497559547424, accuracy 0.82666015625\n",
      "2019-04-10T00:50:27.327704: step 476, loss 0.5589033365249634, accuracy 0.79345703125\n",
      "2019-04-10T00:50:27.376373: step 477, loss 0.4341970384120941, accuracy 0.83642578125\n",
      "2019-04-10T00:50:27.426004: step 478, loss 0.564757764339447, accuracy 0.81689453125\n",
      "2019-04-10T00:50:27.476191: step 479, loss 0.5192922353744507, accuracy 0.83642578125\n",
      "2019-04-10T00:50:27.523325: step 480, loss 0.6107321977615356, accuracy 0.78662109375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:27.736696: step 480, loss 0.495934, accuracy 0.8251953125\n",
      "\n",
      "\n",
      "2019-04-10T00:50:27.793290: step 481, loss 0.557233452796936, accuracy 0.7998046875\n",
      "2019-04-10T00:50:27.842947: step 482, loss 0.4860864281654358, accuracy 0.830078125\n",
      "2019-04-10T00:50:27.894470: step 483, loss 0.4882695972919464, accuracy 0.81689453125\n",
      "2019-04-10T00:50:27.945582: step 484, loss 0.5306679010391235, accuracy 0.79345703125\n",
      "2019-04-10T00:50:28.002684: step 485, loss 0.6421995162963867, accuracy 0.7998046875\n",
      "2019-04-10T00:50:28.064219: step 486, loss 0.5165201425552368, accuracy 0.82666015625\n",
      "2019-04-10T00:50:28.114648: step 487, loss 0.5236518383026123, accuracy 0.8134765625\n",
      "2019-04-10T00:50:28.165137: step 488, loss 0.5278089642524719, accuracy 0.83984375\n",
      "2019-04-10T00:50:28.215809: step 489, loss 0.5326798558235168, accuracy 0.81982421875\n",
      "2019-04-10T00:50:28.265447: step 490, loss 0.5274097919464111, accuracy 0.806640625\n",
      "2019-04-10T00:50:28.313523: step 491, loss 0.48146554827690125, accuracy 0.830078125\n",
      "2019-04-10T00:50:28.376085: step 492, loss 0.46112167835235596, accuracy 0.83349609375\n",
      "2019-04-10T00:50:28.433691: step 493, loss 0.5556623339653015, accuracy 0.81005859375\n",
      "2019-04-10T00:50:28.491220: step 494, loss 0.5474567413330078, accuracy 0.81689453125\n",
      "2019-04-10T00:50:28.539905: step 495, loss 0.48200154304504395, accuracy 0.82666015625\n",
      "2019-04-10T00:50:28.588098: step 496, loss 0.41642364859580994, accuracy 0.8564453125\n",
      "2019-04-10T00:50:28.638334: step 497, loss 0.47356826066970825, accuracy 0.830078125\n",
      "2019-04-10T00:50:28.686126: step 498, loss 0.6082485318183899, accuracy 0.81982421875\n",
      "2019-04-10T00:50:28.736473: step 499, loss 0.5497053265571594, accuracy 0.80322265625\n",
      "2019-04-10T00:50:28.799798: step 500, loss 0.5311209559440613, accuracy 0.830078125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:29.133128: step 500, loss 0.510698, accuracy 0.82958984375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:29.210026: step 501, loss 0.5365764498710632, accuracy 0.81005859375\n",
      "2019-04-10T00:50:29.273059: step 502, loss 0.48284128308296204, accuracy 0.8232421875\n",
      "2019-04-10T00:50:29.333571: step 503, loss 0.5096986293792725, accuracy 0.84326171875\n",
      "2019-04-10T00:50:29.392770: step 504, loss 0.5288507342338562, accuracy 0.806640625\n",
      "2019-04-10T00:50:29.450761: step 505, loss 0.5440773963928223, accuracy 0.79345703125\n",
      "2019-04-10T00:50:29.511028: step 506, loss 0.5641480684280396, accuracy 0.82666015625\n",
      "2019-04-10T00:50:29.574091: step 507, loss 0.5813283920288086, accuracy 0.8232421875\n",
      "2019-04-10T00:50:29.642380: step 508, loss 0.4778987169265747, accuracy 0.81005859375\n",
      "2019-04-10T00:50:29.701346: step 509, loss 0.5484637022018433, accuracy 0.81982421875\n",
      "2019-04-10T00:50:29.761836: step 510, loss 0.6073294281959534, accuracy 0.7734375\n",
      "2019-04-10T00:50:29.824254: step 511, loss 0.6412858963012695, accuracy 0.7734375\n",
      "2019-04-10T00:50:29.884304: step 512, loss 0.4447360336780548, accuracy 0.83642578125\n",
      "2019-04-10T00:50:29.948791: step 513, loss 0.5013219714164734, accuracy 0.80322265625\n",
      "2019-04-10T00:50:30.014716: step 514, loss 0.519313395023346, accuracy 0.81689453125\n",
      "2019-04-10T00:50:30.079145: step 515, loss 0.4889583885669708, accuracy 0.8232421875\n",
      "2019-04-10T00:50:30.141386: step 516, loss 0.47870758175849915, accuracy 0.8466796875\n",
      "2019-04-10T00:50:30.206201: step 517, loss 0.48600828647613525, accuracy 0.82666015625\n",
      "2019-04-10T00:50:30.268135: step 518, loss 0.5546510219573975, accuracy 0.81689453125\n",
      "2019-04-10T00:50:30.331222: step 519, loss 0.41091108322143555, accuracy 0.8564453125\n",
      "2019-04-10T00:50:30.392031: step 520, loss 0.5186886787414551, accuracy 0.85009765625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:30.650520: step 520, loss 0.503246, accuracy 0.8330078125\n",
      "\n",
      "\n",
      "2019-04-10T00:50:30.717166: step 521, loss 0.5946030616760254, accuracy 0.796875\n",
      "2019-04-10T00:50:30.785970: step 522, loss 0.5456902980804443, accuracy 0.806640625\n",
      "2019-04-10T00:50:30.849109: step 523, loss 0.49351632595062256, accuracy 0.83642578125\n",
      "2019-04-10T00:50:30.916684: step 524, loss 0.5158353447914124, accuracy 0.830078125\n",
      "2019-04-10T00:50:30.979531: step 525, loss 0.4625244736671448, accuracy 0.85986328125\n",
      "2019-04-10T00:50:31.046031: step 526, loss 0.5720223188400269, accuracy 0.78662109375\n",
      "2019-04-10T00:50:31.109231: step 527, loss 0.44298669695854187, accuracy 0.86328125\n",
      "2019-04-10T00:50:31.177495: step 528, loss 0.5324609875679016, accuracy 0.83642578125\n",
      "2019-04-10T00:50:31.242988: step 529, loss 0.590271532535553, accuracy 0.806640625\n",
      "2019-04-10T00:50:31.304715: step 530, loss 0.48480769991874695, accuracy 0.81982421875\n",
      "2019-04-10T00:50:31.364529: step 531, loss 0.5258851647377014, accuracy 0.81005859375\n",
      "2019-04-10T00:50:31.431754: step 532, loss 0.47235050797462463, accuracy 0.82666015625\n",
      "2019-04-10T00:50:31.497389: step 533, loss 0.43473124504089355, accuracy 0.83349609375\n",
      "2019-04-10T00:50:31.555993: step 534, loss 0.47626155614852905, accuracy 0.82666015625\n",
      "2019-04-10T00:50:31.623868: step 535, loss 0.531826376914978, accuracy 0.7998046875\n",
      "2019-04-10T00:50:31.691122: step 536, loss 0.5657147169113159, accuracy 0.796875\n",
      "2019-04-10T00:50:31.754833: step 537, loss 0.4788961708545685, accuracy 0.83349609375\n",
      "2019-04-10T00:50:31.820485: step 538, loss 0.3776070773601532, accuracy 0.87353515625\n",
      "2019-04-10T00:50:31.886268: step 539, loss 0.5326120853424072, accuracy 0.796875\n",
      "2019-04-10T00:50:31.957447: step 540, loss 0.5930160284042358, accuracy 0.77001953125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:32.244610: step 540, loss 0.51245, accuracy 0.81884765625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:32.313875: step 541, loss 0.5014934539794922, accuracy 0.83349609375\n",
      "2019-04-10T00:50:32.372212: step 542, loss 0.4942028224468231, accuracy 0.83984375\n",
      "2019-04-10T00:50:32.428593: step 543, loss 0.4565087854862213, accuracy 0.83642578125\n",
      "2019-04-10T00:50:32.490765: step 544, loss 0.6081104874610901, accuracy 0.77685546875\n",
      "2019-04-10T00:50:32.551942: step 545, loss 0.5540880560874939, accuracy 0.806640625\n",
      "2019-04-10T00:50:32.611675: step 546, loss 0.5264520049095154, accuracy 0.81689453125\n",
      "2019-04-10T00:50:32.665883: step 547, loss 0.4479976296424866, accuracy 0.83349609375\n",
      "2019-04-10T00:50:32.718909: step 548, loss 0.46812495589256287, accuracy 0.830078125\n",
      "2019-04-10T00:50:32.768855: step 549, loss 0.6000363826751709, accuracy 0.80322265625\n",
      "2019-04-10T00:50:32.818546: step 550, loss 0.5136704444885254, accuracy 0.783203125\n",
      "2019-04-10T00:50:32.868700: step 551, loss 0.4581122398376465, accuracy 0.83984375\n",
      "2019-04-10T00:50:32.920184: step 552, loss 0.5274097323417664, accuracy 0.80322265625\n",
      "2019-04-10T00:50:32.972718: step 553, loss 0.5180749297142029, accuracy 0.81982421875\n",
      "2019-04-10T00:50:33.025681: step 554, loss 0.514485776424408, accuracy 0.81689453125\n",
      "2019-04-10T00:50:33.086902: step 555, loss 0.5138019919395447, accuracy 0.806640625\n",
      "2019-04-10T00:50:33.146173: step 556, loss 0.47975626587867737, accuracy 0.8134765625\n",
      "2019-04-10T00:50:33.197171: step 557, loss 0.5233379006385803, accuracy 0.81005859375\n",
      "2019-04-10T00:50:33.249076: step 558, loss 0.5883861780166626, accuracy 0.81982421875\n",
      "2019-04-10T00:50:33.298566: step 559, loss 0.4659233093261719, accuracy 0.85009765625\n",
      "2019-04-10T00:50:33.359211: step 560, loss 0.5566345453262329, accuracy 0.81982421875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:33.589836: step 560, loss 0.491424, accuracy 0.8359375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:33.647329: step 561, loss 0.5495906472206116, accuracy 0.82666015625\n",
      "2019-04-10T00:50:33.706428: step 562, loss 0.5205077528953552, accuracy 0.82666015625\n",
      "2019-04-10T00:50:33.758968: step 563, loss 0.5367386341094971, accuracy 0.7998046875\n",
      "2019-04-10T00:50:33.807874: step 564, loss 0.5352085828781128, accuracy 0.81005859375\n",
      "2019-04-10T00:50:33.857313: step 565, loss 0.503420889377594, accuracy 0.806640625\n",
      "2019-04-10T00:50:33.908305: step 566, loss 0.5392374992370605, accuracy 0.81982421875\n",
      "2019-04-10T00:50:33.959611: step 567, loss 0.44838958978652954, accuracy 0.8466796875\n",
      "2019-04-10T00:50:34.107564: step 568, loss 0.4800834059715271, accuracy 0.83349609375\n",
      "2019-04-10T00:50:34.174459: step 569, loss 0.5519377589225769, accuracy 0.806640625\n",
      "2019-04-10T00:50:34.227195: step 570, loss 0.46568429470062256, accuracy 0.82666015625\n",
      "2019-04-10T00:50:34.276022: step 571, loss 0.4399980306625366, accuracy 0.83984375\n",
      "2019-04-10T00:50:34.327450: step 572, loss 0.40312743186950684, accuracy 0.8466796875\n",
      "2019-04-10T00:50:34.379275: step 573, loss 0.5541708469390869, accuracy 0.796875\n",
      "2019-04-10T00:50:34.441083: step 574, loss 0.5098199248313904, accuracy 0.82666015625\n",
      "2019-04-10T00:50:34.495508: step 575, loss 0.5751616358757019, accuracy 0.77685546875\n",
      "2019-04-10T00:50:34.547482: step 576, loss 0.5420904755592346, accuracy 0.7998046875\n",
      "2019-04-10T00:50:34.597686: step 577, loss 0.42465874552726746, accuracy 0.83984375\n",
      "2019-04-10T00:50:34.655789: step 578, loss 0.5593268871307373, accuracy 0.81005859375\n",
      "2019-04-10T00:50:34.712152: step 579, loss 0.44879424571990967, accuracy 0.8701171875\n",
      "2019-04-10T00:50:34.765413: step 580, loss 0.49604761600494385, accuracy 0.8466796875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:34.983784: step 580, loss 0.485867, accuracy 0.83447265625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:35.046026: step 581, loss 0.39741286635398865, accuracy 0.8564453125\n",
      "2019-04-10T00:50:35.100626: step 582, loss 0.5370225310325623, accuracy 0.8232421875\n",
      "2019-04-10T00:50:35.154778: step 583, loss 0.7206620573997498, accuracy 0.759765625\n",
      "2019-04-10T00:50:35.204813: step 584, loss 0.516221284866333, accuracy 0.81005859375\n",
      "2019-04-10T00:50:35.255207: step 585, loss 0.5633121728897095, accuracy 0.80322265625\n",
      "2019-04-10T00:50:35.305676: step 586, loss 0.5044113397598267, accuracy 0.8232421875\n",
      "2019-04-10T00:50:35.357436: step 587, loss 0.4868354797363281, accuracy 0.81982421875\n",
      "2019-04-10T00:50:35.408435: step 588, loss 0.5047541856765747, accuracy 0.84326171875\n",
      "2019-04-10T00:50:35.462757: step 589, loss 0.450118750333786, accuracy 0.8701171875\n",
      "2019-04-10T00:50:35.515174: step 590, loss 0.5211182832717896, accuracy 0.8232421875\n",
      "2019-04-10T00:50:35.568200: step 591, loss 0.4749600291252136, accuracy 0.82666015625\n",
      "2019-04-10T00:50:35.619595: step 592, loss 0.4874035120010376, accuracy 0.8134765625\n",
      "2019-04-10T00:50:35.670163: step 593, loss 0.5057586431503296, accuracy 0.8134765625\n",
      "2019-04-10T00:50:35.721351: step 594, loss 0.46806207299232483, accuracy 0.81689453125\n",
      "2019-04-10T00:50:35.777779: step 595, loss 0.579118013381958, accuracy 0.78662109375\n",
      "2019-04-10T00:50:35.835411: step 596, loss 0.4410872459411621, accuracy 0.85009765625\n",
      "2019-04-10T00:50:35.893988: step 597, loss 0.4086916744709015, accuracy 0.85986328125\n",
      "2019-04-10T00:50:35.967767: step 598, loss 0.5330075025558472, accuracy 0.81689453125\n",
      "2019-04-10T00:50:36.030234: step 599, loss 0.5911803245544434, accuracy 0.80322265625\n",
      "2019-04-10T00:50:36.090591: step 600, loss 0.4650382995605469, accuracy 0.8466796875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:36.331051: step 600, loss 0.476808, accuracy 0.83349609375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:36.389385: step 601, loss 0.4685996472835541, accuracy 0.8232421875\n",
      "2019-04-10T00:50:36.445066: step 602, loss 0.5634052157402039, accuracy 0.81689453125\n",
      "2019-04-10T00:50:36.495900: step 603, loss 0.45274272561073303, accuracy 0.8232421875\n",
      "2019-04-10T00:50:36.548581: step 604, loss 0.48497259616851807, accuracy 0.85009765625\n",
      "2019-04-10T00:50:36.598545: step 605, loss 0.4513368308544159, accuracy 0.81005859375\n",
      "2019-04-10T00:50:36.648664: step 606, loss 0.49891287088394165, accuracy 0.81689453125\n",
      "2019-04-10T00:50:36.697231: step 607, loss 0.36272770166397095, accuracy 0.87646484375\n",
      "2019-04-10T00:50:36.749125: step 608, loss 0.5552989840507507, accuracy 0.78662109375\n",
      "2019-04-10T00:50:36.801697: step 609, loss 0.4238855540752411, accuracy 0.8466796875\n",
      "2019-04-10T00:50:36.851867: step 610, loss 0.4746065139770508, accuracy 0.8232421875\n",
      "2019-04-10T00:50:36.902840: step 611, loss 0.3972572684288025, accuracy 0.85009765625\n",
      "2019-04-10T00:50:36.953035: step 612, loss 0.5453907251358032, accuracy 0.81005859375\n",
      "2019-04-10T00:50:37.002214: step 613, loss 0.5038901567459106, accuracy 0.8134765625\n",
      "2019-04-10T00:50:37.053536: step 614, loss 0.48378291726112366, accuracy 0.8134765625\n",
      "2019-04-10T00:50:37.104622: step 615, loss 0.4681258499622345, accuracy 0.83349609375\n",
      "2019-04-10T00:50:37.156018: step 616, loss 0.5332598090171814, accuracy 0.81005859375\n",
      "2019-04-10T00:50:37.209468: step 617, loss 0.49232855439186096, accuracy 0.84326171875\n",
      "2019-04-10T00:50:37.263360: step 618, loss 0.5355156660079956, accuracy 0.8232421875\n",
      "2019-04-10T00:50:37.316106: step 619, loss 0.5158082842826843, accuracy 0.806640625\n",
      "2019-04-10T00:50:37.368314: step 620, loss 0.5569618344306946, accuracy 0.81005859375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:37.596766: step 620, loss 0.488556, accuracy 0.8388671875\n",
      "\n",
      "\n",
      "2019-04-10T00:50:37.655080: step 621, loss 0.385368287563324, accuracy 0.853515625\n",
      "2019-04-10T00:50:37.709775: step 622, loss 0.5054476261138916, accuracy 0.830078125\n",
      "2019-04-10T00:50:37.760822: step 623, loss 0.4878770411014557, accuracy 0.830078125\n",
      "2019-04-10T00:50:37.810794: step 624, loss 0.4800266623497009, accuracy 0.83349609375\n",
      "2019-04-10T00:50:37.865910: step 625, loss 0.4997450113296509, accuracy 0.830078125\n",
      "2019-04-10T00:50:37.920127: step 626, loss 0.5079464912414551, accuracy 0.830078125\n",
      "2019-04-10T00:50:37.978994: step 627, loss 0.5768795013427734, accuracy 0.796875\n",
      "2019-04-10T00:50:38.043406: step 628, loss 0.44693174958229065, accuracy 0.8466796875\n",
      "2019-04-10T00:50:38.098930: step 629, loss 0.4334280788898468, accuracy 0.853515625\n",
      "2019-04-10T00:50:38.154574: step 630, loss 0.5470050573348999, accuracy 0.81982421875\n",
      "2019-04-10T00:50:38.208586: step 631, loss 0.5753074884414673, accuracy 0.7998046875\n",
      "2019-04-10T00:50:38.262488: step 632, loss 0.506511390209198, accuracy 0.796875\n",
      "2019-04-10T00:50:38.313598: step 633, loss 0.5021492838859558, accuracy 0.80322265625\n",
      "2019-04-10T00:50:38.364173: step 634, loss 0.4881529211997986, accuracy 0.83349609375\n",
      "2019-04-10T00:50:38.415356: step 635, loss 0.526109516620636, accuracy 0.82666015625\n",
      "2019-04-10T00:50:38.463876: step 636, loss 0.46552327275276184, accuracy 0.81689453125\n",
      "2019-04-10T00:50:38.515900: step 637, loss 0.535351037979126, accuracy 0.83349609375\n",
      "2019-04-10T00:50:38.564075: step 638, loss 0.5883692502975464, accuracy 0.78662109375\n",
      "2019-04-10T00:50:38.614658: step 639, loss 0.46793726086616516, accuracy 0.83984375\n",
      "2019-04-10T00:50:38.666704: step 640, loss 0.57701176404953, accuracy 0.8134765625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:38.894551: step 640, loss 0.485812, accuracy 0.833984375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:38.948204: step 641, loss 0.6140285730361938, accuracy 0.7900390625\n",
      "2019-04-10T00:50:39.000452: step 642, loss 0.5279783010482788, accuracy 0.81982421875\n",
      "2019-04-10T00:50:39.049617: step 643, loss 0.5392261147499084, accuracy 0.83642578125\n",
      "2019-04-10T00:50:39.102291: step 644, loss 0.5451122522354126, accuracy 0.8134765625\n",
      "2019-04-10T00:50:39.153821: step 645, loss 0.5203952789306641, accuracy 0.7900390625\n",
      "2019-04-10T00:50:39.207302: step 646, loss 0.5405598878860474, accuracy 0.830078125\n",
      "2019-04-10T00:50:39.258418: step 647, loss 0.4511983096599579, accuracy 0.853515625\n",
      "2019-04-10T00:50:39.310033: step 648, loss 0.41184762120246887, accuracy 0.853515625\n",
      "2019-04-10T00:50:39.365019: step 649, loss 0.5274855494499207, accuracy 0.806640625\n",
      "2019-04-10T00:50:39.426274: step 650, loss 0.601446807384491, accuracy 0.79345703125\n",
      "2019-04-10T00:50:39.488309: step 651, loss 0.5129079222679138, accuracy 0.8232421875\n",
      "2019-04-10T00:50:39.550938: step 652, loss 0.5004964470863342, accuracy 0.82666015625\n",
      "2019-04-10T00:50:39.609776: step 653, loss 0.540442943572998, accuracy 0.7998046875\n",
      "2019-04-10T00:50:39.665326: step 654, loss 0.5281150937080383, accuracy 0.83984375\n",
      "2019-04-10T00:50:39.713947: step 655, loss 0.5072761178016663, accuracy 0.83984375\n",
      "2019-04-10T00:50:39.766789: step 656, loss 0.5664417743682861, accuracy 0.83349609375\n",
      "2019-04-10T00:50:39.820078: step 657, loss 0.441860556602478, accuracy 0.85986328125\n",
      "2019-04-10T00:50:39.878193: step 658, loss 0.47572061419487, accuracy 0.853515625\n",
      "2019-04-10T00:50:39.939309: step 659, loss 0.5905387997627258, accuracy 0.80322265625\n",
      "2019-04-10T00:50:39.993555: step 660, loss 0.47425028681755066, accuracy 0.82666015625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:40.223009: step 660, loss 0.479621, accuracy 0.841796875\n",
      "\n",
      "\n",
      "2019-04-10T00:50:40.276800: step 661, loss 0.465952605009079, accuracy 0.8564453125\n",
      "2019-04-10T00:50:40.329405: step 662, loss 0.3892144560813904, accuracy 0.8701171875\n",
      "2019-04-10T00:50:40.381714: step 663, loss 0.5340893268585205, accuracy 0.83984375\n",
      "2019-04-10T00:50:40.434929: step 664, loss 0.5055023431777954, accuracy 0.82666015625\n",
      "2019-04-10T00:50:40.494345: step 665, loss 0.4535622298717499, accuracy 0.83984375\n",
      "2019-04-10T00:50:40.551334: step 666, loss 0.48263391852378845, accuracy 0.81982421875\n",
      "2019-04-10T00:50:40.604909: step 667, loss 0.4704800546169281, accuracy 0.8232421875\n",
      "2019-04-10T00:50:40.655975: step 668, loss 0.44046005606651306, accuracy 0.85009765625\n",
      "2019-04-10T00:50:40.710059: step 669, loss 0.3999762237071991, accuracy 0.87646484375\n",
      "2019-04-10T00:50:40.765007: step 670, loss 0.4493413269519806, accuracy 0.83984375\n",
      "2019-04-10T00:50:40.827913: step 671, loss 0.5808117389678955, accuracy 0.80322265625\n",
      "2019-04-10T00:50:40.884232: step 672, loss 0.497419536113739, accuracy 0.8134765625\n",
      "2019-04-10T00:50:40.940944: step 673, loss 0.3982411324977875, accuracy 0.8466796875\n",
      "2019-04-10T00:50:40.990908: step 674, loss 0.398619145154953, accuracy 0.85009765625\n",
      "2019-04-10T00:50:41.043188: step 675, loss 0.4559902548789978, accuracy 0.8466796875\n",
      "2019-04-10T00:50:41.100230: step 676, loss 0.5561093688011169, accuracy 0.783203125\n",
      "2019-04-10T00:50:41.160266: step 677, loss 0.4158863425254822, accuracy 0.853515625\n",
      "2019-04-10T00:50:41.220040: step 678, loss 0.4705239236354828, accuracy 0.83984375\n",
      "2019-04-10T00:50:41.277305: step 679, loss 0.5103130340576172, accuracy 0.8232421875\n",
      "2019-04-10T00:50:41.328560: step 680, loss 0.4599491357803345, accuracy 0.853515625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:41.555483: step 680, loss 0.475666, accuracy 0.83935546875\n",
      "\n",
      "\n",
      "2019-04-10T00:50:41.616291: step 681, loss 0.4798138439655304, accuracy 0.83349609375\n",
      "2019-04-10T00:50:41.668285: step 682, loss 0.5284377932548523, accuracy 0.81982421875\n",
      "2019-04-10T00:50:41.723332: step 683, loss 0.4980819821357727, accuracy 0.83349609375\n",
      "2019-04-10T00:50:41.781550: step 684, loss 0.4733715355396271, accuracy 0.82666015625\n",
      "2019-04-10T00:50:41.844129: step 685, loss 0.46998241543769836, accuracy 0.81689453125\n",
      "2019-04-10T00:50:41.900371: step 686, loss 0.49725356698036194, accuracy 0.83349609375\n",
      "2019-04-10T00:50:41.959551: step 687, loss 0.4705064296722412, accuracy 0.8564453125\n",
      "2019-04-10T00:50:42.011223: step 688, loss 0.53301602602005, accuracy 0.8134765625\n",
      "2019-04-10T00:50:42.063503: step 689, loss 0.48391133546829224, accuracy 0.8564453125\n",
      "2019-04-10T00:50:42.117938: step 690, loss 0.4741222560405731, accuracy 0.85009765625\n",
      "2019-04-10T00:50:42.181242: step 691, loss 0.5153496861457825, accuracy 0.79345703125\n",
      "2019-04-10T00:50:42.234216: step 692, loss 0.5232519507408142, accuracy 0.8232421875\n",
      "2019-04-10T00:50:42.288019: step 693, loss 0.4911668002605438, accuracy 0.82666015625\n",
      "2019-04-10T00:50:42.340135: step 694, loss 0.6375818848609924, accuracy 0.81005859375\n",
      "2019-04-10T00:50:42.391754: step 695, loss 0.5040163397789001, accuracy 0.84326171875\n",
      "2019-04-10T00:50:42.451934: step 696, loss 0.5647780299186707, accuracy 0.81689453125\n",
      "2019-04-10T00:50:42.504479: step 697, loss 0.48383045196533203, accuracy 0.830078125\n",
      "2019-04-10T00:50:42.555836: step 698, loss 0.49311646819114685, accuracy 0.8466796875\n",
      "2019-04-10T00:50:42.603831: step 699, loss 0.4860881567001343, accuracy 0.81005859375\n",
      "2019-04-10T00:50:42.656408: step 700, loss 0.4553951919078827, accuracy 0.82666015625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:42.870413: step 700, loss 0.484031, accuracy 0.83837890625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:42.922777: step 701, loss 0.43061986565589905, accuracy 0.85009765625\n",
      "2019-04-10T00:50:42.972454: step 702, loss 0.6100534200668335, accuracy 0.77978515625\n",
      "2019-04-10T00:50:43.025916: step 703, loss 0.4302496910095215, accuracy 0.88671875\n",
      "2019-04-10T00:50:43.075934: step 704, loss 0.506088376045227, accuracy 0.8232421875\n",
      "2019-04-10T00:50:43.128753: step 705, loss 0.5055602788925171, accuracy 0.82666015625\n",
      "2019-04-10T00:50:43.177360: step 706, loss 0.4505826234817505, accuracy 0.83642578125\n",
      "2019-04-10T00:50:43.229625: step 707, loss 0.44461482763290405, accuracy 0.853515625\n",
      "2019-04-10T00:50:43.279107: step 708, loss 0.460903525352478, accuracy 0.85009765625\n",
      "2019-04-10T00:50:43.329780: step 709, loss 0.4612903892993927, accuracy 0.83642578125\n",
      "2019-04-10T00:50:43.385244: step 710, loss 0.6216917634010315, accuracy 0.81689453125\n",
      "2019-04-10T00:50:43.450485: step 711, loss 0.5018590092658997, accuracy 0.83349609375\n",
      "2019-04-10T00:50:43.513516: step 712, loss 0.5028322339057922, accuracy 0.83349609375\n",
      "2019-04-10T00:50:43.577940: step 713, loss 0.47873803973197937, accuracy 0.81689453125\n",
      "2019-04-10T00:50:43.637564: step 714, loss 0.45915114879608154, accuracy 0.8232421875\n",
      "2019-04-10T00:50:43.702807: step 715, loss 0.5825481414794922, accuracy 0.79345703125\n",
      "2019-04-10T00:50:43.763883: step 716, loss 0.5028401017189026, accuracy 0.8134765625\n",
      "2019-04-10T00:50:43.830057: step 717, loss 0.4240340292453766, accuracy 0.8466796875\n",
      "2019-04-10T00:50:43.887077: step 718, loss 0.5520114898681641, accuracy 0.7900390625\n",
      "2019-04-10T00:50:43.951632: step 719, loss 0.5163596868515015, accuracy 0.81689453125\n",
      "2019-04-10T00:50:43.983604: step 720, loss 0.5728898644447327, accuracy 0.81982421875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:44.306371: step 720, loss 0.484101, accuracy 0.83740234375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:44.492392: step 721, loss 0.4612252414226532, accuracy 0.8466796875\n",
      "2019-04-10T00:50:44.552996: step 722, loss 0.5277034044265747, accuracy 0.83349609375\n",
      "2019-04-10T00:50:44.618111: step 723, loss 0.44163212180137634, accuracy 0.85009765625\n",
      "2019-04-10T00:50:44.682246: step 724, loss 0.5716376900672913, accuracy 0.80322265625\n",
      "2019-04-10T00:50:44.745187: step 725, loss 0.4868972897529602, accuracy 0.8232421875\n",
      "2019-04-10T00:50:44.801551: step 726, loss 0.4509425461292267, accuracy 0.83349609375\n",
      "2019-04-10T00:50:44.864355: step 727, loss 0.5514847040176392, accuracy 0.830078125\n",
      "2019-04-10T00:50:44.929553: step 728, loss 0.6052390933036804, accuracy 0.8134765625\n",
      "2019-04-10T00:50:45.003106: step 729, loss 0.5107427835464478, accuracy 0.85009765625\n",
      "2019-04-10T00:50:45.062122: step 730, loss 0.471838116645813, accuracy 0.82666015625\n",
      "2019-04-10T00:50:45.117503: step 731, loss 0.437210351228714, accuracy 0.81689453125\n",
      "2019-04-10T00:50:45.171149: step 732, loss 0.3949160873889923, accuracy 0.8701171875\n",
      "2019-04-10T00:50:45.225735: step 733, loss 0.4501136243343353, accuracy 0.82666015625\n",
      "2019-04-10T00:50:45.277515: step 734, loss 0.551662027835846, accuracy 0.81005859375\n",
      "2019-04-10T00:50:45.332487: step 735, loss 0.45133569836616516, accuracy 0.82666015625\n",
      "2019-04-10T00:50:45.384796: step 736, loss 0.4965731203556061, accuracy 0.84326171875\n",
      "2019-04-10T00:50:45.438920: step 737, loss 0.4880649447441101, accuracy 0.830078125\n",
      "2019-04-10T00:50:45.491423: step 738, loss 0.40244439244270325, accuracy 0.85986328125\n",
      "2019-04-10T00:50:45.543187: step 739, loss 0.5282731056213379, accuracy 0.8232421875\n",
      "2019-04-10T00:50:45.592187: step 740, loss 0.4023926258087158, accuracy 0.87353515625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:45.810596: step 740, loss 0.469726, accuracy 0.84228515625\n",
      "\n",
      "\n",
      "2019-04-10T00:50:45.865412: step 741, loss 0.4457296133041382, accuracy 0.86328125\n",
      "2019-04-10T00:50:45.918068: step 742, loss 0.5326687097549438, accuracy 0.7998046875\n",
      "2019-04-10T00:50:45.984822: step 743, loss 0.5003507733345032, accuracy 0.83984375\n",
      "2019-04-10T00:50:46.048375: step 744, loss 0.5589386820793152, accuracy 0.806640625\n",
      "2019-04-10T00:50:46.100330: step 745, loss 0.5063469409942627, accuracy 0.8564453125\n",
      "2019-04-10T00:50:46.158477: step 746, loss 0.48432978987693787, accuracy 0.8232421875\n",
      "2019-04-10T00:50:46.212455: step 747, loss 0.488279789686203, accuracy 0.81689453125\n",
      "2019-04-10T00:50:46.266907: step 748, loss 0.4383646249771118, accuracy 0.85009765625\n",
      "2019-04-10T00:50:46.322552: step 749, loss 0.4408111572265625, accuracy 0.85009765625\n",
      "2019-04-10T00:50:46.379795: step 750, loss 0.5061608552932739, accuracy 0.82666015625\n",
      "2019-04-10T00:50:46.433861: step 751, loss 0.46142813563346863, accuracy 0.853515625\n",
      "2019-04-10T00:50:46.490798: step 752, loss 0.53895503282547, accuracy 0.806640625\n",
      "2019-04-10T00:50:46.595748: step 753, loss 0.5494264364242554, accuracy 0.8134765625\n",
      "2019-04-10T00:50:46.658673: step 754, loss 0.5452650785446167, accuracy 0.806640625\n",
      "2019-04-10T00:50:46.710736: step 755, loss 0.6288797855377197, accuracy 0.7734375\n",
      "2019-04-10T00:50:46.761556: step 756, loss 0.6109981536865234, accuracy 0.79345703125\n",
      "2019-04-10T00:50:46.811143: step 757, loss 0.4688911437988281, accuracy 0.85009765625\n",
      "2019-04-10T00:50:46.861038: step 758, loss 0.5764835476875305, accuracy 0.81005859375\n",
      "2019-04-10T00:50:46.912881: step 759, loss 0.47037822008132935, accuracy 0.830078125\n",
      "2019-04-10T00:50:46.965954: step 760, loss 0.39148077368736267, accuracy 0.85986328125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:47.181888: step 760, loss 0.479457, accuracy 0.8388671875\n",
      "\n",
      "\n",
      "2019-04-10T00:50:47.246829: step 761, loss 0.4353408217430115, accuracy 0.8564453125\n",
      "2019-04-10T00:50:47.304641: step 762, loss 0.5673628449440002, accuracy 0.783203125\n",
      "2019-04-10T00:50:47.355327: step 763, loss 0.48100388050079346, accuracy 0.796875\n",
      "2019-04-10T00:50:47.405992: step 764, loss 0.47180497646331787, accuracy 0.83349609375\n",
      "2019-04-10T00:50:47.456821: step 765, loss 0.5841946601867676, accuracy 0.7998046875\n",
      "2019-04-10T00:50:47.507560: step 766, loss 0.5068714618682861, accuracy 0.796875\n",
      "2019-04-10T00:50:47.558802: step 767, loss 0.44298094511032104, accuracy 0.8701171875\n",
      "2019-04-10T00:50:47.610438: step 768, loss 0.4835079312324524, accuracy 0.81005859375\n",
      "2019-04-10T00:50:47.660919: step 769, loss 0.3976849913597107, accuracy 0.85986328125\n",
      "2019-04-10T00:50:47.719339: step 770, loss 0.4170260727405548, accuracy 0.82666015625\n",
      "2019-04-10T00:50:47.776355: step 771, loss 0.4189109802246094, accuracy 0.86328125\n",
      "2019-04-10T00:50:47.835953: step 772, loss 0.47975611686706543, accuracy 0.830078125\n",
      "2019-04-10T00:50:47.895054: step 773, loss 0.4562545120716095, accuracy 0.84326171875\n",
      "2019-04-10T00:50:47.949853: step 774, loss 0.5195949077606201, accuracy 0.81689453125\n",
      "2019-04-10T00:50:48.002677: step 775, loss 0.4214123785495758, accuracy 0.82666015625\n",
      "2019-04-10T00:50:48.063955: step 776, loss 0.47667744755744934, accuracy 0.8466796875\n",
      "2019-04-10T00:50:48.117593: step 777, loss 0.6195722818374634, accuracy 0.7998046875\n",
      "2019-04-10T00:50:48.169453: step 778, loss 0.42676645517349243, accuracy 0.87353515625\n",
      "2019-04-10T00:50:48.219394: step 779, loss 0.4736684262752533, accuracy 0.83642578125\n",
      "2019-04-10T00:50:48.277193: step 780, loss 0.5821107029914856, accuracy 0.806640625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:48.524931: step 780, loss 0.490838, accuracy 0.83203125\n",
      "\n",
      "\n",
      "2019-04-10T00:50:48.582546: step 781, loss 0.41550591588020325, accuracy 0.85986328125\n",
      "2019-04-10T00:50:48.636432: step 782, loss 0.6129465103149414, accuracy 0.783203125\n",
      "2019-04-10T00:50:48.695388: step 783, loss 0.48005804419517517, accuracy 0.81689453125\n",
      "2019-04-10T00:50:48.749858: step 784, loss 0.44380491971969604, accuracy 0.8564453125\n",
      "2019-04-10T00:50:48.802930: step 785, loss 0.46355366706848145, accuracy 0.8466796875\n",
      "2019-04-10T00:50:48.863410: step 786, loss 0.5206566452980042, accuracy 0.83642578125\n",
      "2019-04-10T00:50:48.927446: step 787, loss 0.6188911199569702, accuracy 0.77685546875\n",
      "2019-04-10T00:50:48.981483: step 788, loss 0.5427495241165161, accuracy 0.80322265625\n",
      "2019-04-10T00:50:49.033703: step 789, loss 0.5780102014541626, accuracy 0.79345703125\n",
      "2019-04-10T00:50:49.086883: step 790, loss 0.41984668374061584, accuracy 0.87353515625\n",
      "2019-04-10T00:50:49.137996: step 791, loss 0.47436824440956116, accuracy 0.8134765625\n",
      "2019-04-10T00:50:49.188366: step 792, loss 0.5494081377983093, accuracy 0.81005859375\n",
      "2019-04-10T00:50:49.240578: step 793, loss 0.37799108028411865, accuracy 0.88330078125\n",
      "2019-04-10T00:50:49.293000: step 794, loss 0.588991641998291, accuracy 0.806640625\n",
      "2019-04-10T00:50:49.352344: step 795, loss 0.4799570143222809, accuracy 0.85986328125\n",
      "2019-04-10T00:50:49.413807: step 796, loss 0.5494945049285889, accuracy 0.81689453125\n",
      "2019-04-10T00:50:49.468460: step 797, loss 0.4609222412109375, accuracy 0.83984375\n",
      "2019-04-10T00:50:49.522140: step 798, loss 0.34720292687416077, accuracy 0.87353515625\n",
      "2019-04-10T00:50:49.577682: step 799, loss 0.46645569801330566, accuracy 0.83984375\n",
      "2019-04-10T00:50:49.633085: step 800, loss 0.46988582611083984, accuracy 0.8466796875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:49.868725: step 800, loss 0.469568, accuracy 0.84814453125\n",
      "\n",
      "\n",
      "2019-04-10T00:50:49.931913: step 801, loss 0.4662896692752838, accuracy 0.8232421875\n",
      "2019-04-10T00:50:49.991975: step 802, loss 0.4800538718700409, accuracy 0.853515625\n",
      "2019-04-10T00:50:50.051755: step 803, loss 0.479405015707016, accuracy 0.83349609375\n",
      "2019-04-10T00:50:50.115428: step 804, loss 0.5038382411003113, accuracy 0.853515625\n",
      "2019-04-10T00:50:50.172147: step 805, loss 0.5231980681419373, accuracy 0.8232421875\n",
      "2019-04-10T00:50:50.224527: step 806, loss 0.49665579199790955, accuracy 0.84326171875\n",
      "2019-04-10T00:50:50.276573: step 807, loss 0.5558769702911377, accuracy 0.81982421875\n",
      "2019-04-10T00:50:50.328946: step 808, loss 0.4770957827568054, accuracy 0.830078125\n",
      "2019-04-10T00:50:50.380327: step 809, loss 0.4971078634262085, accuracy 0.7998046875\n",
      "2019-04-10T00:50:50.430285: step 810, loss 0.5046764612197876, accuracy 0.8232421875\n",
      "2019-04-10T00:50:50.481520: step 811, loss 0.4835287034511566, accuracy 0.853515625\n",
      "2019-04-10T00:50:50.532680: step 812, loss 0.4397534728050232, accuracy 0.853515625\n",
      "2019-04-10T00:50:50.605774: step 813, loss 0.57538241147995, accuracy 0.77978515625\n",
      "2019-04-10T00:50:50.671650: step 814, loss 0.47822535037994385, accuracy 0.8466796875\n",
      "2019-04-10T00:50:50.729226: step 815, loss 0.4828225076198578, accuracy 0.8232421875\n",
      "2019-04-10T00:50:50.788890: step 816, loss 0.4841345250606537, accuracy 0.84326171875\n",
      "2019-04-10T00:50:50.844976: step 817, loss 0.38404014706611633, accuracy 0.89013671875\n",
      "2019-04-10T00:50:50.918446: step 818, loss 0.48290058970451355, accuracy 0.830078125\n",
      "2019-04-10T00:50:51.000196: step 819, loss 0.48704954981803894, accuracy 0.8466796875\n",
      "2019-04-10T00:50:51.065236: step 820, loss 0.4688588082790375, accuracy 0.83349609375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:51.314321: step 820, loss 0.46766, accuracy 0.84130859375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:51.372205: step 821, loss 0.5497981905937195, accuracy 0.796875\n",
      "2019-04-10T00:50:51.429446: step 822, loss 0.4656680226325989, accuracy 0.81689453125\n",
      "2019-04-10T00:50:51.483710: step 823, loss 0.44596588611602783, accuracy 0.83642578125\n",
      "2019-04-10T00:50:51.540002: step 824, loss 0.46064692735671997, accuracy 0.8564453125\n",
      "2019-04-10T00:50:51.598003: step 825, loss 0.5394375324249268, accuracy 0.8232421875\n",
      "2019-04-10T00:50:51.654304: step 826, loss 0.5103329420089722, accuracy 0.83642578125\n",
      "2019-04-10T00:50:51.709284: step 827, loss 0.5468990802764893, accuracy 0.82666015625\n",
      "2019-04-10T00:50:51.763453: step 828, loss 0.5543832182884216, accuracy 0.796875\n",
      "2019-04-10T00:50:51.822304: step 829, loss 0.4115106463432312, accuracy 0.83984375\n",
      "2019-04-10T00:50:51.878140: step 830, loss 0.46209853887557983, accuracy 0.85009765625\n",
      "2019-04-10T00:50:51.932871: step 831, loss 0.4028357267379761, accuracy 0.83349609375\n",
      "2019-04-10T00:50:51.988231: step 832, loss 0.565933346748352, accuracy 0.81005859375\n",
      "2019-04-10T00:50:52.046766: step 833, loss 0.5320057272911072, accuracy 0.8232421875\n",
      "2019-04-10T00:50:52.102520: step 834, loss 0.4369202256202698, accuracy 0.82666015625\n",
      "2019-04-10T00:50:52.161083: step 835, loss 0.5682329535484314, accuracy 0.830078125\n",
      "2019-04-10T00:50:52.216910: step 836, loss 0.5361505746841431, accuracy 0.81982421875\n",
      "2019-04-10T00:50:52.278893: step 837, loss 0.43369075655937195, accuracy 0.84326171875\n",
      "2019-04-10T00:50:52.337081: step 838, loss 0.4741387963294983, accuracy 0.8466796875\n",
      "2019-04-10T00:50:52.397017: step 839, loss 0.5077462792396545, accuracy 0.81689453125\n",
      "2019-04-10T00:50:52.455667: step 840, loss 0.3670993149280548, accuracy 0.88330078125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:52.696285: step 840, loss 0.461648, accuracy 0.84521484375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:52.753407: step 841, loss 0.5732101798057556, accuracy 0.79345703125\n",
      "2019-04-10T00:50:52.808151: step 842, loss 0.5482783317565918, accuracy 0.81982421875\n",
      "2019-04-10T00:50:52.875482: step 843, loss 0.46508559584617615, accuracy 0.83642578125\n",
      "2019-04-10T00:50:52.935338: step 844, loss 0.40582379698753357, accuracy 0.87646484375\n",
      "2019-04-10T00:50:52.990979: step 845, loss 0.4592951834201813, accuracy 0.83984375\n",
      "2019-04-10T00:50:53.045950: step 846, loss 0.4778338074684143, accuracy 0.83642578125\n",
      "2019-04-10T00:50:53.101933: step 847, loss 0.4898899793624878, accuracy 0.80322265625\n",
      "2019-04-10T00:50:53.157787: step 848, loss 0.46104684472084045, accuracy 0.83349609375\n",
      "2019-04-10T00:50:53.214392: step 849, loss 0.5303664803504944, accuracy 0.783203125\n",
      "2019-04-10T00:50:53.268048: step 850, loss 0.48316890001296997, accuracy 0.81982421875\n",
      "2019-04-10T00:50:53.335099: step 851, loss 0.5907005071640015, accuracy 0.783203125\n",
      "2019-04-10T00:50:53.403917: step 852, loss 0.47451674938201904, accuracy 0.83642578125\n",
      "2019-04-10T00:50:53.476989: step 853, loss 0.4836482107639313, accuracy 0.8466796875\n",
      "2019-04-10T00:50:53.547149: step 854, loss 0.43868088722229004, accuracy 0.83984375\n",
      "2019-04-10T00:50:53.615116: step 855, loss 0.5535313487052917, accuracy 0.783203125\n",
      "2019-04-10T00:50:53.683036: step 856, loss 0.602337121963501, accuracy 0.7900390625\n",
      "2019-04-10T00:50:53.750010: step 857, loss 0.42146801948547363, accuracy 0.8466796875\n",
      "2019-04-10T00:50:53.814015: step 858, loss 0.44261786341667175, accuracy 0.8564453125\n",
      "2019-04-10T00:50:53.879767: step 859, loss 0.4452352523803711, accuracy 0.84326171875\n",
      "2019-04-10T00:50:53.946435: step 860, loss 0.4845547080039978, accuracy 0.8134765625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:54.175623: step 860, loss 0.480988, accuracy 0.8359375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:54.233616: step 861, loss 0.42958104610443115, accuracy 0.85009765625\n",
      "2019-04-10T00:50:54.303168: step 862, loss 0.43540695309638977, accuracy 0.85009765625\n",
      "2019-04-10T00:50:54.367770: step 863, loss 0.4380190074443817, accuracy 0.85009765625\n",
      "2019-04-10T00:50:54.425028: step 864, loss 0.4900508522987366, accuracy 0.8466796875\n",
      "2019-04-10T00:50:54.490898: step 865, loss 0.5008357763290405, accuracy 0.85986328125\n",
      "2019-04-10T00:50:54.554578: step 866, loss 0.5407175421714783, accuracy 0.830078125\n",
      "2019-04-10T00:50:54.618153: step 867, loss 0.5289483666419983, accuracy 0.80322265625\n",
      "2019-04-10T00:50:54.682238: step 868, loss 0.5784069299697876, accuracy 0.8134765625\n",
      "2019-04-10T00:50:54.744883: step 869, loss 0.47082188725471497, accuracy 0.84326171875\n",
      "2019-04-10T00:50:54.807157: step 870, loss 0.5439413189888, accuracy 0.79345703125\n",
      "2019-04-10T00:50:54.867079: step 871, loss 0.449629008769989, accuracy 0.853515625\n",
      "2019-04-10T00:50:54.932976: step 872, loss 0.46000245213508606, accuracy 0.853515625\n",
      "2019-04-10T00:50:54.996842: step 873, loss 0.4325782358646393, accuracy 0.853515625\n",
      "2019-04-10T00:50:55.057588: step 874, loss 0.47997379302978516, accuracy 0.85009765625\n",
      "2019-04-10T00:50:55.118065: step 875, loss 0.47116634249687195, accuracy 0.84326171875\n",
      "2019-04-10T00:50:55.178149: step 876, loss 0.5198656320571899, accuracy 0.830078125\n",
      "2019-04-10T00:50:55.234324: step 877, loss 0.4913938343524933, accuracy 0.8232421875\n",
      "2019-04-10T00:50:55.289738: step 878, loss 0.4372650682926178, accuracy 0.84326171875\n",
      "2019-04-10T00:50:55.345803: step 879, loss 0.43638238310813904, accuracy 0.85009765625\n",
      "2019-04-10T00:50:55.402810: step 880, loss 0.45134714245796204, accuracy 0.83642578125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:55.636346: step 880, loss 0.463001, accuracy 0.845703125\n",
      "\n",
      "\n",
      "2019-04-10T00:50:55.697408: step 881, loss 0.493351548910141, accuracy 0.83349609375\n",
      "2019-04-10T00:50:55.751580: step 882, loss 0.5309417247772217, accuracy 0.80322265625\n",
      "2019-04-10T00:50:55.807169: step 883, loss 0.4213971495628357, accuracy 0.85986328125\n",
      "2019-04-10T00:50:55.863257: step 884, loss 0.5221195220947266, accuracy 0.8232421875\n",
      "2019-04-10T00:50:55.921555: step 885, loss 0.5038896799087524, accuracy 0.83642578125\n",
      "2019-04-10T00:50:55.976675: step 886, loss 0.4824226498603821, accuracy 0.83349609375\n",
      "2019-04-10T00:50:56.036199: step 887, loss 0.5358462929725647, accuracy 0.796875\n",
      "2019-04-10T00:50:56.092979: step 888, loss 0.49261048436164856, accuracy 0.81689453125\n",
      "2019-04-10T00:50:56.149165: step 889, loss 0.4962795078754425, accuracy 0.830078125\n",
      "2019-04-10T00:50:56.204229: step 890, loss 0.505519688129425, accuracy 0.8232421875\n",
      "2019-04-10T00:50:56.257659: step 891, loss 0.49646297097206116, accuracy 0.83349609375\n",
      "2019-04-10T00:50:56.314325: step 892, loss 0.478544145822525, accuracy 0.806640625\n",
      "2019-04-10T00:50:56.369019: step 893, loss 0.540733277797699, accuracy 0.7998046875\n",
      "2019-04-10T00:50:56.437973: step 894, loss 0.4379870593547821, accuracy 0.8232421875\n",
      "2019-04-10T00:50:56.502538: step 895, loss 0.45021307468414307, accuracy 0.84326171875\n",
      "2019-04-10T00:50:56.572348: step 896, loss 0.4533504843711853, accuracy 0.8232421875\n",
      "2019-04-10T00:50:56.636375: step 897, loss 0.4881598949432373, accuracy 0.8466796875\n",
      "2019-04-10T00:50:56.698078: step 898, loss 0.5149627327919006, accuracy 0.8232421875\n",
      "2019-04-10T00:50:56.757482: step 899, loss 0.3861525356769562, accuracy 0.87646484375\n",
      "2019-04-10T00:50:56.814382: step 900, loss 0.3876672685146332, accuracy 0.88330078125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:57.047984: step 900, loss 0.477194, accuracy 0.833984375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:57.108041: step 901, loss 0.41985321044921875, accuracy 0.86328125\n",
      "2019-04-10T00:50:57.165921: step 902, loss 0.5427608489990234, accuracy 0.83642578125\n",
      "2019-04-10T00:50:57.221983: step 903, loss 0.4753446877002716, accuracy 0.8134765625\n",
      "2019-04-10T00:50:57.276281: step 904, loss 0.4765666127204895, accuracy 0.8466796875\n",
      "2019-04-10T00:50:57.334933: step 905, loss 0.5297426581382751, accuracy 0.7998046875\n",
      "2019-04-10T00:50:57.390509: step 906, loss 0.4236663579940796, accuracy 0.85986328125\n",
      "2019-04-10T00:50:57.446815: step 907, loss 0.5120323300361633, accuracy 0.80322265625\n",
      "2019-04-10T00:50:57.502594: step 908, loss 0.40761446952819824, accuracy 0.85009765625\n",
      "2019-04-10T00:50:57.566294: step 909, loss 0.4994317591190338, accuracy 0.82666015625\n",
      "2019-04-10T00:50:57.623115: step 910, loss 0.40249377489089966, accuracy 0.86328125\n",
      "2019-04-10T00:50:57.678550: step 911, loss 0.4447590112686157, accuracy 0.8232421875\n",
      "2019-04-10T00:50:57.732895: step 912, loss 0.4678265452384949, accuracy 0.82666015625\n",
      "2019-04-10T00:50:57.788778: step 913, loss 0.4920937120914459, accuracy 0.81982421875\n",
      "2019-04-10T00:50:57.844314: step 914, loss 0.39984017610549927, accuracy 0.87353515625\n",
      "2019-04-10T00:50:57.900443: step 915, loss 0.4626246690750122, accuracy 0.83642578125\n",
      "2019-04-10T00:50:57.953887: step 916, loss 0.496695876121521, accuracy 0.8232421875\n",
      "2019-04-10T00:50:58.011412: step 917, loss 0.5264279842376709, accuracy 0.806640625\n",
      "2019-04-10T00:50:58.066818: step 918, loss 0.49288031458854675, accuracy 0.85009765625\n",
      "2019-04-10T00:50:58.124133: step 919, loss 0.45960190892219543, accuracy 0.8232421875\n",
      "2019-04-10T00:50:58.180542: step 920, loss 0.4564990699291229, accuracy 0.83349609375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:58.417545: step 920, loss 0.462338, accuracy 0.8427734375\n",
      "\n",
      "\n",
      "2019-04-10T00:50:58.479066: step 921, loss 0.49685510993003845, accuracy 0.81689453125\n",
      "2019-04-10T00:50:58.535984: step 922, loss 0.587664008140564, accuracy 0.796875\n",
      "2019-04-10T00:50:58.594928: step 923, loss 0.5310604572296143, accuracy 0.81005859375\n",
      "2019-04-10T00:50:58.651876: step 924, loss 0.367931991815567, accuracy 0.85986328125\n",
      "2019-04-10T00:50:58.708921: step 925, loss 0.4257955253124237, accuracy 0.8466796875\n",
      "2019-04-10T00:50:58.763760: step 926, loss 0.5738695859909058, accuracy 0.83984375\n",
      "2019-04-10T00:50:58.820043: step 927, loss 0.4936993420124054, accuracy 0.830078125\n",
      "2019-04-10T00:50:58.876233: step 928, loss 0.4299917221069336, accuracy 0.84326171875\n",
      "2019-04-10T00:50:58.936318: step 929, loss 0.5271028876304626, accuracy 0.79345703125\n",
      "2019-04-10T00:50:58.994336: step 930, loss 0.44734644889831543, accuracy 0.85009765625\n",
      "2019-04-10T00:50:59.052947: step 931, loss 0.4885009229183197, accuracy 0.87353515625\n",
      "2019-04-10T00:50:59.111008: step 932, loss 0.4990581274032593, accuracy 0.83349609375\n",
      "2019-04-10T00:50:59.166271: step 933, loss 0.4778916835784912, accuracy 0.81689453125\n",
      "2019-04-10T00:50:59.223062: step 934, loss 0.39171895384788513, accuracy 0.86669921875\n",
      "2019-04-10T00:50:59.282880: step 935, loss 0.5240862369537354, accuracy 0.83642578125\n",
      "2019-04-10T00:50:59.341323: step 936, loss 0.4866068959236145, accuracy 0.8134765625\n",
      "2019-04-10T00:50:59.412087: step 937, loss 0.4616938829421997, accuracy 0.8466796875\n",
      "2019-04-10T00:50:59.469253: step 938, loss 0.5286960601806641, accuracy 0.830078125\n",
      "2019-04-10T00:50:59.537242: step 939, loss 0.3956587016582489, accuracy 0.86328125\n",
      "2019-04-10T00:50:59.594292: step 940, loss 0.5144463777542114, accuracy 0.82666015625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:50:59.840871: step 940, loss 0.480261, accuracy 0.83203125\n",
      "\n",
      "\n",
      "2019-04-10T00:50:59.902351: step 941, loss 0.6256402730941772, accuracy 0.81689453125\n",
      "2019-04-10T00:50:59.960137: step 942, loss 0.47567540407180786, accuracy 0.81982421875\n",
      "2019-04-10T00:51:00.018345: step 943, loss 0.4446408450603485, accuracy 0.8564453125\n",
      "2019-04-10T00:51:00.077446: step 944, loss 0.49358928203582764, accuracy 0.83984375\n",
      "2019-04-10T00:51:00.134054: step 945, loss 0.5438310503959656, accuracy 0.81689453125\n",
      "2019-04-10T00:51:00.189778: step 946, loss 0.4545656442642212, accuracy 0.853515625\n",
      "2019-04-10T00:51:00.244766: step 947, loss 0.5112015008926392, accuracy 0.83349609375\n",
      "2019-04-10T00:51:00.304933: step 948, loss 0.5180417895317078, accuracy 0.830078125\n",
      "2019-04-10T00:51:00.361050: step 949, loss 0.5796651840209961, accuracy 0.81005859375\n",
      "2019-04-10T00:51:00.418870: step 950, loss 0.44982290267944336, accuracy 0.83349609375\n",
      "2019-04-10T00:51:00.474917: step 951, loss 0.46969136595726013, accuracy 0.82666015625\n",
      "2019-04-10T00:51:00.533855: step 952, loss 0.5380240678787231, accuracy 0.8134765625\n",
      "2019-04-10T00:51:00.601252: step 953, loss 0.43213364481925964, accuracy 0.83984375\n",
      "2019-04-10T00:51:00.666849: step 954, loss 0.4893595278263092, accuracy 0.8232421875\n",
      "2019-04-10T00:51:00.720912: step 955, loss 0.5147405862808228, accuracy 0.830078125\n",
      "2019-04-10T00:51:00.779502: step 956, loss 0.3645060956478119, accuracy 0.8935546875\n",
      "2019-04-10T00:51:00.836533: step 957, loss 0.4553337097167969, accuracy 0.83642578125\n",
      "2019-04-10T00:51:00.895531: step 958, loss 0.48547032475471497, accuracy 0.853515625\n",
      "2019-04-10T00:51:00.955387: step 959, loss 0.4518739879131317, accuracy 0.830078125\n",
      "2019-04-10T00:51:01.010782: step 960, loss 0.4151553809642792, accuracy 0.83349609375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:01.248961: step 960, loss 0.467565, accuracy 0.84326171875\n",
      "\n",
      "\n",
      "2019-04-10T00:51:01.307327: step 961, loss 0.5656740665435791, accuracy 0.81689453125\n",
      "2019-04-10T00:51:01.364383: step 962, loss 0.37611985206604004, accuracy 0.85986328125\n",
      "2019-04-10T00:51:01.421988: step 963, loss 0.4377995431423187, accuracy 0.830078125\n",
      "2019-04-10T00:51:01.481732: step 964, loss 0.530823290348053, accuracy 0.81005859375\n",
      "2019-04-10T00:51:01.536686: step 965, loss 0.4349125027656555, accuracy 0.8798828125\n",
      "2019-04-10T00:51:01.591779: step 966, loss 0.4392843544483185, accuracy 0.8466796875\n",
      "2019-04-10T00:51:01.646878: step 967, loss 0.4821945130825043, accuracy 0.830078125\n",
      "2019-04-10T00:51:01.703192: step 968, loss 0.5399853587150574, accuracy 0.83349609375\n",
      "2019-04-10T00:51:01.759130: step 969, loss 0.39599931240081787, accuracy 0.86328125\n",
      "2019-04-10T00:51:01.815895: step 970, loss 0.4439249634742737, accuracy 0.853515625\n",
      "2019-04-10T00:51:01.870291: step 971, loss 0.4630156457424164, accuracy 0.83349609375\n",
      "2019-04-10T00:51:01.924336: step 972, loss 0.40366798639297485, accuracy 0.86669921875\n",
      "2019-04-10T00:51:01.978264: step 973, loss 0.5537174940109253, accuracy 0.806640625\n",
      "2019-04-10T00:51:02.036084: step 974, loss 0.482538104057312, accuracy 0.83984375\n",
      "2019-04-10T00:51:02.091639: step 975, loss 0.4824539124965668, accuracy 0.806640625\n",
      "2019-04-10T00:51:02.148884: step 976, loss 0.44508951902389526, accuracy 0.85009765625\n",
      "2019-04-10T00:51:02.204830: step 977, loss 0.46451541781425476, accuracy 0.85009765625\n",
      "2019-04-10T00:51:02.261246: step 978, loss 0.47095656394958496, accuracy 0.8232421875\n",
      "2019-04-10T00:51:02.322922: step 979, loss 0.48911920189857483, accuracy 0.83642578125\n",
      "2019-04-10T00:51:02.387984: step 980, loss 0.4718760550022125, accuracy 0.83349609375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:02.640510: step 980, loss 0.467576, accuracy 0.845703125\n",
      "\n",
      "\n",
      "2019-04-10T00:51:02.698654: step 981, loss 0.4192800521850586, accuracy 0.86328125\n",
      "2019-04-10T00:51:02.757937: step 982, loss 0.4978850185871124, accuracy 0.8466796875\n",
      "2019-04-10T00:51:02.814395: step 983, loss 0.426786333322525, accuracy 0.85986328125\n",
      "2019-04-10T00:51:02.873128: step 984, loss 0.48831889033317566, accuracy 0.8466796875\n",
      "2019-04-10T00:51:02.931574: step 985, loss 0.5935898423194885, accuracy 0.7998046875\n",
      "2019-04-10T00:51:02.989058: step 986, loss 0.45435354113578796, accuracy 0.82666015625\n",
      "2019-04-10T00:51:03.045658: step 987, loss 0.45275554060935974, accuracy 0.83984375\n",
      "2019-04-10T00:51:03.105096: step 988, loss 0.5099091529846191, accuracy 0.81982421875\n",
      "2019-04-10T00:51:03.160193: step 989, loss 0.5062618851661682, accuracy 0.83642578125\n",
      "2019-04-10T00:51:03.217577: step 990, loss 0.4748595654964447, accuracy 0.84326171875\n",
      "2019-04-10T00:51:03.272409: step 991, loss 0.4912545382976532, accuracy 0.83349609375\n",
      "2019-04-10T00:51:03.329916: step 992, loss 0.47841358184814453, accuracy 0.83642578125\n",
      "2019-04-10T00:51:03.386975: step 993, loss 0.518887996673584, accuracy 0.806640625\n",
      "2019-04-10T00:51:03.446458: step 994, loss 0.3871559500694275, accuracy 0.87646484375\n",
      "2019-04-10T00:51:03.502821: step 995, loss 0.46213674545288086, accuracy 0.86328125\n",
      "2019-04-10T00:51:03.558594: step 996, loss 0.6193907856941223, accuracy 0.783203125\n",
      "2019-04-10T00:51:03.613617: step 997, loss 0.4730863869190216, accuracy 0.83984375\n",
      "2019-04-10T00:51:03.669188: step 998, loss 0.4877946078777313, accuracy 0.796875\n",
      "2019-04-10T00:51:03.726466: step 999, loss 0.473753422498703, accuracy 0.82666015625\n",
      "2019-04-10T00:51:03.781454: step 1000, loss 0.47717708349227905, accuracy 0.8466796875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:04.016071: step 1000, loss 0.455548, accuracy 0.8505859375\n",
      "\n",
      "\n",
      "2019-04-10T00:51:04.074148: step 1001, loss 0.5294122099876404, accuracy 0.830078125\n",
      "2019-04-10T00:51:04.130177: step 1002, loss 0.48344671726226807, accuracy 0.83642578125\n",
      "2019-04-10T00:51:04.184615: step 1003, loss 0.4721927344799042, accuracy 0.84326171875\n",
      "2019-04-10T00:51:04.240210: step 1004, loss 0.4482462704181671, accuracy 0.84326171875\n",
      "2019-04-10T00:51:04.297050: step 1005, loss 0.4017118215560913, accuracy 0.83984375\n",
      "2019-04-10T00:51:04.352149: step 1006, loss 0.4199206233024597, accuracy 0.84326171875\n",
      "2019-04-10T00:51:04.407565: step 1007, loss 0.5142595767974854, accuracy 0.83349609375\n",
      "2019-04-10T00:51:04.463914: step 1008, loss 0.40886613726615906, accuracy 0.8466796875\n",
      "2019-04-10T00:51:04.519543: step 1009, loss 0.41428709030151367, accuracy 0.87646484375\n",
      "2019-04-10T00:51:04.576652: step 1010, loss 0.4854428172111511, accuracy 0.830078125\n",
      "2019-04-10T00:51:04.631009: step 1011, loss 0.6175277233123779, accuracy 0.77001953125\n",
      "2019-04-10T00:51:04.686854: step 1012, loss 0.4138745963573456, accuracy 0.87353515625\n",
      "2019-04-10T00:51:04.741130: step 1013, loss 0.48879969120025635, accuracy 0.84326171875\n",
      "2019-04-10T00:51:04.794490: step 1014, loss 0.5759979486465454, accuracy 0.8232421875\n",
      "2019-04-10T00:51:04.850583: step 1015, loss 0.4459782838821411, accuracy 0.8466796875\n",
      "2019-04-10T00:51:04.906948: step 1016, loss 0.43486204743385315, accuracy 0.8466796875\n",
      "2019-04-10T00:51:04.961525: step 1017, loss 0.4681127965450287, accuracy 0.81982421875\n",
      "2019-04-10T00:51:05.018397: step 1018, loss 0.4695728123188019, accuracy 0.83642578125\n",
      "2019-04-10T00:51:05.073108: step 1019, loss 0.4436423182487488, accuracy 0.85009765625\n",
      "2019-04-10T00:51:05.127165: step 1020, loss 0.5302209854125977, accuracy 0.82666015625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:05.361611: step 1020, loss 0.460716, accuracy 0.8427734375\n",
      "\n",
      "\n",
      "2019-04-10T00:51:05.423465: step 1021, loss 0.4240070879459381, accuracy 0.83349609375\n",
      "2019-04-10T00:51:05.479563: step 1022, loss 0.46409520506858826, accuracy 0.8466796875\n",
      "2019-04-10T00:51:05.534078: step 1023, loss 0.5219637751579285, accuracy 0.83349609375\n",
      "2019-04-10T00:51:05.589512: step 1024, loss 0.5221465229988098, accuracy 0.81982421875\n",
      "2019-04-10T00:51:05.645835: step 1025, loss 0.5848761200904846, accuracy 0.796875\n",
      "2019-04-10T00:51:05.703219: step 1026, loss 0.44125309586524963, accuracy 0.81005859375\n",
      "2019-04-10T00:51:05.756761: step 1027, loss 0.5148044228553772, accuracy 0.81982421875\n",
      "2019-04-10T00:51:05.819431: step 1028, loss 0.4723972976207733, accuracy 0.8466796875\n",
      "2019-04-10T00:51:05.875920: step 1029, loss 0.5439385771751404, accuracy 0.84326171875\n",
      "2019-04-10T00:51:05.931721: step 1030, loss 0.5077504515647888, accuracy 0.830078125\n",
      "2019-04-10T00:51:05.995298: step 1031, loss 0.506889820098877, accuracy 0.830078125\n",
      "2019-04-10T00:51:06.053694: step 1032, loss 0.5995239615440369, accuracy 0.7998046875\n",
      "2019-04-10T00:51:06.108715: step 1033, loss 0.49029311537742615, accuracy 0.8564453125\n",
      "2019-04-10T00:51:06.162757: step 1034, loss 0.38090285658836365, accuracy 0.8798828125\n",
      "2019-04-10T00:51:06.218931: step 1035, loss 0.49690166115760803, accuracy 0.81982421875\n",
      "2019-04-10T00:51:06.272746: step 1036, loss 0.37005072832107544, accuracy 0.8798828125\n",
      "2019-04-10T00:51:06.327489: step 1037, loss 0.4760760962963104, accuracy 0.8564453125\n",
      "2019-04-10T00:51:06.382883: step 1038, loss 0.47100505232810974, accuracy 0.8466796875\n",
      "2019-04-10T00:51:06.437977: step 1039, loss 0.4943959414958954, accuracy 0.8232421875\n",
      "2019-04-10T00:51:06.495493: step 1040, loss 0.4557921886444092, accuracy 0.84326171875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:06.764341: step 1040, loss 0.452561, accuracy 0.845703125\n",
      "\n",
      "\n",
      "2019-04-10T00:51:06.820512: step 1041, loss 0.46060121059417725, accuracy 0.853515625\n",
      "2019-04-10T00:51:06.875708: step 1042, loss 0.4287169277667999, accuracy 0.86669921875\n",
      "2019-04-10T00:51:06.930732: step 1043, loss 0.3472954332828522, accuracy 0.86669921875\n",
      "2019-04-10T00:51:06.988383: step 1044, loss 0.47303086519241333, accuracy 0.81005859375\n",
      "2019-04-10T00:51:07.043016: step 1045, loss 0.46265777945518494, accuracy 0.8466796875\n",
      "2019-04-10T00:51:07.106392: step 1046, loss 0.5084354281425476, accuracy 0.81005859375\n",
      "2019-04-10T00:51:07.182508: step 1047, loss 0.5896697044372559, accuracy 0.7900390625\n",
      "2019-04-10T00:51:07.254678: step 1048, loss 0.45108234882354736, accuracy 0.83642578125\n",
      "2019-04-10T00:51:07.322883: step 1049, loss 0.5257085561752319, accuracy 0.8232421875\n",
      "2019-04-10T00:51:07.386262: step 1050, loss 0.4590338170528412, accuracy 0.87353515625\n",
      "2019-04-10T00:51:07.448834: step 1051, loss 0.4448242783546448, accuracy 0.8564453125\n",
      "2019-04-10T00:51:07.508787: step 1052, loss 0.5596229434013367, accuracy 0.78662109375\n",
      "2019-04-10T00:51:07.571194: step 1053, loss 0.447348028421402, accuracy 0.85009765625\n",
      "2019-04-10T00:51:07.626053: step 1054, loss 0.535324215888977, accuracy 0.80322265625\n",
      "2019-04-10T00:51:07.682427: step 1055, loss 0.43570342659950256, accuracy 0.85986328125\n",
      "2019-04-10T00:51:07.739215: step 1056, loss 0.46478602290153503, accuracy 0.83642578125\n",
      "2019-04-10T00:51:07.806660: step 1057, loss 0.5394900441169739, accuracy 0.7900390625\n",
      "2019-04-10T00:51:07.869293: step 1058, loss 0.43431881070137024, accuracy 0.85986328125\n",
      "2019-04-10T00:51:07.925780: step 1059, loss 0.5190913677215576, accuracy 0.81689453125\n",
      "2019-04-10T00:51:07.981788: step 1060, loss 0.5289538502693176, accuracy 0.81689453125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:08.220226: step 1060, loss 0.46309, accuracy 0.84326171875\n",
      "\n",
      "\n",
      "2019-04-10T00:51:08.291827: step 1061, loss 0.4217526912689209, accuracy 0.8564453125\n",
      "2019-04-10T00:51:08.352350: step 1062, loss 0.44714751839637756, accuracy 0.82666015625\n",
      "2019-04-10T00:51:08.411629: step 1063, loss 0.5065798163414001, accuracy 0.83642578125\n",
      "2019-04-10T00:51:08.472153: step 1064, loss 0.47694188356399536, accuracy 0.84326171875\n",
      "2019-04-10T00:51:08.538840: step 1065, loss 0.4663339853286743, accuracy 0.82666015625\n",
      "2019-04-10T00:51:08.600093: step 1066, loss 0.47049790620803833, accuracy 0.87646484375\n",
      "2019-04-10T00:51:08.654855: step 1067, loss 0.4454217553138733, accuracy 0.85009765625\n",
      "2019-04-10T00:51:08.709351: step 1068, loss 0.4983367323875427, accuracy 0.83349609375\n",
      "2019-04-10T00:51:08.765127: step 1069, loss 0.41788148880004883, accuracy 0.84326171875\n",
      "2019-04-10T00:51:08.818315: step 1070, loss 0.45415231585502625, accuracy 0.8564453125\n",
      "2019-04-10T00:51:08.872385: step 1071, loss 0.45463690161705017, accuracy 0.830078125\n",
      "2019-04-10T00:51:08.930362: step 1072, loss 0.5232462882995605, accuracy 0.806640625\n",
      "2019-04-10T00:51:08.986931: step 1073, loss 0.4551341235637665, accuracy 0.83349609375\n",
      "2019-04-10T00:51:09.040695: step 1074, loss 0.5063242316246033, accuracy 0.83984375\n",
      "2019-04-10T00:51:09.096207: step 1075, loss 0.5061277747154236, accuracy 0.8232421875\n",
      "2019-04-10T00:51:09.163165: step 1076, loss 0.46257904171943665, accuracy 0.83984375\n",
      "2019-04-10T00:51:09.226055: step 1077, loss 0.4828919470310211, accuracy 0.8466796875\n",
      "2019-04-10T00:51:09.284457: step 1078, loss 0.47620198130607605, accuracy 0.83349609375\n",
      "2019-04-10T00:51:09.341188: step 1079, loss 0.39946627616882324, accuracy 0.85009765625\n",
      "2019-04-10T00:51:09.369343: step 1080, loss 0.4219951331615448, accuracy 0.8798828125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:09.614317: step 1080, loss 0.465674, accuracy 0.8447265625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:09.786636: step 1081, loss 0.6088016629219055, accuracy 0.806640625\n",
      "2019-04-10T00:51:09.841939: step 1082, loss 0.5172519087791443, accuracy 0.83349609375\n",
      "2019-04-10T00:51:09.898293: step 1083, loss 0.4543858468532562, accuracy 0.85986328125\n",
      "2019-04-10T00:51:09.954195: step 1084, loss 0.5512523651123047, accuracy 0.80322265625\n",
      "2019-04-10T00:51:10.007790: step 1085, loss 0.5502074956893921, accuracy 0.80322265625\n",
      "2019-04-10T00:51:10.061650: step 1086, loss 0.5003544688224792, accuracy 0.830078125\n",
      "2019-04-10T00:51:10.127073: step 1087, loss 0.4921621084213257, accuracy 0.8232421875\n",
      "2019-04-10T00:51:10.184692: step 1088, loss 0.5131728053092957, accuracy 0.830078125\n",
      "2019-04-10T00:51:10.241634: step 1089, loss 0.466261088848114, accuracy 0.830078125\n",
      "2019-04-10T00:51:10.298474: step 1090, loss 0.4091360569000244, accuracy 0.85009765625\n",
      "2019-04-10T00:51:10.358090: step 1091, loss 0.4980495274066925, accuracy 0.83984375\n",
      "2019-04-10T00:51:10.420403: step 1092, loss 0.38091444969177246, accuracy 0.85986328125\n",
      "2019-04-10T00:51:10.476435: step 1093, loss 0.5206006169319153, accuracy 0.853515625\n",
      "2019-04-10T00:51:10.532164: step 1094, loss 0.5218068361282349, accuracy 0.830078125\n",
      "2019-04-10T00:51:10.588565: step 1095, loss 0.46403858065605164, accuracy 0.8134765625\n",
      "2019-04-10T00:51:10.647690: step 1096, loss 0.41002795100212097, accuracy 0.84326171875\n",
      "2019-04-10T00:51:10.701545: step 1097, loss 0.45529302954673767, accuracy 0.8701171875\n",
      "2019-04-10T00:51:10.753760: step 1098, loss 0.5323036909103394, accuracy 0.83349609375\n",
      "2019-04-10T00:51:10.805766: step 1099, loss 0.4293444752693176, accuracy 0.853515625\n",
      "2019-04-10T00:51:10.861364: step 1100, loss 0.4690708816051483, accuracy 0.8134765625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:11.085652: step 1100, loss 0.465992, accuracy 0.8427734375\n",
      "\n",
      "\n",
      "2019-04-10T00:51:11.143463: step 1101, loss 0.5489938259124756, accuracy 0.7998046875\n",
      "2019-04-10T00:51:11.198568: step 1102, loss 0.4787021279335022, accuracy 0.83642578125\n",
      "2019-04-10T00:51:11.252918: step 1103, loss 0.4509090781211853, accuracy 0.83984375\n",
      "2019-04-10T00:51:11.314272: step 1104, loss 0.5153930187225342, accuracy 0.830078125\n",
      "2019-04-10T00:51:11.371650: step 1105, loss 0.41063329577445984, accuracy 0.84326171875\n",
      "2019-04-10T00:51:11.428382: step 1106, loss 0.5023275017738342, accuracy 0.82666015625\n",
      "2019-04-10T00:51:11.484247: step 1107, loss 0.43812325596809387, accuracy 0.82666015625\n",
      "2019-04-10T00:51:11.539801: step 1108, loss 0.4657992422580719, accuracy 0.8232421875\n",
      "2019-04-10T00:51:11.604592: step 1109, loss 0.42334213852882385, accuracy 0.86669921875\n",
      "2019-04-10T00:51:11.660480: step 1110, loss 0.40814945101737976, accuracy 0.8564453125\n",
      "2019-04-10T00:51:11.716179: step 1111, loss 0.46906977891921997, accuracy 0.81689453125\n",
      "2019-04-10T00:51:11.771248: step 1112, loss 0.46787047386169434, accuracy 0.83984375\n",
      "2019-04-10T00:51:11.830080: step 1113, loss 0.4714947044849396, accuracy 0.81982421875\n",
      "2019-04-10T00:51:11.894075: step 1114, loss 0.4982883632183075, accuracy 0.830078125\n",
      "2019-04-10T00:51:11.952661: step 1115, loss 0.46595773100852966, accuracy 0.81005859375\n",
      "2019-04-10T00:51:12.008539: step 1116, loss 0.42765140533447266, accuracy 0.8466796875\n",
      "2019-04-10T00:51:12.062567: step 1117, loss 0.5590051412582397, accuracy 0.8134765625\n",
      "2019-04-10T00:51:12.117898: step 1118, loss 0.47462764382362366, accuracy 0.83984375\n",
      "2019-04-10T00:51:12.180733: step 1119, loss 0.5737998485565186, accuracy 0.79345703125\n",
      "2019-04-10T00:51:12.242450: step 1120, loss 0.5479576587677002, accuracy 0.80322265625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:12.469513: step 1120, loss 0.468446, accuracy 0.83984375\n",
      "\n",
      "\n",
      "2019-04-10T00:51:12.528600: step 1121, loss 0.4950549304485321, accuracy 0.806640625\n",
      "2019-04-10T00:51:12.586022: step 1122, loss 0.5047998428344727, accuracy 0.7900390625\n",
      "2019-04-10T00:51:12.651097: step 1123, loss 0.538263738155365, accuracy 0.8466796875\n",
      "2019-04-10T00:51:12.707889: step 1124, loss 0.434640496969223, accuracy 0.86328125\n",
      "2019-04-10T00:51:12.766064: step 1125, loss 0.5192593932151794, accuracy 0.8466796875\n",
      "2019-04-10T00:51:12.823719: step 1126, loss 0.4784073531627655, accuracy 0.83984375\n",
      "2019-04-10T00:51:12.886357: step 1127, loss 0.37743687629699707, accuracy 0.87646484375\n",
      "2019-04-10T00:51:12.948435: step 1128, loss 0.4899250268936157, accuracy 0.83349609375\n",
      "2019-04-10T00:51:13.014206: step 1129, loss 0.47256168723106384, accuracy 0.83642578125\n",
      "2019-04-10T00:51:13.070383: step 1130, loss 0.46476423740386963, accuracy 0.8466796875\n",
      "2019-04-10T00:51:13.128172: step 1131, loss 0.43204936385154724, accuracy 0.8701171875\n",
      "2019-04-10T00:51:13.183919: step 1132, loss 0.41385161876678467, accuracy 0.853515625\n",
      "2019-04-10T00:51:13.239646: step 1133, loss 0.4110662639141083, accuracy 0.8564453125\n",
      "2019-04-10T00:51:13.295454: step 1134, loss 0.42565977573394775, accuracy 0.87353515625\n",
      "2019-04-10T00:51:13.361784: step 1135, loss 0.4116547107696533, accuracy 0.8466796875\n",
      "2019-04-10T00:51:13.417516: step 1136, loss 0.5178681015968323, accuracy 0.82666015625\n",
      "2019-04-10T00:51:13.477372: step 1137, loss 0.4992228150367737, accuracy 0.83349609375\n",
      "2019-04-10T00:51:13.531797: step 1138, loss 0.5416373610496521, accuracy 0.796875\n",
      "2019-04-10T00:51:13.586815: step 1139, loss 0.45982834696769714, accuracy 0.83642578125\n",
      "2019-04-10T00:51:13.644530: step 1140, loss 0.557767927646637, accuracy 0.80322265625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:13.880764: step 1140, loss 0.461807, accuracy 0.84521484375\n",
      "\n",
      "\n",
      "2019-04-10T00:51:13.943144: step 1141, loss 0.44503509998321533, accuracy 0.84326171875\n",
      "2019-04-10T00:51:14.003093: step 1142, loss 0.4541104733943939, accuracy 0.8232421875\n",
      "2019-04-10T00:51:14.064192: step 1143, loss 0.4797119200229645, accuracy 0.8466796875\n",
      "2019-04-10T00:51:14.116922: step 1144, loss 0.40073472261428833, accuracy 0.86669921875\n",
      "2019-04-10T00:51:14.174425: step 1145, loss 0.45931771397590637, accuracy 0.82666015625\n",
      "2019-04-10T00:51:14.230581: step 1146, loss 0.4569118618965149, accuracy 0.853515625\n",
      "2019-04-10T00:51:14.287570: step 1147, loss 0.5492459535598755, accuracy 0.82666015625\n",
      "2019-04-10T00:51:14.347926: step 1148, loss 0.5383713841438293, accuracy 0.81982421875\n",
      "2019-04-10T00:51:14.403587: step 1149, loss 0.4764678478240967, accuracy 0.8232421875\n",
      "2019-04-10T00:51:14.457830: step 1150, loss 0.4812834560871124, accuracy 0.85009765625\n",
      "2019-04-10T00:51:14.514279: step 1151, loss 0.5399267673492432, accuracy 0.796875\n",
      "2019-04-10T00:51:14.568989: step 1152, loss 0.4553535580635071, accuracy 0.85009765625\n",
      "2019-04-10T00:51:14.627386: step 1153, loss 0.4684952199459076, accuracy 0.8466796875\n",
      "2019-04-10T00:51:14.687777: step 1154, loss 0.4643520712852478, accuracy 0.8232421875\n",
      "2019-04-10T00:51:14.745283: step 1155, loss 0.40583059191703796, accuracy 0.86328125\n",
      "2019-04-10T00:51:14.800896: step 1156, loss 0.4438367187976837, accuracy 0.8564453125\n",
      "2019-04-10T00:51:14.857023: step 1157, loss 0.4502106308937073, accuracy 0.84326171875\n",
      "2019-04-10T00:51:14.914553: step 1158, loss 0.4730256199836731, accuracy 0.81689453125\n",
      "2019-04-10T00:51:14.977370: step 1159, loss 0.5229296088218689, accuracy 0.83984375\n",
      "2019-04-10T00:51:15.032871: step 1160, loss 0.4313880503177643, accuracy 0.83984375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:15.265516: step 1160, loss 0.439473, accuracy 0.85205078125\n",
      "\n",
      "\n",
      "2019-04-10T00:51:15.330379: step 1161, loss 0.5253636837005615, accuracy 0.82666015625\n",
      "2019-04-10T00:51:15.387197: step 1162, loss 0.4347722828388214, accuracy 0.8466796875\n",
      "2019-04-10T00:51:15.443281: step 1163, loss 0.401784211397171, accuracy 0.86328125\n",
      "2019-04-10T00:51:15.498250: step 1164, loss 0.49967509508132935, accuracy 0.806640625\n",
      "2019-04-10T00:51:15.556677: step 1165, loss 0.45682668685913086, accuracy 0.85009765625\n",
      "2019-04-10T00:51:15.617423: step 1166, loss 0.4429573714733124, accuracy 0.83642578125\n",
      "2019-04-10T00:51:15.676281: step 1167, loss 0.47881805896759033, accuracy 0.82666015625\n",
      "2019-04-10T00:51:15.730997: step 1168, loss 0.4942115247249603, accuracy 0.8466796875\n",
      "2019-04-10T00:51:15.790607: step 1169, loss 0.44229960441589355, accuracy 0.85986328125\n",
      "2019-04-10T00:51:15.845914: step 1170, loss 0.4356728494167328, accuracy 0.853515625\n",
      "2019-04-10T00:51:15.908913: step 1171, loss 0.4920397996902466, accuracy 0.8232421875\n",
      "2019-04-10T00:51:15.967764: step 1172, loss 0.4807650148868561, accuracy 0.83349609375\n",
      "2019-04-10T00:51:16.025534: step 1173, loss 0.4573507308959961, accuracy 0.81982421875\n",
      "2019-04-10T00:51:16.081676: step 1174, loss 0.4056781828403473, accuracy 0.87353515625\n",
      "2019-04-10T00:51:16.136785: step 1175, loss 0.5111358761787415, accuracy 0.8134765625\n",
      "2019-04-10T00:51:16.194767: step 1176, loss 0.49685677886009216, accuracy 0.82666015625\n",
      "2019-04-10T00:51:16.257618: step 1177, loss 0.4834968149662018, accuracy 0.84326171875\n",
      "2019-04-10T00:51:16.314671: step 1178, loss 0.43782171607017517, accuracy 0.853515625\n",
      "2019-04-10T00:51:16.370838: step 1179, loss 0.43474721908569336, accuracy 0.85009765625\n",
      "2019-04-10T00:51:16.425728: step 1180, loss 0.3480865955352783, accuracy 0.89013671875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:16.664573: step 1180, loss 0.474944, accuracy 0.8349609375\n",
      "\n",
      "\n",
      "2019-04-10T00:51:16.721661: step 1181, loss 0.428488552570343, accuracy 0.8232421875\n",
      "2019-04-10T00:51:16.774448: step 1182, loss 0.47835657000541687, accuracy 0.81982421875\n",
      "2019-04-10T00:51:16.829158: step 1183, loss 0.5138892531394958, accuracy 0.8232421875\n",
      "2019-04-10T00:51:16.882369: step 1184, loss 0.40484902262687683, accuracy 0.83349609375\n",
      "2019-04-10T00:51:16.941590: step 1185, loss 0.49791067838668823, accuracy 0.81005859375\n",
      "2019-04-10T00:51:16.996447: step 1186, loss 0.42707493901252747, accuracy 0.84326171875\n",
      "2019-04-10T00:51:17.054946: step 1187, loss 0.3739217519760132, accuracy 0.87646484375\n",
      "2019-04-10T00:51:17.108282: step 1188, loss 0.4931866526603699, accuracy 0.81005859375\n",
      "2019-04-10T00:51:17.165214: step 1189, loss 0.3970920741558075, accuracy 0.85986328125\n",
      "2019-04-10T00:51:17.218620: step 1190, loss 0.4398745596408844, accuracy 0.8564453125\n",
      "2019-04-10T00:51:17.279729: step 1191, loss 0.5240088701248169, accuracy 0.82666015625\n",
      "2019-04-10T00:51:17.335941: step 1192, loss 0.44895267486572266, accuracy 0.830078125\n",
      "2019-04-10T00:51:17.391704: step 1193, loss 0.3923362195491791, accuracy 0.8701171875\n",
      "2019-04-10T00:51:17.448262: step 1194, loss 0.47989821434020996, accuracy 0.8232421875\n",
      "2019-04-10T00:51:17.506669: step 1195, loss 0.481443852186203, accuracy 0.83642578125\n",
      "2019-04-10T00:51:17.566906: step 1196, loss 0.43338948488235474, accuracy 0.83642578125\n",
      "2019-04-10T00:51:17.625085: step 1197, loss 0.47238439321517944, accuracy 0.83349609375\n",
      "2019-04-10T00:51:17.678807: step 1198, loss 0.5159743428230286, accuracy 0.81005859375\n",
      "2019-04-10T00:51:17.737000: step 1199, loss 0.4283003807067871, accuracy 0.8466796875\n",
      "2019-04-10T00:51:17.795600: step 1200, loss 0.4811770021915436, accuracy 0.82666015625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:18.029284: step 1200, loss 0.451817, accuracy 0.8525390625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:18.085908: step 1201, loss 0.46045511960983276, accuracy 0.84326171875\n",
      "2019-04-10T00:51:18.138636: step 1202, loss 0.4692472815513611, accuracy 0.83642578125\n",
      "2019-04-10T00:51:18.193673: step 1203, loss 0.46964532136917114, accuracy 0.830078125\n",
      "2019-04-10T00:51:18.253742: step 1204, loss 0.49367114901542664, accuracy 0.81982421875\n",
      "2019-04-10T00:51:18.310728: step 1205, loss 0.4884125292301178, accuracy 0.8564453125\n",
      "2019-04-10T00:51:18.364018: step 1206, loss 0.560738205909729, accuracy 0.806640625\n",
      "2019-04-10T00:51:18.417245: step 1207, loss 0.47942373156547546, accuracy 0.8232421875\n",
      "2019-04-10T00:51:18.471978: step 1208, loss 0.5781773328781128, accuracy 0.82666015625\n",
      "2019-04-10T00:51:18.529151: step 1209, loss 0.5134328603744507, accuracy 0.830078125\n",
      "2019-04-10T00:51:18.588445: step 1210, loss 0.4888140857219696, accuracy 0.796875\n",
      "2019-04-10T00:51:18.642373: step 1211, loss 0.4727555215358734, accuracy 0.8701171875\n",
      "2019-04-10T00:51:18.697753: step 1212, loss 0.49392640590667725, accuracy 0.83642578125\n",
      "2019-04-10T00:51:18.752476: step 1213, loss 0.5287715196609497, accuracy 0.8232421875\n",
      "2019-04-10T00:51:18.808332: step 1214, loss 0.521155595779419, accuracy 0.82666015625\n",
      "2019-04-10T00:51:18.864531: step 1215, loss 0.4416644871234894, accuracy 0.8564453125\n",
      "2019-04-10T00:51:18.923184: step 1216, loss 0.4952008128166199, accuracy 0.830078125\n",
      "2019-04-10T00:51:18.976768: step 1217, loss 0.45043009519577026, accuracy 0.85009765625\n",
      "2019-04-10T00:51:19.033119: step 1218, loss 0.4747784435749054, accuracy 0.83349609375\n",
      "2019-04-10T00:51:19.087940: step 1219, loss 0.5080594420433044, accuracy 0.8134765625\n",
      "2019-04-10T00:51:19.145633: step 1220, loss 0.4590262770652771, accuracy 0.85009765625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:19.368874: step 1220, loss 0.462175, accuracy 0.84716796875\n",
      "\n",
      "\n",
      "2019-04-10T00:51:19.428223: step 1221, loss 0.48295170068740845, accuracy 0.8564453125\n",
      "2019-04-10T00:51:19.483275: step 1222, loss 0.4473838806152344, accuracy 0.8564453125\n",
      "2019-04-10T00:51:19.536709: step 1223, loss 0.46360981464385986, accuracy 0.8466796875\n",
      "2019-04-10T00:51:19.589920: step 1224, loss 0.5196772217750549, accuracy 0.8232421875\n",
      "2019-04-10T00:51:19.646274: step 1225, loss 0.42650675773620605, accuracy 0.82666015625\n",
      "2019-04-10T00:51:19.705520: step 1226, loss 0.462466835975647, accuracy 0.84326171875\n",
      "2019-04-10T00:51:19.760486: step 1227, loss 0.49755290150642395, accuracy 0.81689453125\n",
      "2019-04-10T00:51:19.813101: step 1228, loss 0.43807974457740784, accuracy 0.86328125\n",
      "2019-04-10T00:51:19.868215: step 1229, loss 0.4363645315170288, accuracy 0.85986328125\n",
      "2019-04-10T00:51:19.923046: step 1230, loss 0.3989008665084839, accuracy 0.8564453125\n",
      "2019-04-10T00:51:19.978214: step 1231, loss 0.5054785013198853, accuracy 0.83642578125\n",
      "2019-04-10T00:51:20.036160: step 1232, loss 0.5473340749740601, accuracy 0.8232421875\n",
      "2019-04-10T00:51:20.090370: step 1233, loss 0.6019500494003296, accuracy 0.7998046875\n",
      "2019-04-10T00:51:20.142936: step 1234, loss 0.5346450805664062, accuracy 0.8232421875\n",
      "2019-04-10T00:51:20.196559: step 1235, loss 0.5071479678153992, accuracy 0.8232421875\n",
      "2019-04-10T00:51:20.248570: step 1236, loss 0.433790385723114, accuracy 0.8701171875\n",
      "2019-04-10T00:51:20.303147: step 1237, loss 0.3930125832557678, accuracy 0.853515625\n",
      "2019-04-10T00:51:20.361141: step 1238, loss 0.5623841881752014, accuracy 0.81005859375\n",
      "2019-04-10T00:51:20.413777: step 1239, loss 0.288887083530426, accuracy 0.87646484375\n",
      "2019-04-10T00:51:20.465688: step 1240, loss 0.5373255014419556, accuracy 0.806640625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:20.693362: step 1240, loss 0.439974, accuracy 0.85595703125\n",
      "\n",
      "\n",
      "2019-04-10T00:51:20.750706: step 1241, loss 0.4360314905643463, accuracy 0.8466796875\n",
      "2019-04-10T00:51:20.803967: step 1242, loss 0.5094082355499268, accuracy 0.81689453125\n",
      "2019-04-10T00:51:20.855721: step 1243, loss 0.4356778562068939, accuracy 0.853515625\n",
      "2019-04-10T00:51:20.908265: step 1244, loss 0.5424518585205078, accuracy 0.81982421875\n",
      "2019-04-10T00:51:20.961217: step 1245, loss 0.4459530711174011, accuracy 0.84326171875\n",
      "2019-04-10T00:51:21.021129: step 1246, loss 0.5404775738716125, accuracy 0.80322265625\n",
      "2019-04-10T00:51:21.080379: step 1247, loss 0.46586909890174866, accuracy 0.83984375\n",
      "2019-04-10T00:51:21.133714: step 1248, loss 0.43576014041900635, accuracy 0.8564453125\n",
      "2019-04-10T00:51:21.187895: step 1249, loss 0.4529561400413513, accuracy 0.830078125\n",
      "2019-04-10T00:51:21.241842: step 1250, loss 0.4823451340198517, accuracy 0.83984375\n",
      "2019-04-10T00:51:21.299879: step 1251, loss 0.41716429591178894, accuracy 0.86328125\n",
      "2019-04-10T00:51:21.356472: step 1252, loss 0.5588958263397217, accuracy 0.8134765625\n",
      "2019-04-10T00:51:21.410688: step 1253, loss 0.4322029650211334, accuracy 0.853515625\n",
      "2019-04-10T00:51:21.465201: step 1254, loss 0.4708947241306305, accuracy 0.8134765625\n",
      "2019-04-10T00:51:21.517857: step 1255, loss 0.43731510639190674, accuracy 0.8701171875\n",
      "2019-04-10T00:51:21.572703: step 1256, loss 0.5042229294776917, accuracy 0.83642578125\n",
      "2019-04-10T00:51:21.633407: step 1257, loss 0.5660083293914795, accuracy 0.84326171875\n",
      "2019-04-10T00:51:21.686366: step 1258, loss 0.43868961930274963, accuracy 0.8701171875\n",
      "2019-04-10T00:51:21.740827: step 1259, loss 0.4562325179576874, accuracy 0.853515625\n",
      "2019-04-10T00:51:21.794498: step 1260, loss 0.45576542615890503, accuracy 0.83642578125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:22.021145: step 1260, loss 0.451246, accuracy 0.8466796875\n",
      "\n",
      "\n",
      "2019-04-10T00:51:22.078972: step 1261, loss 0.46838057041168213, accuracy 0.84326171875\n",
      "2019-04-10T00:51:22.131132: step 1262, loss 0.47433945536613464, accuracy 0.82666015625\n",
      "2019-04-10T00:51:22.183620: step 1263, loss 0.4791991114616394, accuracy 0.8466796875\n",
      "2019-04-10T00:51:22.237862: step 1264, loss 0.4809364378452301, accuracy 0.82666015625\n",
      "2019-04-10T00:51:22.292738: step 1265, loss 0.397297739982605, accuracy 0.8564453125\n",
      "2019-04-10T00:51:22.348042: step 1266, loss 0.5169717073440552, accuracy 0.8232421875\n",
      "2019-04-10T00:51:22.402561: step 1267, loss 0.42012709379196167, accuracy 0.86328125\n",
      "2019-04-10T00:51:22.456350: step 1268, loss 0.4676963686943054, accuracy 0.830078125\n",
      "2019-04-10T00:51:22.511325: step 1269, loss 0.4551170766353607, accuracy 0.8466796875\n",
      "2019-04-10T00:51:22.567766: step 1270, loss 0.46753671765327454, accuracy 0.83349609375\n",
      "2019-04-10T00:51:22.625825: step 1271, loss 0.48445531725883484, accuracy 0.83984375\n",
      "2019-04-10T00:51:22.681948: step 1272, loss 0.3796207308769226, accuracy 0.8564453125\n",
      "2019-04-10T00:51:22.736348: step 1273, loss 0.5393993258476257, accuracy 0.8466796875\n",
      "2019-04-10T00:51:22.790534: step 1274, loss 0.5030894875526428, accuracy 0.83349609375\n",
      "2019-04-10T00:51:22.847404: step 1275, loss 0.43551650643348694, accuracy 0.85986328125\n",
      "2019-04-10T00:51:22.902340: step 1276, loss 0.4738643765449524, accuracy 0.84326171875\n",
      "2019-04-10T00:51:22.955492: step 1277, loss 0.5054357051849365, accuracy 0.83642578125\n",
      "2019-04-10T00:51:23.011937: step 1278, loss 0.46442312002182007, accuracy 0.8466796875\n",
      "2019-04-10T00:51:23.064202: step 1279, loss 0.523928165435791, accuracy 0.7998046875\n",
      "2019-04-10T00:51:23.116917: step 1280, loss 0.4753151535987854, accuracy 0.83349609375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:23.350259: step 1280, loss 0.459497, accuracy 0.84375\n",
      "\n",
      "\n",
      "2019-04-10T00:51:23.409561: step 1281, loss 0.4591330587863922, accuracy 0.82666015625\n",
      "2019-04-10T00:51:23.465136: step 1282, loss 0.47576969861984253, accuracy 0.83349609375\n",
      "2019-04-10T00:51:23.614675: step 1283, loss 0.5148592591285706, accuracy 0.79345703125\n",
      "2019-04-10T00:51:23.685625: step 1284, loss 0.4089961349964142, accuracy 0.8701171875\n",
      "2019-04-10T00:51:23.738243: step 1285, loss 0.4537382125854492, accuracy 0.84326171875\n",
      "2019-04-10T00:51:23.789299: step 1286, loss 0.46515214443206787, accuracy 0.81982421875\n",
      "2019-04-10T00:51:23.841250: step 1287, loss 0.5451890826225281, accuracy 0.830078125\n",
      "2019-04-10T00:51:23.895816: step 1288, loss 0.4613984227180481, accuracy 0.83642578125\n",
      "2019-04-10T00:51:23.950280: step 1289, loss 0.47805124521255493, accuracy 0.8466796875\n",
      "2019-04-10T00:51:24.003323: step 1290, loss 0.4478954076766968, accuracy 0.86328125\n",
      "2019-04-10T00:51:24.056355: step 1291, loss 0.40816256403923035, accuracy 0.86669921875\n",
      "2019-04-10T00:51:24.111900: step 1292, loss 0.4398149251937866, accuracy 0.8466796875\n",
      "2019-04-10T00:51:24.173073: step 1293, loss 0.49950143694877625, accuracy 0.82666015625\n",
      "2019-04-10T00:51:24.232313: step 1294, loss 0.4382859766483307, accuracy 0.85009765625\n",
      "2019-04-10T00:51:24.298573: step 1295, loss 0.48852041363716125, accuracy 0.83349609375\n",
      "2019-04-10T00:51:24.354571: step 1296, loss 0.44115179777145386, accuracy 0.83984375\n",
      "2019-04-10T00:51:24.413933: step 1297, loss 0.5293837189674377, accuracy 0.81689453125\n",
      "2019-04-10T00:51:24.468860: step 1298, loss 0.5378226637840271, accuracy 0.83349609375\n",
      "2019-04-10T00:51:24.522338: step 1299, loss 0.48159265518188477, accuracy 0.8701171875\n",
      "2019-04-10T00:51:24.580262: step 1300, loss 0.605234682559967, accuracy 0.81982421875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:24.809763: step 1300, loss 0.447816, accuracy 0.8515625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:24.867763: step 1301, loss 0.5020729899406433, accuracy 0.83349609375\n",
      "2019-04-10T00:51:24.926789: step 1302, loss 0.4207487404346466, accuracy 0.85009765625\n",
      "2019-04-10T00:51:24.984912: step 1303, loss 0.5823912620544434, accuracy 0.806640625\n",
      "2019-04-10T00:51:25.041535: step 1304, loss 0.4391031861305237, accuracy 0.86669921875\n",
      "2019-04-10T00:51:25.100505: step 1305, loss 0.3229425251483917, accuracy 0.896484375\n",
      "2019-04-10T00:51:25.154069: step 1306, loss 0.41032373905181885, accuracy 0.84326171875\n",
      "2019-04-10T00:51:25.208863: step 1307, loss 0.5155108571052551, accuracy 0.84326171875\n",
      "2019-04-10T00:51:25.266301: step 1308, loss 0.5067124962806702, accuracy 0.81005859375\n",
      "2019-04-10T00:51:25.323773: step 1309, loss 0.3756266236305237, accuracy 0.88330078125\n",
      "2019-04-10T00:51:25.381441: step 1310, loss 0.41613122820854187, accuracy 0.86328125\n",
      "2019-04-10T00:51:25.438068: step 1311, loss 0.5548807978630066, accuracy 0.7998046875\n",
      "2019-04-10T00:51:25.494480: step 1312, loss 0.4463920593261719, accuracy 0.85986328125\n",
      "2019-04-10T00:51:25.550887: step 1313, loss 0.3523161709308624, accuracy 0.87646484375\n",
      "2019-04-10T00:51:25.608411: step 1314, loss 0.41445687413215637, accuracy 0.853515625\n",
      "2019-04-10T00:51:25.667073: step 1315, loss 0.37390080094337463, accuracy 0.85009765625\n",
      "2019-04-10T00:51:25.721263: step 1316, loss 0.5513104796409607, accuracy 0.83984375\n",
      "2019-04-10T00:51:25.774525: step 1317, loss 0.4422305226325989, accuracy 0.8232421875\n",
      "2019-04-10T00:51:25.825379: step 1318, loss 0.5755780339241028, accuracy 0.78662109375\n",
      "2019-04-10T00:51:25.882894: step 1319, loss 0.46613094210624695, accuracy 0.83349609375\n",
      "2019-04-10T00:51:25.941171: step 1320, loss 0.5638430118560791, accuracy 0.8232421875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:26.172347: step 1320, loss 0.459266, accuracy 0.84423828125\n",
      "\n",
      "\n",
      "2019-04-10T00:51:26.228658: step 1321, loss 0.4656885266304016, accuracy 0.8466796875\n",
      "2019-04-10T00:51:26.283762: step 1322, loss 0.5752739310264587, accuracy 0.83984375\n",
      "2019-04-10T00:51:26.341617: step 1323, loss 0.43513229489326477, accuracy 0.81982421875\n",
      "2019-04-10T00:51:26.396193: step 1324, loss 0.5173324346542358, accuracy 0.8466796875\n",
      "2019-04-10T00:51:26.449144: step 1325, loss 0.5054555535316467, accuracy 0.8232421875\n",
      "2019-04-10T00:51:26.503602: step 1326, loss 0.42788270115852356, accuracy 0.853515625\n",
      "2019-04-10T00:51:26.556283: step 1327, loss 0.34560006856918335, accuracy 0.87353515625\n",
      "2019-04-10T00:51:26.613837: step 1328, loss 0.5037322640419006, accuracy 0.81982421875\n",
      "2019-04-10T00:51:26.671760: step 1329, loss 0.4613006114959717, accuracy 0.83984375\n",
      "2019-04-10T00:51:26.727100: step 1330, loss 0.5087050199508667, accuracy 0.8232421875\n",
      "2019-04-10T00:51:26.780158: step 1331, loss 0.42377564311027527, accuracy 0.83642578125\n",
      "2019-04-10T00:51:26.834050: step 1332, loss 0.49794742465019226, accuracy 0.83642578125\n",
      "2019-04-10T00:51:26.887643: step 1333, loss 0.4018058776855469, accuracy 0.86669921875\n",
      "2019-04-10T00:51:26.943828: step 1334, loss 0.49369049072265625, accuracy 0.83642578125\n",
      "2019-04-10T00:51:26.998831: step 1335, loss 0.39608046412467957, accuracy 0.8798828125\n",
      "2019-04-10T00:51:27.054500: step 1336, loss 0.5103999972343445, accuracy 0.8134765625\n",
      "2019-04-10T00:51:27.108031: step 1337, loss 0.4919944703578949, accuracy 0.8466796875\n",
      "2019-04-10T00:51:27.163279: step 1338, loss 0.4523378610610962, accuracy 0.82666015625\n",
      "2019-04-10T00:51:27.215713: step 1339, loss 0.4235519468784332, accuracy 0.82666015625\n",
      "2019-04-10T00:51:27.271952: step 1340, loss 0.5294626951217651, accuracy 0.81689453125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:27.513765: step 1340, loss 0.451164, accuracy 0.853515625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:27.570401: step 1341, loss 0.4874657690525055, accuracy 0.85009765625\n",
      "2019-04-10T00:51:27.622841: step 1342, loss 0.4332934617996216, accuracy 0.85986328125\n",
      "2019-04-10T00:51:27.684663: step 1343, loss 0.4188975989818573, accuracy 0.853515625\n",
      "2019-04-10T00:51:27.750459: step 1344, loss 0.4634169042110443, accuracy 0.8232421875\n",
      "2019-04-10T00:51:27.805726: step 1345, loss 0.44279715418815613, accuracy 0.8466796875\n",
      "2019-04-10T00:51:27.858264: step 1346, loss 0.540523111820221, accuracy 0.82666015625\n",
      "2019-04-10T00:51:27.911966: step 1347, loss 0.4511740505695343, accuracy 0.83984375\n",
      "2019-04-10T00:51:27.970025: step 1348, loss 0.46047478914260864, accuracy 0.8466796875\n",
      "2019-04-10T00:51:28.040595: step 1349, loss 0.4245709776878357, accuracy 0.83642578125\n",
      "2019-04-10T00:51:28.099415: step 1350, loss 0.46603551506996155, accuracy 0.83642578125\n",
      "2019-04-10T00:51:28.152932: step 1351, loss 0.4029441177845001, accuracy 0.85009765625\n",
      "2019-04-10T00:51:28.206083: step 1352, loss 0.4774853587150574, accuracy 0.82666015625\n",
      "2019-04-10T00:51:28.260296: step 1353, loss 0.4791024923324585, accuracy 0.8232421875\n",
      "2019-04-10T00:51:28.314132: step 1354, loss 0.4741039574146271, accuracy 0.81982421875\n",
      "2019-04-10T00:51:28.370017: step 1355, loss 0.5015777349472046, accuracy 0.8134765625\n",
      "2019-04-10T00:51:28.427099: step 1356, loss 0.4767031967639923, accuracy 0.8564453125\n",
      "2019-04-10T00:51:28.481799: step 1357, loss 0.4207458198070526, accuracy 0.8564453125\n",
      "2019-04-10T00:51:28.538608: step 1358, loss 0.3619635999202728, accuracy 0.87646484375\n",
      "2019-04-10T00:51:28.591452: step 1359, loss 0.5166856646537781, accuracy 0.81982421875\n",
      "2019-04-10T00:51:28.643618: step 1360, loss 0.5474924445152283, accuracy 0.81982421875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:28.884924: step 1360, loss 0.47982, accuracy 0.83447265625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:28.945038: step 1361, loss 0.44464534521102905, accuracy 0.85009765625\n",
      "2019-04-10T00:51:29.003052: step 1362, loss 0.4584108889102936, accuracy 0.81982421875\n",
      "2019-04-10T00:51:29.057516: step 1363, loss 0.5072817802429199, accuracy 0.853515625\n",
      "2019-04-10T00:51:29.115550: step 1364, loss 0.5324951410293579, accuracy 0.81982421875\n",
      "2019-04-10T00:51:29.175950: step 1365, loss 0.5505841374397278, accuracy 0.80322265625\n",
      "2019-04-10T00:51:29.237542: step 1366, loss 0.45801156759262085, accuracy 0.81982421875\n",
      "2019-04-10T00:51:29.294286: step 1367, loss 0.46360692381858826, accuracy 0.84326171875\n",
      "2019-04-10T00:51:29.356975: step 1368, loss 0.42037728428840637, accuracy 0.85986328125\n",
      "2019-04-10T00:51:29.413043: step 1369, loss 0.3952425420284271, accuracy 0.83984375\n",
      "2019-04-10T00:51:29.470319: step 1370, loss 0.4398459792137146, accuracy 0.86328125\n",
      "2019-04-10T00:51:29.527664: step 1371, loss 0.5431365966796875, accuracy 0.7998046875\n",
      "2019-04-10T00:51:29.588056: step 1372, loss 0.44118040800094604, accuracy 0.83642578125\n",
      "2019-04-10T00:51:29.643778: step 1373, loss 0.49628838896751404, accuracy 0.853515625\n",
      "2019-04-10T00:51:29.700804: step 1374, loss 0.44245845079421997, accuracy 0.8232421875\n",
      "2019-04-10T00:51:29.753896: step 1375, loss 0.46846625208854675, accuracy 0.83349609375\n",
      "2019-04-10T00:51:29.808985: step 1376, loss 0.49113988876342773, accuracy 0.83984375\n",
      "2019-04-10T00:51:29.867782: step 1377, loss 0.4219273030757904, accuracy 0.853515625\n",
      "2019-04-10T00:51:29.927668: step 1378, loss 0.46669328212738037, accuracy 0.84326171875\n",
      "2019-04-10T00:51:29.981919: step 1379, loss 0.4969552159309387, accuracy 0.81689453125\n",
      "2019-04-10T00:51:30.038196: step 1380, loss 0.4647185802459717, accuracy 0.81982421875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:30.262651: step 1380, loss 0.459219, accuracy 0.84619140625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:30.320367: step 1381, loss 0.5705849528312683, accuracy 0.796875\n",
      "2019-04-10T00:51:30.376433: step 1382, loss 0.5166274309158325, accuracy 0.8466796875\n",
      "2019-04-10T00:51:30.432836: step 1383, loss 0.5037302374839783, accuracy 0.83642578125\n",
      "2019-04-10T00:51:30.487313: step 1384, loss 0.505402147769928, accuracy 0.79345703125\n",
      "2019-04-10T00:51:30.541974: step 1385, loss 0.5249475836753845, accuracy 0.85009765625\n",
      "2019-04-10T00:51:30.594725: step 1386, loss 0.4260169565677643, accuracy 0.83984375\n",
      "2019-04-10T00:51:30.649856: step 1387, loss 0.4916687607765198, accuracy 0.81005859375\n",
      "2019-04-10T00:51:30.702463: step 1388, loss 0.45234227180480957, accuracy 0.8466796875\n",
      "2019-04-10T00:51:30.756991: step 1389, loss 0.4595414698123932, accuracy 0.84326171875\n",
      "2019-04-10T00:51:30.810205: step 1390, loss 0.45370620489120483, accuracy 0.85986328125\n",
      "2019-04-10T00:51:30.865624: step 1391, loss 0.5262156128883362, accuracy 0.83984375\n",
      "2019-04-10T00:51:30.920667: step 1392, loss 0.4714471995830536, accuracy 0.81005859375\n",
      "2019-04-10T00:51:30.975787: step 1393, loss 0.5225533246994019, accuracy 0.81005859375\n",
      "2019-04-10T00:51:31.028654: step 1394, loss 0.47374725341796875, accuracy 0.85986328125\n",
      "2019-04-10T00:51:31.085171: step 1395, loss 0.5228838324546814, accuracy 0.830078125\n",
      "2019-04-10T00:51:31.141825: step 1396, loss 0.5207149982452393, accuracy 0.8564453125\n",
      "2019-04-10T00:51:31.198542: step 1397, loss 0.3951248228549957, accuracy 0.86328125\n",
      "2019-04-10T00:51:31.252232: step 1398, loss 0.4516524374485016, accuracy 0.85009765625\n",
      "2019-04-10T00:51:31.308570: step 1399, loss 0.5542559623718262, accuracy 0.82666015625\n",
      "2019-04-10T00:51:31.363639: step 1400, loss 0.4368915259838104, accuracy 0.83349609375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:31.623940: step 1400, loss 0.461582, accuracy 0.83984375\n",
      "\n",
      "\n",
      "2019-04-10T00:51:31.689132: step 1401, loss 0.5219108462333679, accuracy 0.8134765625\n",
      "2019-04-10T00:51:31.756217: step 1402, loss 0.40086835622787476, accuracy 0.85986328125\n",
      "2019-04-10T00:51:31.819239: step 1403, loss 0.4473074972629547, accuracy 0.83349609375\n",
      "2019-04-10T00:51:31.886261: step 1404, loss 0.4224143624305725, accuracy 0.83349609375\n",
      "2019-04-10T00:51:31.962185: step 1405, loss 0.45768406987190247, accuracy 0.83642578125\n",
      "2019-04-10T00:51:32.035967: step 1406, loss 0.44905775785446167, accuracy 0.85009765625\n",
      "2019-04-10T00:51:32.101540: step 1407, loss 0.4706089198589325, accuracy 0.853515625\n",
      "2019-04-10T00:51:32.159496: step 1408, loss 0.38808226585388184, accuracy 0.86328125\n",
      "2019-04-10T00:51:32.216955: step 1409, loss 0.4665443003177643, accuracy 0.85986328125\n",
      "2019-04-10T00:51:32.267522: step 1410, loss 0.4997929334640503, accuracy 0.81982421875\n",
      "2019-04-10T00:51:32.325676: step 1411, loss 0.4922581613063812, accuracy 0.83349609375\n",
      "2019-04-10T00:51:32.390409: step 1412, loss 0.40578755736351013, accuracy 0.853515625\n",
      "2019-04-10T00:51:32.457750: step 1413, loss 0.4955427944660187, accuracy 0.84326171875\n",
      "2019-04-10T00:51:32.519460: step 1414, loss 0.5198712348937988, accuracy 0.81689453125\n",
      "2019-04-10T00:51:32.573142: step 1415, loss 0.4161714017391205, accuracy 0.86328125\n",
      "2019-04-10T00:51:32.638187: step 1416, loss 0.3522472083568573, accuracy 0.8935546875\n",
      "2019-04-10T00:51:32.698031: step 1417, loss 0.507590115070343, accuracy 0.81689453125\n",
      "2019-04-10T00:51:32.752650: step 1418, loss 0.4824659824371338, accuracy 0.83349609375\n",
      "2019-04-10T00:51:32.808872: step 1419, loss 0.4252881407737732, accuracy 0.83349609375\n",
      "2019-04-10T00:51:32.859092: step 1420, loss 0.41104191541671753, accuracy 0.85986328125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:33.091187: step 1420, loss 0.49214, accuracy 0.8369140625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:33.150002: step 1421, loss 0.5054912567138672, accuracy 0.83984375\n",
      "2019-04-10T00:51:33.202225: step 1422, loss 0.4609558582305908, accuracy 0.83349609375\n",
      "2019-04-10T00:51:33.264180: step 1423, loss 0.48393428325653076, accuracy 0.830078125\n",
      "2019-04-10T00:51:33.325693: step 1424, loss 0.5872746706008911, accuracy 0.83349609375\n",
      "2019-04-10T00:51:33.384417: step 1425, loss 0.4953655004501343, accuracy 0.7900390625\n",
      "2019-04-10T00:51:33.434235: step 1426, loss 0.38992446660995483, accuracy 0.85986328125\n",
      "2019-04-10T00:51:33.488180: step 1427, loss 0.42432066798210144, accuracy 0.8232421875\n",
      "2019-04-10T00:51:33.541285: step 1428, loss 0.5184399485588074, accuracy 0.79345703125\n",
      "2019-04-10T00:51:33.594491: step 1429, loss 0.5737217664718628, accuracy 0.78662109375\n",
      "2019-04-10T00:51:33.645859: step 1430, loss 0.5564107894897461, accuracy 0.7900390625\n",
      "2019-04-10T00:51:33.698768: step 1431, loss 0.5887373089790344, accuracy 0.78662109375\n",
      "2019-04-10T00:51:33.752076: step 1432, loss 0.4517752528190613, accuracy 0.83349609375\n",
      "2019-04-10T00:51:33.803592: step 1433, loss 0.4523046314716339, accuracy 0.8564453125\n",
      "2019-04-10T00:51:33.858103: step 1434, loss 0.4861135482788086, accuracy 0.81689453125\n",
      "2019-04-10T00:51:33.923195: step 1435, loss 0.38856741786003113, accuracy 0.87353515625\n",
      "2019-04-10T00:51:33.985456: step 1436, loss 0.46664246916770935, accuracy 0.84326171875\n",
      "2019-04-10T00:51:34.044994: step 1437, loss 0.3731178939342499, accuracy 0.8798828125\n",
      "2019-04-10T00:51:34.105555: step 1438, loss 0.5806571245193481, accuracy 0.79345703125\n",
      "2019-04-10T00:51:34.173136: step 1439, loss 0.5419995784759521, accuracy 0.81982421875\n",
      "2019-04-10T00:51:34.200452: step 1440, loss 0.44638049602508545, accuracy 0.81005859375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:34.471462: step 1440, loss 0.462902, accuracy 0.84619140625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:34.633788: step 1441, loss 0.5931150913238525, accuracy 0.83349609375\n",
      "2019-04-10T00:51:34.686449: step 1442, loss 0.43284231424331665, accuracy 0.8466796875\n",
      "2019-04-10T00:51:34.738445: step 1443, loss 0.514218270778656, accuracy 0.83642578125\n",
      "2019-04-10T00:51:34.789965: step 1444, loss 0.42792800068855286, accuracy 0.830078125\n",
      "2019-04-10T00:51:34.840482: step 1445, loss 0.5268358588218689, accuracy 0.82666015625\n",
      "2019-04-10T00:51:34.891463: step 1446, loss 0.48596808314323425, accuracy 0.8134765625\n",
      "2019-04-10T00:51:34.940808: step 1447, loss 0.5287472605705261, accuracy 0.81982421875\n",
      "2019-04-10T00:51:34.992148: step 1448, loss 0.4878213107585907, accuracy 0.83349609375\n",
      "2019-04-10T00:51:35.041940: step 1449, loss 0.3518769145011902, accuracy 0.87353515625\n",
      "2019-04-10T00:51:35.090431: step 1450, loss 0.4941912293434143, accuracy 0.83642578125\n",
      "2019-04-10T00:51:35.143053: step 1451, loss 0.46880483627319336, accuracy 0.85009765625\n",
      "2019-04-10T00:51:35.193390: step 1452, loss 0.4153654873371124, accuracy 0.84326171875\n",
      "2019-04-10T00:51:35.245174: step 1453, loss 0.45427069067955017, accuracy 0.830078125\n",
      "2019-04-10T00:51:35.294508: step 1454, loss 0.410521000623703, accuracy 0.87646484375\n",
      "2019-04-10T00:51:35.348808: step 1455, loss 0.45763346552848816, accuracy 0.84326171875\n",
      "2019-04-10T00:51:35.400380: step 1456, loss 0.432266503572464, accuracy 0.84326171875\n",
      "2019-04-10T00:51:35.450374: step 1457, loss 0.45132049918174744, accuracy 0.830078125\n",
      "2019-04-10T00:51:35.499658: step 1458, loss 0.4720733165740967, accuracy 0.83984375\n",
      "2019-04-10T00:51:35.547971: step 1459, loss 0.4283495545387268, accuracy 0.83642578125\n",
      "2019-04-10T00:51:35.601429: step 1460, loss 0.43922778964042664, accuracy 0.8564453125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:35.821834: step 1460, loss 0.458029, accuracy 0.85107421875\n",
      "\n",
      "\n",
      "2019-04-10T00:51:35.874486: step 1461, loss 0.4263663589954376, accuracy 0.8564453125\n",
      "2019-04-10T00:51:35.924079: step 1462, loss 0.45440012216567993, accuracy 0.84326171875\n",
      "2019-04-10T00:51:35.971869: step 1463, loss 0.45843958854675293, accuracy 0.83642578125\n",
      "2019-04-10T00:51:36.023002: step 1464, loss 0.47001150250434875, accuracy 0.84326171875\n",
      "2019-04-10T00:51:36.072127: step 1465, loss 0.5026025176048279, accuracy 0.79345703125\n",
      "2019-04-10T00:51:36.121435: step 1466, loss 0.48247066140174866, accuracy 0.83349609375\n",
      "2019-04-10T00:51:36.172800: step 1467, loss 0.4120689630508423, accuracy 0.87353515625\n",
      "2019-04-10T00:51:36.222356: step 1468, loss 0.5054542422294617, accuracy 0.830078125\n",
      "2019-04-10T00:51:36.271197: step 1469, loss 0.442808598279953, accuracy 0.85986328125\n",
      "2019-04-10T00:51:36.322081: step 1470, loss 0.4831257164478302, accuracy 0.8232421875\n",
      "2019-04-10T00:51:36.369996: step 1471, loss 0.432917982339859, accuracy 0.83349609375\n",
      "2019-04-10T00:51:36.417238: step 1472, loss 0.5442147850990295, accuracy 0.78662109375\n",
      "2019-04-10T00:51:36.465406: step 1473, loss 0.5081202983856201, accuracy 0.83349609375\n",
      "2019-04-10T00:51:36.513267: step 1474, loss 0.4689023196697235, accuracy 0.82666015625\n",
      "2019-04-10T00:51:36.565994: step 1475, loss 0.5255142450332642, accuracy 0.8466796875\n",
      "2019-04-10T00:51:36.615168: step 1476, loss 0.49399781227111816, accuracy 0.84326171875\n",
      "2019-04-10T00:51:36.664663: step 1477, loss 0.49380379915237427, accuracy 0.81005859375\n",
      "2019-04-10T00:51:36.713286: step 1478, loss 0.4236052632331848, accuracy 0.85986328125\n",
      "2019-04-10T00:51:36.761297: step 1479, loss 0.5282508134841919, accuracy 0.783203125\n",
      "2019-04-10T00:51:36.810538: step 1480, loss 0.38170552253723145, accuracy 0.8798828125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:37.026254: step 1480, loss 0.45838, accuracy 0.8408203125\n",
      "\n",
      "\n",
      "2019-04-10T00:51:37.079210: step 1481, loss 0.5040788054466248, accuracy 0.85009765625\n",
      "2019-04-10T00:51:37.126990: step 1482, loss 0.3700573742389679, accuracy 0.89013671875\n",
      "2019-04-10T00:51:37.175354: step 1483, loss 0.4536513388156891, accuracy 0.85986328125\n",
      "2019-04-10T00:51:37.235902: step 1484, loss 0.40579795837402344, accuracy 0.85009765625\n",
      "2019-04-10T00:51:37.287493: step 1485, loss 0.44342413544654846, accuracy 0.83349609375\n",
      "2019-04-10T00:51:37.336306: step 1486, loss 0.4817466735839844, accuracy 0.8232421875\n",
      "2019-04-10T00:51:37.385386: step 1487, loss 0.5418063998222351, accuracy 0.7998046875\n",
      "2019-04-10T00:51:37.434730: step 1488, loss 0.3491448760032654, accuracy 0.87646484375\n",
      "2019-04-10T00:51:37.488947: step 1489, loss 0.4729984402656555, accuracy 0.84326171875\n",
      "2019-04-10T00:51:37.541516: step 1490, loss 0.48369765281677246, accuracy 0.8564453125\n",
      "2019-04-10T00:51:37.595653: step 1491, loss 0.40096741914749146, accuracy 0.8798828125\n",
      "2019-04-10T00:51:37.647639: step 1492, loss 0.4532776474952698, accuracy 0.82666015625\n",
      "2019-04-10T00:51:37.697880: step 1493, loss 0.46616098284721375, accuracy 0.85009765625\n",
      "2019-04-10T00:51:37.753523: step 1494, loss 0.47028273344039917, accuracy 0.830078125\n",
      "2019-04-10T00:51:37.805941: step 1495, loss 0.45360422134399414, accuracy 0.830078125\n",
      "2019-04-10T00:51:37.856444: step 1496, loss 0.43596959114074707, accuracy 0.8466796875\n",
      "2019-04-10T00:51:37.908460: step 1497, loss 0.398218035697937, accuracy 0.86669921875\n",
      "2019-04-10T00:51:37.966154: step 1498, loss 0.5840588808059692, accuracy 0.7900390625\n",
      "2019-04-10T00:51:38.017614: step 1499, loss 0.41957172751426697, accuracy 0.85009765625\n",
      "2019-04-10T00:51:38.079013: step 1500, loss 0.34034183621406555, accuracy 0.853515625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:38.376240: step 1500, loss 0.449997, accuracy 0.8466796875\n",
      "\n",
      "\n",
      "2019-04-10T00:51:38.445546: step 1501, loss 0.40116503834724426, accuracy 0.83642578125\n",
      "2019-04-10T00:51:38.514925: step 1502, loss 0.42487314343452454, accuracy 0.853515625\n",
      "2019-04-10T00:51:38.601138: step 1503, loss 0.46544381976127625, accuracy 0.8466796875\n",
      "2019-04-10T00:51:38.753256: step 1504, loss 0.5883373618125916, accuracy 0.8134765625\n",
      "2019-04-10T00:51:38.833485: step 1505, loss 0.4985215663909912, accuracy 0.81689453125\n",
      "2019-04-10T00:51:38.903348: step 1506, loss 0.4053964614868164, accuracy 0.85009765625\n",
      "2019-04-10T00:51:38.974330: step 1507, loss 0.4194771945476532, accuracy 0.85009765625\n",
      "2019-04-10T00:51:39.042431: step 1508, loss 0.5401235222816467, accuracy 0.8134765625\n",
      "2019-04-10T00:51:39.105580: step 1509, loss 0.40293803811073303, accuracy 0.853515625\n",
      "2019-04-10T00:51:39.169153: step 1510, loss 0.5389608144760132, accuracy 0.82666015625\n",
      "2019-04-10T00:51:39.239392: step 1511, loss 0.5019184350967407, accuracy 0.81689453125\n",
      "2019-04-10T00:51:39.312577: step 1512, loss 0.521883487701416, accuracy 0.83984375\n",
      "2019-04-10T00:51:39.382442: step 1513, loss 0.48791006207466125, accuracy 0.853515625\n",
      "2019-04-10T00:51:39.449758: step 1514, loss 0.35042059421539307, accuracy 0.8798828125\n",
      "2019-04-10T00:51:39.514620: step 1515, loss 0.3412584364414215, accuracy 0.8798828125\n",
      "2019-04-10T00:51:39.574926: step 1516, loss 0.48072993755340576, accuracy 0.81982421875\n",
      "2019-04-10T00:51:39.635094: step 1517, loss 0.5426205992698669, accuracy 0.85009765625\n",
      "2019-04-10T00:51:39.693867: step 1518, loss 0.5045226812362671, accuracy 0.853515625\n",
      "2019-04-10T00:51:39.753590: step 1519, loss 0.3901407718658447, accuracy 0.8701171875\n",
      "2019-04-10T00:51:39.816978: step 1520, loss 0.46596384048461914, accuracy 0.85009765625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:40.107154: step 1520, loss 0.450512, accuracy 0.84716796875\n",
      "\n",
      "\n",
      "2019-04-10T00:51:40.178760: step 1521, loss 0.39143434166908264, accuracy 0.8466796875\n",
      "2019-04-10T00:51:40.244191: step 1522, loss 0.5026495456695557, accuracy 0.82666015625\n",
      "2019-04-10T00:51:40.303907: step 1523, loss 0.5339078307151794, accuracy 0.8134765625\n",
      "2019-04-10T00:51:40.364110: step 1524, loss 0.5151991248130798, accuracy 0.830078125\n",
      "2019-04-10T00:51:40.427185: step 1525, loss 0.39824599027633667, accuracy 0.88671875\n",
      "2019-04-10T00:51:40.488074: step 1526, loss 0.38306093215942383, accuracy 0.8564453125\n",
      "2019-04-10T00:51:40.549699: step 1527, loss 0.5891335606575012, accuracy 0.82666015625\n",
      "2019-04-10T00:51:40.610892: step 1528, loss 0.5100610256195068, accuracy 0.82666015625\n",
      "2019-04-10T00:51:40.675621: step 1529, loss 0.4931771755218506, accuracy 0.853515625\n",
      "2019-04-10T00:51:40.741252: step 1530, loss 0.4358162581920624, accuracy 0.8466796875\n",
      "2019-04-10T00:51:40.802018: step 1531, loss 0.4202694892883301, accuracy 0.83349609375\n",
      "2019-04-10T00:51:40.862449: step 1532, loss 0.47645559906959534, accuracy 0.83349609375\n",
      "2019-04-10T00:51:40.924641: step 1533, loss 0.537792444229126, accuracy 0.7900390625\n",
      "2019-04-10T00:51:40.980797: step 1534, loss 0.4702640771865845, accuracy 0.8466796875\n",
      "2019-04-10T00:51:41.040633: step 1535, loss 0.4708225429058075, accuracy 0.830078125\n",
      "2019-04-10T00:51:41.095364: step 1536, loss 0.4003584682941437, accuracy 0.86669921875\n",
      "2019-04-10T00:51:41.155710: step 1537, loss 0.47974783182144165, accuracy 0.81689453125\n",
      "2019-04-10T00:51:41.216515: step 1538, loss 0.5509501099586487, accuracy 0.830078125\n",
      "2019-04-10T00:51:41.275900: step 1539, loss 0.43767431378364563, accuracy 0.8466796875\n",
      "2019-04-10T00:51:41.334267: step 1540, loss 0.4842148423194885, accuracy 0.81005859375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:41.585160: step 1540, loss 0.452974, accuracy 0.85205078125\n",
      "\n",
      "\n",
      "2019-04-10T00:51:41.644592: step 1541, loss 0.40095075964927673, accuracy 0.85009765625\n",
      "2019-04-10T00:51:41.699125: step 1542, loss 0.4948829114437103, accuracy 0.8466796875\n",
      "2019-04-10T00:51:41.757591: step 1543, loss 0.45789727568626404, accuracy 0.85009765625\n",
      "2019-04-10T00:51:41.818655: step 1544, loss 0.41972610354423523, accuracy 0.8564453125\n",
      "2019-04-10T00:51:41.879136: step 1545, loss 0.5342003703117371, accuracy 0.83642578125\n",
      "2019-04-10T00:51:41.941756: step 1546, loss 0.4045845568180084, accuracy 0.85009765625\n",
      "2019-04-10T00:51:42.000691: step 1547, loss 0.4977128207683563, accuracy 0.8564453125\n",
      "2019-04-10T00:51:42.058569: step 1548, loss 0.4972519874572754, accuracy 0.83642578125\n",
      "2019-04-10T00:51:42.116144: step 1549, loss 0.4456970691680908, accuracy 0.85009765625\n",
      "2019-04-10T00:51:42.171631: step 1550, loss 0.4086880683898926, accuracy 0.84326171875\n",
      "2019-04-10T00:51:42.225210: step 1551, loss 0.5262178778648376, accuracy 0.81982421875\n",
      "2019-04-10T00:51:42.279384: step 1552, loss 0.4897291958332062, accuracy 0.85986328125\n",
      "2019-04-10T00:51:42.337976: step 1553, loss 0.5100467205047607, accuracy 0.8134765625\n",
      "2019-04-10T00:51:42.394868: step 1554, loss 0.535099446773529, accuracy 0.81689453125\n",
      "2019-04-10T00:51:42.449454: step 1555, loss 0.5061538219451904, accuracy 0.7900390625\n",
      "2019-04-10T00:51:42.504665: step 1556, loss 0.41814637184143066, accuracy 0.83984375\n",
      "2019-04-10T00:51:42.557602: step 1557, loss 0.406352162361145, accuracy 0.8564453125\n",
      "2019-04-10T00:51:42.612587: step 1558, loss 0.4397168457508087, accuracy 0.87646484375\n",
      "2019-04-10T00:51:42.664375: step 1559, loss 0.30405277013778687, accuracy 0.8935546875\n",
      "2019-04-10T00:51:42.718335: step 1560, loss 0.4272737205028534, accuracy 0.86669921875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:42.942869: step 1560, loss 0.455516, accuracy 0.84619140625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:42.997705: step 1561, loss 0.5075206756591797, accuracy 0.81689453125\n",
      "2019-04-10T00:51:43.051624: step 1562, loss 0.4714808762073517, accuracy 0.8466796875\n",
      "2019-04-10T00:51:43.103269: step 1563, loss 0.49713724851608276, accuracy 0.85986328125\n",
      "2019-04-10T00:51:43.156522: step 1564, loss 0.38705822825431824, accuracy 0.85009765625\n",
      "2019-04-10T00:51:43.216259: step 1565, loss 0.4017595052719116, accuracy 0.83984375\n",
      "2019-04-10T00:51:43.271597: step 1566, loss 0.5054229497909546, accuracy 0.830078125\n",
      "2019-04-10T00:51:43.323323: step 1567, loss 0.4699321985244751, accuracy 0.84326171875\n",
      "2019-04-10T00:51:43.378409: step 1568, loss 0.6225699782371521, accuracy 0.81005859375\n",
      "2019-04-10T00:51:43.432269: step 1569, loss 0.3510487973690033, accuracy 0.88330078125\n",
      "2019-04-10T00:51:43.496975: step 1570, loss 0.4613105356693268, accuracy 0.85009765625\n",
      "2019-04-10T00:51:43.550267: step 1571, loss 0.47061532735824585, accuracy 0.82666015625\n",
      "2019-04-10T00:51:43.602291: step 1572, loss 0.41800421476364136, accuracy 0.83984375\n",
      "2019-04-10T00:51:43.656433: step 1573, loss 0.4784853756427765, accuracy 0.80322265625\n",
      "2019-04-10T00:51:43.715797: step 1574, loss 0.41775262355804443, accuracy 0.87353515625\n",
      "2019-04-10T00:51:43.770643: step 1575, loss 0.4541238844394684, accuracy 0.84326171875\n",
      "2019-04-10T00:51:43.823798: step 1576, loss 0.4578600525856018, accuracy 0.81982421875\n",
      "2019-04-10T00:51:43.877083: step 1577, loss 0.6117467284202576, accuracy 0.796875\n",
      "2019-04-10T00:51:43.928228: step 1578, loss 0.4926547706127167, accuracy 0.83642578125\n",
      "2019-04-10T00:51:43.982186: step 1579, loss 0.46894729137420654, accuracy 0.83349609375\n",
      "2019-04-10T00:51:44.035130: step 1580, loss 0.48813700675964355, accuracy 0.81982421875\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:44.264824: step 1580, loss 0.460321, accuracy 0.83642578125\n",
      "\n",
      "\n",
      "2019-04-10T00:51:44.324605: step 1581, loss 0.44082459807395935, accuracy 0.83984375\n",
      "2019-04-10T00:51:44.377496: step 1582, loss 0.4764542579650879, accuracy 0.81689453125\n",
      "2019-04-10T00:51:44.431651: step 1583, loss 0.42579367756843567, accuracy 0.85986328125\n",
      "2019-04-10T00:51:44.487849: step 1584, loss 0.4193597435951233, accuracy 0.8564453125\n",
      "2019-04-10T00:51:44.546114: step 1585, loss 0.5037511587142944, accuracy 0.83349609375\n",
      "2019-04-10T00:51:44.599566: step 1586, loss 0.5073086619377136, accuracy 0.83349609375\n",
      "2019-04-10T00:51:44.651969: step 1587, loss 0.5330094695091248, accuracy 0.83349609375\n",
      "2019-04-10T00:51:44.705130: step 1588, loss 0.4522099792957306, accuracy 0.83642578125\n",
      "2019-04-10T00:51:44.757616: step 1589, loss 0.42835062742233276, accuracy 0.853515625\n",
      "2019-04-10T00:51:44.811149: step 1590, loss 0.46951720118522644, accuracy 0.83349609375\n",
      "2019-04-10T00:51:44.864931: step 1591, loss 0.46182215213775635, accuracy 0.85009765625\n",
      "2019-04-10T00:51:44.933834: step 1592, loss 0.49372246861457825, accuracy 0.8134765625\n",
      "2019-04-10T00:51:44.990535: step 1593, loss 0.4461071193218231, accuracy 0.83349609375\n",
      "2019-04-10T00:51:45.044361: step 1594, loss 0.49123769998550415, accuracy 0.81689453125\n",
      "2019-04-10T00:51:45.097349: step 1595, loss 0.4717192053794861, accuracy 0.83349609375\n",
      "2019-04-10T00:51:45.149512: step 1596, loss 0.4650545120239258, accuracy 0.85986328125\n",
      "2019-04-10T00:51:45.203466: step 1597, loss 0.5756728649139404, accuracy 0.806640625\n",
      "2019-04-10T00:51:45.258066: step 1598, loss 0.49662140011787415, accuracy 0.83984375\n",
      "2019-04-10T00:51:45.312333: step 1599, loss 0.424606591463089, accuracy 0.86669921875\n",
      "2019-04-10T00:51:45.373796: step 1600, loss 0.527454137802124, accuracy 0.83984375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:45.616162: step 1600, loss 0.447495, accuracy 0.84814453125\n",
      "\n",
      "\n",
      "2019-04-10T00:51:45.674826: step 1601, loss 0.5512236952781677, accuracy 0.79345703125\n",
      "2019-04-10T00:51:45.733414: step 1602, loss 0.4620439112186432, accuracy 0.87353515625\n",
      "2019-04-10T00:51:45.796981: step 1603, loss 0.4431836009025574, accuracy 0.86669921875\n",
      "2019-04-10T00:51:45.863136: step 1604, loss 0.4262092709541321, accuracy 0.87353515625\n",
      "2019-04-10T00:51:45.915890: step 1605, loss 0.42028945684432983, accuracy 0.83642578125\n",
      "2019-04-10T00:51:45.968940: step 1606, loss 0.48662039637565613, accuracy 0.83642578125\n",
      "2019-04-10T00:51:46.022793: step 1607, loss 0.4552105665206909, accuracy 0.85986328125\n",
      "2019-04-10T00:51:46.077160: step 1608, loss 0.4959593713283539, accuracy 0.83642578125\n",
      "2019-04-10T00:51:46.131385: step 1609, loss 0.3988828659057617, accuracy 0.8466796875\n",
      "2019-04-10T00:51:46.183216: step 1610, loss 0.4880676865577698, accuracy 0.81982421875\n",
      "2019-04-10T00:51:46.255108: step 1611, loss 0.4742302894592285, accuracy 0.85986328125\n",
      "2019-04-10T00:51:46.311062: step 1612, loss 0.4908742308616638, accuracy 0.83349609375\n",
      "2019-04-10T00:51:46.365635: step 1613, loss 0.3358190357685089, accuracy 0.87353515625\n",
      "2019-04-10T00:51:46.420850: step 1614, loss 0.44160196185112, accuracy 0.85009765625\n",
      "2019-04-10T00:51:46.474602: step 1615, loss 0.4876883029937744, accuracy 0.830078125\n",
      "2019-04-10T00:51:46.527805: step 1616, loss 0.49776163697242737, accuracy 0.82666015625\n",
      "2019-04-10T00:51:46.578091: step 1617, loss 0.5530942678451538, accuracy 0.81689453125\n",
      "2019-04-10T00:51:46.632014: step 1618, loss 0.4757893979549408, accuracy 0.8232421875\n",
      "2019-04-10T00:51:46.690485: step 1619, loss 0.47495579719543457, accuracy 0.85009765625\n",
      "2019-04-10T00:51:46.747080: step 1620, loss 0.4638858139514923, accuracy 0.85009765625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:47.004863: step 1620, loss 0.455973, accuracy 0.85107421875\n",
      "\n",
      "\n",
      "2019-04-10T00:51:47.062919: step 1621, loss 0.5213707685470581, accuracy 0.81005859375\n",
      "2019-04-10T00:51:47.119281: step 1622, loss 0.48975372314453125, accuracy 0.83349609375\n",
      "2019-04-10T00:51:47.180489: step 1623, loss 0.4918295741081238, accuracy 0.81689453125\n",
      "2019-04-10T00:51:47.236149: step 1624, loss 0.5052947402000427, accuracy 0.81982421875\n",
      "2019-04-10T00:51:47.289485: step 1625, loss 0.4299866855144501, accuracy 0.8466796875\n",
      "2019-04-10T00:51:47.343805: step 1626, loss 0.5011692047119141, accuracy 0.83984375\n",
      "2019-04-10T00:51:47.397099: step 1627, loss 0.43783289194107056, accuracy 0.8466796875\n",
      "2019-04-10T00:51:47.453775: step 1628, loss 0.5132784247398376, accuracy 0.83349609375\n",
      "2019-04-10T00:51:47.506197: step 1629, loss 0.4404313266277313, accuracy 0.81005859375\n",
      "2019-04-10T00:51:47.564053: step 1630, loss 0.3591232895851135, accuracy 0.87353515625\n",
      "2019-04-10T00:51:47.623848: step 1631, loss 0.3455989956855774, accuracy 0.87353515625\n",
      "2019-04-10T00:51:47.678152: step 1632, loss 0.4676429331302643, accuracy 0.83349609375\n",
      "2019-04-10T00:51:47.730184: step 1633, loss 0.5037583708763123, accuracy 0.82666015625\n",
      "2019-04-10T00:51:47.783192: step 1634, loss 0.5466959476470947, accuracy 0.830078125\n",
      "2019-04-10T00:51:47.833774: step 1635, loss 0.4457038938999176, accuracy 0.83642578125\n",
      "2019-04-10T00:51:47.885374: step 1636, loss 0.3850423991680145, accuracy 0.86328125\n",
      "2019-04-10T00:51:47.939088: step 1637, loss 0.4547995626926422, accuracy 0.830078125\n",
      "2019-04-10T00:51:47.989835: step 1638, loss 0.6867864727973938, accuracy 0.7900390625\n",
      "2019-04-10T00:51:48.059575: step 1639, loss 0.6043460369110107, accuracy 0.7998046875\n",
      "2019-04-10T00:51:48.112908: step 1640, loss 0.4796806275844574, accuracy 0.83642578125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:48.346374: step 1640, loss 0.449751, accuracy 0.84375\n",
      "\n",
      "\n",
      "2019-04-10T00:51:48.404137: step 1641, loss 0.45651113986968994, accuracy 0.85009765625\n",
      "2019-04-10T00:51:48.458576: step 1642, loss 0.4427120089530945, accuracy 0.8564453125\n",
      "2019-04-10T00:51:48.523648: step 1643, loss 0.44919031858444214, accuracy 0.84326171875\n",
      "2019-04-10T00:51:48.581789: step 1644, loss 0.3950960040092468, accuracy 0.8564453125\n",
      "2019-04-10T00:51:48.641299: step 1645, loss 0.45082250237464905, accuracy 0.83642578125\n",
      "2019-04-10T00:51:48.692492: step 1646, loss 0.4739137887954712, accuracy 0.83349609375\n",
      "2019-04-10T00:51:48.746514: step 1647, loss 0.5186964273452759, accuracy 0.81005859375\n",
      "2019-04-10T00:51:48.799820: step 1648, loss 0.4927428066730499, accuracy 0.8232421875\n",
      "2019-04-10T00:51:48.853646: step 1649, loss 0.4944312572479248, accuracy 0.806640625\n",
      "2019-04-10T00:51:48.905114: step 1650, loss 0.4394373595714569, accuracy 0.83984375\n",
      "2019-04-10T00:51:48.969681: step 1651, loss 0.43386590480804443, accuracy 0.83984375\n",
      "2019-04-10T00:51:49.036218: step 1652, loss 0.47344252467155457, accuracy 0.83642578125\n",
      "2019-04-10T00:51:49.097389: step 1653, loss 0.4665408432483673, accuracy 0.853515625\n",
      "2019-04-10T00:51:49.156743: step 1654, loss 0.43006622791290283, accuracy 0.8564453125\n",
      "2019-04-10T00:51:49.217096: step 1655, loss 0.5249260067939758, accuracy 0.8134765625\n",
      "2019-04-10T00:51:49.278600: step 1656, loss 0.4208426773548126, accuracy 0.86669921875\n",
      "2019-04-10T00:51:49.341002: step 1657, loss 0.5258627533912659, accuracy 0.83349609375\n",
      "2019-04-10T00:51:49.401668: step 1658, loss 0.39153537154197693, accuracy 0.86328125\n",
      "2019-04-10T00:51:49.470237: step 1659, loss 0.6291484236717224, accuracy 0.80322265625\n",
      "2019-04-10T00:51:49.529831: step 1660, loss 0.46334314346313477, accuracy 0.853515625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:49.772436: step 1660, loss 0.46247, accuracy 0.84423828125\n",
      "\n",
      "\n",
      "2019-04-10T00:51:49.827529: step 1661, loss 0.4473389685153961, accuracy 0.87646484375\n",
      "2019-04-10T00:51:49.881798: step 1662, loss 0.4407382309436798, accuracy 0.853515625\n",
      "2019-04-10T00:51:49.941553: step 1663, loss 0.5138780474662781, accuracy 0.853515625\n",
      "2019-04-10T00:51:49.999138: step 1664, loss 0.4749433398246765, accuracy 0.81689453125\n",
      "2019-04-10T00:51:50.055776: step 1665, loss 0.384585440158844, accuracy 0.87646484375\n",
      "2019-04-10T00:51:50.115211: step 1666, loss 0.43537911772727966, accuracy 0.83349609375\n",
      "2019-04-10T00:51:50.171272: step 1667, loss 0.35796836018562317, accuracy 0.89013671875\n",
      "2019-04-10T00:51:50.232815: step 1668, loss 0.46776270866394043, accuracy 0.84326171875\n",
      "2019-04-10T00:51:50.289405: step 1669, loss 0.36672312021255493, accuracy 0.86669921875\n",
      "2019-04-10T00:51:50.346689: step 1670, loss 0.4130648672580719, accuracy 0.85986328125\n",
      "2019-04-10T00:51:50.401205: step 1671, loss 0.49728113412857056, accuracy 0.83349609375\n",
      "2019-04-10T00:51:50.456977: step 1672, loss 0.5187300443649292, accuracy 0.81005859375\n",
      "2019-04-10T00:51:50.514426: step 1673, loss 0.42683491110801697, accuracy 0.83642578125\n",
      "2019-04-10T00:51:50.573307: step 1674, loss 0.5872374773025513, accuracy 0.81005859375\n",
      "2019-04-10T00:51:50.626997: step 1675, loss 0.4284791052341461, accuracy 0.8466796875\n",
      "2019-04-10T00:51:50.680896: step 1676, loss 0.4916702210903168, accuracy 0.81689453125\n",
      "2019-04-10T00:51:50.738452: step 1677, loss 0.3941822946071625, accuracy 0.85986328125\n",
      "2019-04-10T00:51:50.794294: step 1678, loss 0.3941928446292877, accuracy 0.86669921875\n",
      "2019-04-10T00:51:50.849233: step 1679, loss 0.47567325830459595, accuracy 0.83349609375\n",
      "2019-04-10T00:51:50.903321: step 1680, loss 0.5803002715110779, accuracy 0.80322265625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:51.138939: step 1680, loss 0.459166, accuracy 0.8447265625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:51.195310: step 1681, loss 0.4087017774581909, accuracy 0.8798828125\n",
      "2019-04-10T00:51:51.249759: step 1682, loss 0.5950947999954224, accuracy 0.81982421875\n",
      "2019-04-10T00:51:51.310532: step 1683, loss 0.5215849280357361, accuracy 0.83984375\n",
      "2019-04-10T00:51:51.368574: step 1684, loss 0.42928430438041687, accuracy 0.83642578125\n",
      "2019-04-10T00:51:51.427187: step 1685, loss 0.5110530257225037, accuracy 0.8232421875\n",
      "2019-04-10T00:51:51.484577: step 1686, loss 0.4609346389770508, accuracy 0.8466796875\n",
      "2019-04-10T00:51:51.538487: step 1687, loss 0.4714599549770355, accuracy 0.83349609375\n",
      "2019-04-10T00:51:51.595509: step 1688, loss 0.4473508298397064, accuracy 0.83349609375\n",
      "2019-04-10T00:51:51.649108: step 1689, loss 0.3892684876918793, accuracy 0.853515625\n",
      "2019-04-10T00:51:51.703700: step 1690, loss 0.5768423080444336, accuracy 0.830078125\n",
      "2019-04-10T00:51:51.757255: step 1691, loss 0.4258882403373718, accuracy 0.8466796875\n",
      "2019-04-10T00:51:51.812111: step 1692, loss 0.4680922329425812, accuracy 0.853515625\n",
      "2019-04-10T00:51:51.869959: step 1693, loss 0.5033654570579529, accuracy 0.83349609375\n",
      "2019-04-10T00:51:51.932595: step 1694, loss 0.38147518038749695, accuracy 0.87353515625\n",
      "2019-04-10T00:51:51.988909: step 1695, loss 0.4948485195636749, accuracy 0.8466796875\n",
      "2019-04-10T00:51:52.044009: step 1696, loss 0.4527965784072876, accuracy 0.83642578125\n",
      "2019-04-10T00:51:52.098025: step 1697, loss 0.511413037776947, accuracy 0.85986328125\n",
      "2019-04-10T00:51:52.151430: step 1698, loss 0.43549907207489014, accuracy 0.84326171875\n",
      "2019-04-10T00:51:52.203804: step 1699, loss 0.507347583770752, accuracy 0.81689453125\n",
      "2019-04-10T00:51:52.256458: step 1700, loss 0.58720463514328, accuracy 0.78662109375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:52.483834: step 1700, loss 0.459437, accuracy 0.84228515625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:52.540741: step 1701, loss 0.39152422547340393, accuracy 0.86328125\n",
      "2019-04-10T00:51:52.595680: step 1702, loss 0.532027006149292, accuracy 0.80322265625\n",
      "2019-04-10T00:51:52.653152: step 1703, loss 0.5299433469772339, accuracy 0.82666015625\n",
      "2019-04-10T00:51:52.704793: step 1704, loss 0.580280065536499, accuracy 0.77978515625\n",
      "2019-04-10T00:51:52.757630: step 1705, loss 0.4488990306854248, accuracy 0.81982421875\n",
      "2019-04-10T00:51:52.810162: step 1706, loss 0.3650728762149811, accuracy 0.89013671875\n",
      "2019-04-10T00:51:52.867762: step 1707, loss 0.5195798873901367, accuracy 0.853515625\n",
      "2019-04-10T00:51:52.923960: step 1708, loss 0.46765193343162537, accuracy 0.83642578125\n",
      "2019-04-10T00:51:52.976223: step 1709, loss 0.40252283215522766, accuracy 0.85986328125\n",
      "2019-04-10T00:51:53.029468: step 1710, loss 0.5098365545272827, accuracy 0.85009765625\n",
      "2019-04-10T00:51:53.083852: step 1711, loss 0.5016981959342957, accuracy 0.84326171875\n",
      "2019-04-10T00:51:53.139870: step 1712, loss 0.46862491965293884, accuracy 0.8466796875\n",
      "2019-04-10T00:51:53.195095: step 1713, loss 0.44797179102897644, accuracy 0.83642578125\n",
      "2019-04-10T00:51:53.249207: step 1714, loss 0.4430183470249176, accuracy 0.84326171875\n",
      "2019-04-10T00:51:53.305634: step 1715, loss 0.4808076024055481, accuracy 0.80322265625\n",
      "2019-04-10T00:51:53.361763: step 1716, loss 0.5229743719100952, accuracy 0.830078125\n",
      "2019-04-10T00:51:53.415916: step 1717, loss 0.48871374130249023, accuracy 0.83349609375\n",
      "2019-04-10T00:51:53.468821: step 1718, loss 0.5068700909614563, accuracy 0.830078125\n",
      "2019-04-10T00:51:53.520478: step 1719, loss 0.5417875647544861, accuracy 0.806640625\n",
      "2019-04-10T00:51:53.572786: step 1720, loss 0.5181715488433838, accuracy 0.80322265625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:53.803534: step 1720, loss 0.468682, accuracy 0.84375\n",
      "\n",
      "\n",
      "2019-04-10T00:51:53.858957: step 1721, loss 0.5176013708114624, accuracy 0.85009765625\n",
      "2019-04-10T00:51:53.911254: step 1722, loss 0.4354052245616913, accuracy 0.8232421875\n",
      "2019-04-10T00:51:53.962068: step 1723, loss 0.4580167531967163, accuracy 0.83642578125\n",
      "2019-04-10T00:51:54.016580: step 1724, loss 0.43458738923072815, accuracy 0.83642578125\n",
      "2019-04-10T00:51:54.071703: step 1725, loss 0.3961118757724762, accuracy 0.8466796875\n",
      "2019-04-10T00:51:54.123587: step 1726, loss 0.43280038237571716, accuracy 0.853515625\n",
      "2019-04-10T00:51:54.177729: step 1727, loss 0.5284680128097534, accuracy 0.830078125\n",
      "2019-04-10T00:51:54.228960: step 1728, loss 0.5960490107536316, accuracy 0.81005859375\n",
      "2019-04-10T00:51:54.286770: step 1729, loss 0.36626577377319336, accuracy 0.87353515625\n",
      "2019-04-10T00:51:54.341673: step 1730, loss 0.4121607840061188, accuracy 0.86328125\n",
      "2019-04-10T00:51:54.395824: step 1731, loss 0.39895886182785034, accuracy 0.85009765625\n",
      "2019-04-10T00:51:54.448491: step 1732, loss 0.4698258936405182, accuracy 0.84326171875\n",
      "2019-04-10T00:51:54.499869: step 1733, loss 0.4684181213378906, accuracy 0.83349609375\n",
      "2019-04-10T00:51:54.551699: step 1734, loss 0.432182252407074, accuracy 0.86328125\n",
      "2019-04-10T00:51:54.603859: step 1735, loss 0.4003821909427643, accuracy 0.8466796875\n",
      "2019-04-10T00:51:54.660535: step 1736, loss 0.38939639925956726, accuracy 0.87353515625\n",
      "2019-04-10T00:51:54.717957: step 1737, loss 0.3823062479496002, accuracy 0.8564453125\n",
      "2019-04-10T00:51:54.779450: step 1738, loss 0.5133736729621887, accuracy 0.83642578125\n",
      "2019-04-10T00:51:54.833833: step 1739, loss 0.40428048372268677, accuracy 0.853515625\n",
      "2019-04-10T00:51:54.885509: step 1740, loss 0.45075735449790955, accuracy 0.82666015625\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:55.106661: step 1740, loss 0.435204, accuracy 0.85009765625\n",
      "\n",
      "\n",
      "2019-04-10T00:51:55.161776: step 1741, loss 0.5695018768310547, accuracy 0.83984375\n",
      "2019-04-10T00:51:55.213238: step 1742, loss 0.43299630284309387, accuracy 0.8232421875\n",
      "2019-04-10T00:51:55.270400: step 1743, loss 0.42880910634994507, accuracy 0.830078125\n",
      "2019-04-10T00:51:55.325919: step 1744, loss 0.32414087653160095, accuracy 0.8935546875\n",
      "2019-04-10T00:51:55.381189: step 1745, loss 0.4099416732788086, accuracy 0.86328125\n",
      "2019-04-10T00:51:55.434610: step 1746, loss 0.428018182516098, accuracy 0.86328125\n",
      "2019-04-10T00:51:55.491872: step 1747, loss 0.46287715435028076, accuracy 0.830078125\n",
      "2019-04-10T00:51:55.544298: step 1748, loss 0.41861239075660706, accuracy 0.82666015625\n",
      "2019-04-10T00:51:55.602125: step 1749, loss 0.38544562458992004, accuracy 0.86328125\n",
      "2019-04-10T00:51:55.655013: step 1750, loss 0.5950735211372375, accuracy 0.7900390625\n",
      "2019-04-10T00:51:55.711939: step 1751, loss 0.5547334551811218, accuracy 0.7998046875\n",
      "2019-04-10T00:51:55.773280: step 1752, loss 0.45347845554351807, accuracy 0.86328125\n",
      "2019-04-10T00:51:55.830748: step 1753, loss 0.5428123474121094, accuracy 0.8134765625\n",
      "2019-04-10T00:51:55.883081: step 1754, loss 0.6002892851829529, accuracy 0.796875\n",
      "2019-04-10T00:51:55.939611: step 1755, loss 0.5260311365127563, accuracy 0.8232421875\n",
      "2019-04-10T00:51:55.993608: step 1756, loss 0.5275625586509705, accuracy 0.7998046875\n",
      "2019-04-10T00:51:56.047500: step 1757, loss 0.47450363636016846, accuracy 0.8232421875\n",
      "2019-04-10T00:51:56.099839: step 1758, loss 0.48827823996543884, accuracy 0.830078125\n",
      "2019-04-10T00:51:56.152511: step 1759, loss 0.5249262452125549, accuracy 0.81689453125\n",
      "2019-04-10T00:51:56.203648: step 1760, loss 0.48264408111572266, accuracy 0.83642578125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:56.433214: step 1760, loss 0.459195, accuracy 0.84521484375\n",
      "\n",
      "\n",
      "2019-04-10T00:51:56.490094: step 1761, loss 0.45032042264938354, accuracy 0.84326171875\n",
      "2019-04-10T00:51:56.544780: step 1762, loss 0.5151865482330322, accuracy 0.81005859375\n",
      "2019-04-10T00:51:56.599786: step 1763, loss 0.5660374164581299, accuracy 0.82666015625\n",
      "2019-04-10T00:51:56.655075: step 1764, loss 0.43664854764938354, accuracy 0.83984375\n",
      "2019-04-10T00:51:56.708095: step 1765, loss 0.4379800856113434, accuracy 0.830078125\n",
      "2019-04-10T00:51:56.771225: step 1766, loss 0.4334560036659241, accuracy 0.83642578125\n",
      "2019-04-10T00:51:56.827891: step 1767, loss 0.5196830630302429, accuracy 0.83984375\n",
      "2019-04-10T00:51:56.883390: step 1768, loss 0.4830763638019562, accuracy 0.81689453125\n",
      "2019-04-10T00:51:56.935426: step 1769, loss 0.46107223629951477, accuracy 0.83642578125\n",
      "2019-04-10T00:51:56.990252: step 1770, loss 0.4696562588214874, accuracy 0.83642578125\n",
      "2019-04-10T00:51:57.045263: step 1771, loss 0.44844377040863037, accuracy 0.85009765625\n",
      "2019-04-10T00:51:57.102485: step 1772, loss 0.3822590410709381, accuracy 0.84326171875\n",
      "2019-04-10T00:51:57.161245: step 1773, loss 0.5316091775894165, accuracy 0.8232421875\n",
      "2019-04-10T00:51:57.220187: step 1774, loss 0.3887331783771515, accuracy 0.87646484375\n",
      "2019-04-10T00:51:57.280267: step 1775, loss 0.48239508271217346, accuracy 0.81005859375\n",
      "2019-04-10T00:51:57.339845: step 1776, loss 0.43278080224990845, accuracy 0.84326171875\n",
      "2019-04-10T00:51:57.394007: step 1777, loss 0.4849039316177368, accuracy 0.853515625\n",
      "2019-04-10T00:51:57.454058: step 1778, loss 0.4318799376487732, accuracy 0.8466796875\n",
      "2019-04-10T00:51:57.510777: step 1779, loss 0.39600393176078796, accuracy 0.86669921875\n",
      "2019-04-10T00:51:57.566615: step 1780, loss 0.47613489627838135, accuracy 0.83984375\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:57.799054: step 1780, loss 0.44449, accuracy 0.85205078125\n",
      "\n",
      "\n",
      "2019-04-10T00:51:57.861894: step 1781, loss 0.42145198583602905, accuracy 0.86669921875\n",
      "2019-04-10T00:51:57.920807: step 1782, loss 0.39489880204200745, accuracy 0.8466796875\n",
      "2019-04-10T00:51:57.973901: step 1783, loss 0.38385188579559326, accuracy 0.86328125\n",
      "2019-04-10T00:51:58.031937: step 1784, loss 0.5655409097671509, accuracy 0.83349609375\n",
      "2019-04-10T00:51:58.086787: step 1785, loss 0.39018917083740234, accuracy 0.85986328125\n",
      "2019-04-10T00:51:58.143171: step 1786, loss 0.5195352435112, accuracy 0.796875\n",
      "2019-04-10T00:51:58.197488: step 1787, loss 0.47849705815315247, accuracy 0.83642578125\n",
      "2019-04-10T00:51:58.254595: step 1788, loss 0.43102097511291504, accuracy 0.85009765625\n",
      "2019-04-10T00:51:58.315365: step 1789, loss 0.38527220487594604, accuracy 0.85986328125\n",
      "2019-04-10T00:51:58.373578: step 1790, loss 0.4777508080005646, accuracy 0.81982421875\n",
      "2019-04-10T00:51:58.427788: step 1791, loss 0.38728809356689453, accuracy 0.86669921875\n",
      "2019-04-10T00:51:58.484981: step 1792, loss 0.5340239405632019, accuracy 0.83642578125\n",
      "2019-04-10T00:51:58.540954: step 1793, loss 0.4765295386314392, accuracy 0.83642578125\n",
      "2019-04-10T00:51:58.597367: step 1794, loss 0.3781433701515198, accuracy 0.85986328125\n",
      "2019-04-10T00:51:58.651736: step 1795, loss 0.5509340167045593, accuracy 0.80322265625\n",
      "2019-04-10T00:51:58.708717: step 1796, loss 0.4629363417625427, accuracy 0.81689453125\n",
      "2019-04-10T00:51:58.766340: step 1797, loss 0.4279339015483856, accuracy 0.84326171875\n",
      "2019-04-10T00:51:58.832599: step 1798, loss 0.4447939693927765, accuracy 0.8232421875\n",
      "2019-04-10T00:51:58.890212: step 1799, loss 0.3924952447414398, accuracy 0.8466796875\n",
      "2019-04-10T00:51:58.915152: step 1800, loss 0.399338036775589, accuracy 0.8798828125\n",
      "\n",
      "Evaluation:\n",
      "2019-04-10T00:51:59.139447: step 1800, loss 0.445461, accuracy 0.853515625\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "model_train = ConvolutionalNeuralNetworkTrain()\n",
    "aaa = model_train.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
